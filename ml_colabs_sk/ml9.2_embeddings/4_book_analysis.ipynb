{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "as6_q5mQoi63",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "**POZNÁMKA: Tento notebook je určený pre platformu Google Colab. Je však možné ho spustiť (možno s drobnými úpravami) aj ako štandardný Jupyter notebook.** \n",
    "\n",
    "Tento notebook je inšpirovaný podobnými analýzami od iných autorov (napr. [Character Networks Visualization for Les Misérables](https://studentwork.prattsi.org/infovis/labs/character-networks-visualization-for-les-miserables/) a [Game of Thrones – Co-occurrence Network of Characters](https://studentwork.prattsi.org/infovis/labs/game-of-thrones-co-occurrence-network-of-characters/)). Zdrojový kód je však pôvodný a – na rozdiel od iných prác – celý proces, vrátane vizualizácie – sa realizuje v Python-e.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 899
    },
    "colab_type": "code",
    "id": "tpza3PdGXK6v",
    "outputId": "f85cf0de-2655-4d75-9e32-917e3c2aac7a"
   },
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install umap-learn python-louvain textblob\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WHqkgEF8PZuX"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import minmax_scale, MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import community # package python-louvain\n",
    "import networkx as nx\n",
    "from umap import UMAP\n",
    "from textblob import TextBlob\n",
    "\n",
    "from IPython.display import display\n",
    "from matplotlib.colors import to_hex\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "PwvKJoC0PZiQ",
    "outputId": "956fb8b6-0256-41ac-cd84-7cb29d20d2c7"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(\"https://www.dropbox.com/s/424vr9du2f480d9/three_musketeers.txt?dl=1\", directory=\"data\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# We also need some data from the nltk package\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zRXssoDSP2G4"
   },
   "outputs": [],
   "source": [
    "#@title -- Auxiliary Functions -- { display-mode: \"form\" }\n",
    "def refine_tags(tree, words, case_sensitive=True, infuse_text=False, tag=None):\n",
    "    class NOTHING: pass\n",
    "    \n",
    "    if isinstance(words, set):\n",
    "        words = {it: tag for it in words}\n",
    "    \n",
    "    if infuse_text:\n",
    "        make_item = lambda word, tag: (word, \"{}:{}\".format(tag, word))\n",
    "    else:\n",
    "        make_item = lambda word, tag: (word, tag)\n",
    "    \n",
    "    if case_sensitive:\n",
    "        normalize = lambda x: x\n",
    "    else:\n",
    "        normalize = lambda x: x.lower()\n",
    "        words = {normalize(k): v for k,v in words.items()}\n",
    "        \n",
    "    for item in tree:\n",
    "        if isinstance(item, tuple):\n",
    "            word, tag = item\n",
    "            words_tag = words.get(normalize(word), NOTHING())\n",
    "            \n",
    "            # not in dict: yield the original item\n",
    "            if isinstance(words_tag, NOTHING):\n",
    "                yield item\n",
    "            # tag is None: do not change the orginal tag\n",
    "            elif words_tag is None:                \n",
    "                yield make_item(word, tag)\n",
    "            # change the tag to words_tag\n",
    "            else:\n",
    "                yield make_item(word, words_tag)\n",
    "        else:\n",
    "            yield nltk.Tree(item.label(), refine_tags(item, words))\n",
    "\n",
    "def refine_chunks(chunked_sents, wordset, grammar, case_sensitive=True,\n",
    "                  infuse_text=False, tag=None):    \n",
    "    refined = (list(refine_tags(sent, wordset,\n",
    "                                case_sensitive=case_sensitive,\n",
    "                                infuse_text=infuse_text, tag=tag))\n",
    "                    for sent in chunked_sents)\n",
    "    rechunked = nltk.RegexpParser(grammar).parse_sents(refined)\n",
    "    return rechunked\n",
    "\n",
    "def mix_colors(colors):\n",
    "    return tuple(np.asarray(colors).mean(axis=0))\n",
    "\n",
    "def compute_cooccurence(entity_occurences, context=15, multi_count=False):\n",
    "    cooccurence = {}\n",
    "\n",
    "    for (ent1, occ1), (ent2, occ2) in itertools.combinations(\n",
    "        entity_occurences.items(), 2\n",
    "    ):\n",
    "        num_occ = 0\n",
    "        jstart = 0\n",
    "        \n",
    "        # for our algorithm to work, we need to make sure the occurrence sequences are sorted\n",
    "        occ1 = sorted(occ1)\n",
    "        occ2 = sorted(occ2)\n",
    "      \n",
    "        for i in range(len(occ1)):\n",
    "            for j in range(jstart, len(occ2)):\n",
    "                if occ1[i] >= occ2[j] - context and occ1[i] <= occ2[j] + context:\n",
    "                    # we have found a co-occurence\n",
    "                    num_occ += 1\n",
    "                    \n",
    "                    if not multi_count:\n",
    "                        # once we get a match, we increment i and set jstart\n",
    "                        # j + 1 so that we do not count the same co-occurrence\n",
    "                        # more than once\n",
    "                        jstart = j + 1\n",
    "                        continue\n",
    "                \n",
    "                elif occ1[i] < occ2[j] - context:\n",
    "                    # we continue with the next i, since occ2[j]\n",
    "                    # will only get larger\n",
    "                    break\n",
    "                    \n",
    "                elif occ1[i] > occ2[j] + context:\n",
    "                    # occ1[i] will only get larger, so we do not need to\n",
    "                    # consider this j in future iterations\n",
    "                    jstart = j + 1\n",
    "\n",
    "        cooccurence[(ent1, ent2)] = num_occ\n",
    "\n",
    "    return cooccurence\n",
    "  \n",
    "color_list = np.array([\n",
    "    (0, 150, 117),\n",
    "    (0, 196, 255),\n",
    "    (115, 192, 0),\n",
    "    (255, 85, 132),\n",
    "    (204, 173, 170),\n",
    "    (155, 116, 216),\n",
    "    \n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "]) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tcBIvkAJPLfs",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "## Grafy literárnych postáv založené na počte spoločných výskytov\n",
    "\n",
    "V tomto notebook-u budeme realizovať zaujímavú analýzu, ktorá spája prístupy z oblasti spracovania prirodzeného jazyka na jednej strane a z oblasti teórie grafov a analýzy sietí na druhej strane. Naším cieľom bude vytvoriť graf literárnych postáv z určitej knihy a následne na ňom vykonať určité analýzy a vizualizovať ich výsledky. \"Príbuznosť\" postáv vo výslednej sieti bude závisieť od počtu ich spoločných výskytov v rámci toho istého kontextu. Po vytvorení siete ukážeme, ako je možné vypočítať viacero ukazovateľov – ako je napríklad centralita jednotlivých vrcholov, ako detegovať komunity v sieti (skupiny postáv, ktoré spolu nejakým spôsobom súvisia) atď. Tento typ analýzy má dnes mnoho praktických aplikácií: napr. v analýze sociálnych sietí.\n",
    "\n",
    "Náš celkový postup bude vyzerať takto:\n",
    "\n",
    "* Identifikovať v texte pomenované entity: v našom prípade knižné postavy.\n",
    "* Určiť príbuznosť postáv na základe počtu spoločných výskytov v tom istom kontexte.\n",
    "* Na základe extrahovaných údajov skonštruovať sieť.\n",
    "* Vykonať analýzu siete a výsledky vizualizovať.\n",
    "### Načítanie textu\n",
    "\n",
    "V prvom kroku, samozrejme, načítame text knihy, ktorú budeme analyzovať. Keďže sa jedná len o jednu knihu, objem textových dát nebude príliš veľký a celý naraz sa bez problémov zmestí do operačnej pamäte. V opačnom prípade by sme ho museli rozdeliť na menšie časti a spracúvať postupne.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXFQjfIdPLgJ"
   },
   "outputs": [],
   "source": [
    "with open('data/three_musketeers.txt', 'r', encoding=\"utf8\") as f:\n",
    "    sample = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEYr79J8PLgS",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Predspracovanie textu\n",
    "\n",
    "V ďalšom kroku text predspracujeme. Najprv môžeme text mierne štandardizovať: napríklad nahradiť špeciálne typy úvodzoviek a inej interpunkcie kánonickými tvarmi a pod. Potom text rozdelíme na jednotlivé vety. Následne sa budú jednotlivé vety tokenizovať, t.j. rozdelia sa na menšie jednotky (slová, slovné spojenia).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "94-AUxyKPLgV",
    "outputId": "f9a463f8-aaa0-4657-e4d7-dd1ac674fbbf",
    "tags": [
     "sk"
    ]
   },
   "outputs": [],
   "source": [
    "# Špeciálne úvodzovky ‘’ nahradíme klasickými jednoduchými úvodzovkami '.\n",
    "trans_table = str.maketrans(\"‘’\", \"''\")\n",
    "sample = sample.translate(trans_table)\n",
    "\n",
    "# Text rozdelíme na vety.\n",
    "sentences = nltk.sent_tokenize(sample)\n",
    "\n",
    "# Zopár viet vypíšeme ako príkad.\n",
    "for sent in sentences[15:18]:\n",
    "    print(sent, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "gaSyVzrhPLge",
    "outputId": "04cbd7eb-0078-4dd9-eae9-3dab829a3833"
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "for tok_sent in tokenized_sentences[15:18]:\n",
    "    print(tok_sent, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LlLzXU6cfOgB",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Ďalej jednotlivé symboly (token-y) otagujeme, čím získame informáciu o ich úlohe vo vete – napríklad či predstavujú podstatné meno, sloveso a pod.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "gmlW5YZHPLgm",
    "outputId": "9b581293-8836-4c49-f847-13c4430f70da"
   },
   "outputs": [],
   "source": [
    "tagged_sentences = nltk.pos_tag_sents(tokenized_sentences)\n",
    "\n",
    "for tag_sent in tagged_sentences[15:18]:\n",
    "    print(tag_sent, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EY7vEOVlfii_",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Ďalej skúsime extrahovať informácie o štruktúre textu pomocou tzv. chunkovania.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Hf2oqPGPPLgv",
    "outputId": "430d98c4-c49d-40d0-bc63-8bab4c7df5f0"
   },
   "outputs": [],
   "source": [
    "chunked_sentences_sample = nltk.ne_chunk_sents(tagged_sentences[15:18])\n",
    "\n",
    "for chunked_sent in chunked_sentences_sample:\n",
    "    print(chunked_sent, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iiCCk-x6PLg5",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Rozpoznávanie entít\n",
    "\n",
    "Po chunkovaní vieme aplikovať algoritmus na rozpoznávanie pomenovaných entít. Keďže balíček `nltk` realizuje túto úlohu jednou z klasických metód, ktoré nevynikajú perfektnou robustnosťou a aj s ohľadom na charakter každého konkrétneho textu môže byť výsledky potrebné korigovať aj ručne, alebo prípadne upraviť vstupné dáta tak, aby sa dosiahol korektný výsledok.\n",
    "\n",
    "V každom prípade si budeme potrebovať uložiť aj miesta výskytu jednotlivých entít, aby sme neskôr boli schopní vyrátať, ako často sa entity vyskytovali spoločne v tom istom kontexte.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZETnGPAPLg-",
    "tags": [
     "sk"
    ]
   },
   "outputs": [],
   "source": [
    "# V texte rozpoznáme entity.\n",
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences)\n",
    "\n",
    "# Ak sa pred osobou vyskytuje titul, vtiahneme ho do mena entity.\n",
    "titles = {\"professor\", \"madame\", \"madam\", \"mr\", \"mr.\", \"mrs\", \"mrs.\",\n",
    "          \"miss\", \"uncle\", \"aunt\", \"lord\"}\n",
    "rechunked_sentences = refine_chunks(chunked_sentences, titles, tag=\"TIT\",\n",
    "                          grammar = r\"\"\"\n",
    "                              PERSON: {<TIT><PERSON>}\n",
    "                          \"\"\",\n",
    "                          case_sensitive=False)\n",
    "\n",
    "# Zhromaždíme miesta výskytov jednotlivých entít.\n",
    "# Paralelne si ukladáme finálnu podobu predspracovaného textu.\n",
    "entity_occurences = defaultdict(list)\n",
    "chunk_list = []\n",
    "num_chunks = 0\n",
    "\n",
    "for sent_tree in rechunked_sentences:    \n",
    "    for node in sent_tree:\n",
    "        # a regular tagged token\n",
    "        if isinstance(node, tuple):\n",
    "            chunk_list.append(node[0])\n",
    "        \n",
    "        # a sub-tree corresponding to an entity\n",
    "        else:\n",
    "            identifier = \" \".join((leaf[0] for leaf in node.leaves())).strip()    \n",
    "            entity_occurences[identifier].append(num_chunks)\n",
    "            chunk_list.append(identifier)\n",
    "        \n",
    "        num_chunks += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QqERtkl_PLhG",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Zlučovanie rôznych mien tých istých entít\n",
    "\n",
    "Nedá očakávať, že proces rozpoznávania entít prebehne plne automaticky. Niektoré postavy sa v knihách označujú viacerými menami: napríklad rôznymi prezývkami, zdrobneninami a pod. Zlúčiť všetky tieto rozličné mená dokopy ako označenia tej istej entity je väčšinou bez skutočného porozumenia textu (ktorým existujúce metódy jednoducho nedisponujú) nemožné, preto budeme musieť aspoň časť tejto práce vykonať ručne.\n",
    "\n",
    "Nájdené entity si teda zobrazíme, zotriedené podľa počtu výskytov a kde je to potrebné, ručne ich zlúčime. Zlučovanie by bolo možné aj čiastočne automatizovať, napr. pomocou informácií z iného zdroja: môže sa nám napríklad podariť nájsť webovú stránku so zoznamom postáv aj s ich rôznymi menami a prezývkami.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QwgT47sgPLhJ",
    "outputId": "0590bddd-a752-44be-be8f-730d6a18c8cf"
   },
   "outputs": [],
   "source": [
    "for k, v in sorted(entity_occurences.items(), key=lambda it: -len(it[1])):\n",
    "    print(\"{}x\\t{}\".format(len(v), k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GpCdloSLPLhQ",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Manuálne môžeme entity zlúčiť napríklad pomocou slovníka v tvare:\n",
    "\n",
    "```\n",
    "entity_dict = {\n",
    "    'identifikator_entity': {'Meno Entity', 'Alternativne Meno 1', 'Alternativne Meno 2', ...},\n",
    "\n",
    "    ...\n",
    "}\n",
    "```\n",
    "Entity, ktoré sa vyskytujú málokrát, môžu byť menej podstatné: podľa uváženia ich môžeme v tejto fáze zo slovníka úplne vypustiť.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pbmty5F0PLhT"
   },
   "outputs": [],
   "source": [
    "entity_dict = {\n",
    "    \"Athos\": {\"Athos\", \"Monsieur Athos\"},\n",
    "    \"Milady\": {\"Milady\", \"Winter\", \"MILADY\"},\n",
    "    \"Porthos\": {\"Porthos\", \"Monsieur Porthos\", \"PORTHOS\"},\n",
    "    \"Aramis\": {\"Aramis\", \"Monsieur Aramis\", \"ARAMIS\"},\n",
    "    \"Felton\": {\"Felton\"},\n",
    "    \"Bonacieux\": {\"Bonacieux\", \"Madame Bonacieux\", \"Monsieur Bonacieux\"},\n",
    "    \"Treville\": {\"Treville\"},\n",
    "    \"Planchet\": {\"Planchet\"},\n",
    "    \"Buckingham\": {\"Buckingham\"},\n",
    "    \"Grimaud\": {\"Grimaud\"},\n",
    "    \"Bazin\": {\"Bazin\"},\n",
    "    \"Mousqueton\": {\"Mousqueton\"},\n",
    "    \"La Rochelle\": {\"La Rochelle\"},\n",
    "    \"Richelieu\": {\"Richelieu\", \"Eminence\", \"Cardinal\", \"Monsieur Cardinal\"},\n",
    "    \"d'Artagnan\": {\"Gascon\"},\n",
    "    \"Rochefort\": {\"Rochefort\"},\n",
    "    \"de Chevreuse\": {\"Madame de Chevreuse\"},\n",
    "    \"Madame Coquenard\": {\"Madame Coquenard\", \"Coquenard\"},\n",
    "    \"Monsieur Dessessart\": {\"Monsieur Dessessart\"},\n",
    "    \"Louis XIII\": {\"Louis XIII\", \"Louis\"},\n",
    "    \"Louis XIV\": {\"Louis XIV\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xjVKmnhYPLhb",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Získaný slovník teraz invertujeme (tak, aby mapoval všetky alternatívne mená na ten istý identifikátor postavy).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-1Wq2XXPLhc"
   },
   "outputs": [],
   "source": [
    "reverse_entity_dict = defaultdict(set)\n",
    "\n",
    "for entity, names in entity_dict.items():\n",
    "    for name in names:\n",
    "        reverse_entity_dict[name].update({entity})\n",
    "        \n",
    "reverse_entity_dict = dict(reverse_entity_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CzegWRljPLhi",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "V prípade potreby môžeme otestovať, či sme na nejaké dôležité entity nezabudli:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2beY5r-iPLhk",
    "outputId": "8653474c-6c91-4585-e5e4-2a86c27588e9"
   },
   "outputs": [],
   "source": [
    "forgotten_entity_occurences = {k: v for k, v in entity_occurences.items() if not k in reverse_entity_dict}\n",
    "\n",
    "for k, v in sorted(forgotten_entity_occurences.items(), key=lambda it: -len(it[1])):\n",
    "    print(\"{}x\\t{}\".format(len(v), k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GpNEWeEVPLhr",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Zoznam výskytov transformujeme pomocou inverzného slovníka entít.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UQdZCEoPLht"
   },
   "outputs": [],
   "source": [
    "translated_occurences = defaultdict(list)\n",
    "\n",
    "for entity, occurences in entity_occurences.items():\n",
    "    try:\n",
    "        for translated_entity in reverse_entity_dict[entity]:\n",
    "            translated_occurences[translated_entity].extend(occurences)\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41I2vJqLPLhz",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Nájdeme spoločné výskyty entít pomocou preddefinovanej funkcie.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2PtYzoydPLh4"
   },
   "outputs": [],
   "source": [
    "cooccurence = compute_cooccurence(translated_occurences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LjHQXC3NPLh8",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Výsledky uložíme do CSV súborov: osobitne vrcholy (entity) a osobitne hrany (ohodnotené počtom spoločných výskytov).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v14C4pQ8PLh_"
   },
   "outputs": [],
   "source": [
    "with open(\"output/nodes.csv\", \"w\") as nodes_file:\n",
    "    nodes_file.write(\"Id,Label,Occurences\\n\")\n",
    "   \n",
    "    for entity, occurences in translated_occurences.items():\n",
    "        nodes_file.write(\"{},{},{}\\n\".format(entity, entity, len(occurences)))\n",
    "\n",
    "with open(\"output/edges.csv\", \"w\") as edges_file:\n",
    "    edges_file.write(\"Source,Target,Type,id,weight\\n\")\n",
    "   \n",
    "    for i, ((ent1, ent2), num_cooccur) in enumerate(cooccurence.items()):\n",
    "        if num_cooccur > 0:\n",
    "            edges_file.write(\"{},{},Undirected,{},{}\\n\".format(ent1, ent2, i, num_cooccur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rRjtv1BojAJ",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Konštrukcia, analýza a vizualizácia grafu\n",
    "\n",
    "Ďalej skonštruujeme graf, ktorý vznikol na základe analýzy textu a budeme naň aplikovať metódy analýzy grafov. Všetky tieto kroky budeme realizovať v jazyku Python. Bolo by však rovnako jednoduché načítať CSV súbory, ktoré sme vyššie vytvorili, v nejakom externom nástroji, ako je napr. známy softvér na vizualizáciu grafov [gephi](https://gephi.org/). Zvyšný postup súvisiaci s analýzou a vizualizáciou grafu by sa potom dal realizovať tam.\n",
    "\n",
    "V našom prípade použijeme na konštrukciu grafu python-ový balíček `networkx`. Do novo vytvoreného grafu pridáme všetky vrcholy uložené v súbore `nodes.csv`. Zaujímajú nás ID vrcholov, mená postáv, ktoré sa s nimi spájajú, a celkový počet výskytov príslušných postáv, preto tieto údaje priložíme ku vrcholom ako dáta:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ajnos9LFojAL"
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "nodes = pd.read_csv(\"output/nodes.csv\")\n",
    "for nid, (node, label, occurences) in nodes[[\"Id\", \"Label\", \"Occurences\"]].iterrows():  \n",
    "    G.add_node(node, label=label, occurences=occurences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cN1hNa-bojAW",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Ďalej načítame súbor `edges.csv` a pridáme do grafu všetky hrany. Váhy im priradíme na základe počtu spoločných výskytov postáv v texte, ako je zaznamený v CSV súbore:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dctof5rcojAY"
   },
   "outputs": [],
   "source": [
    "edges = pd.read_csv(\"output/edges.csv\")\n",
    "G.add_weighted_edges_from((\n",
    "       (src, tgt, w)\n",
    "       for i, (src, tgt, w) in\n",
    "       edges[[\"Source\", \"Target\", \"weight\"]].iterrows()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q58v8KgoojAf",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Ak sú v grafe nejaké vrcholy, ktoré sú izolované (nie sú prepojené hranami s inými vrcholmi), odstránime ich, aby bol graf prehľadnejší. Tiež konvertujeme identifikátory vrcholov na celočíselné, aby sa s nimi ľahšie pracovalo:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C3Df7YYuojAh"
   },
   "outputs": [],
   "source": [
    "G.remove_nodes_from(list(nx.isolates(G)))\n",
    "G = nx.convert_node_labels_to_integers(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VkgC0YI2ojAo",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "#### Určenie \"dôležitosti\" vrcholov pomocou page-rank-u\n",
    "\n",
    "Pri vizualizácii grafov sa ponúka veľa možností, ako do výslednej grafickej reprezentácie zakódovať relevantné informácie. Jedna z možností je využiť veľkosť vrcholov. V našom prípade vypočítame pre každý vrchol indikátor centrality známy ako page-rank. Voľne povedané, page-rank bude indikovať, aký dôležitý je vrchol v rámci grafu. Page-rank-y všetkých vrcholov preškálujeme do rozumného rozsahu a na výsledku založíme veľkosť vrcholov. Ten istý prístup použijeme aj na preškálovanie veľkostí fontov, ktoré sa použijú v popiskoch vrcholov.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "acCXBBL0ojAp",
    "outputId": "db0fe7cd-79a8-4a16-aa4f-22b4ceed253d"
   },
   "outputs": [],
   "source": [
    "page_rank = nx.pagerank(G)\n",
    "sizes = [page_rank[node] for node in G.nodes()]\n",
    "sizes = minmax_scale(sizes, (80, 1000))\n",
    "\n",
    "font_scaler = MinMaxScaler((12, 20))\n",
    "font_scaler.fit(np.reshape(sizes, (-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "28bzKYZ9ojAv",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "#### Hrúbka a transparentnosť hrán\n",
    "\n",
    "Hrúbky hrán v grafe určíme jednoducho preškálovaním váh jednotlivých hrán: t.j. počtov spoločných výskytov. Tie isté hodnoty, len v inej škále, použijeme aj na určenie miery transparentnosti jednotlivých hrán. Keď niektoré hrany zobrazíme transparentnejšie, bude graf pôsobiť prehľadnejšie.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3T3g7f7ojAy"
   },
   "outputs": [],
   "source": [
    "widths = [edge[2] for edge in G.edges(data='weight')]\n",
    "widths = minmax_scale(widths, (1, 5))\n",
    "edge_alpha = minmax_scale(widths, (0.3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g22TfnbyojA3",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "#### Detekcia komunít v grafe\n",
    "\n",
    "Ďalej použijeme detekciu komunít z balíčka `community`, aby sme vrcholy rozdelili podľa komunít. Hrubo povedané, ide o približný ekvivalent zhlukovania pre grafy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7YBj5CchRE-i"
   },
   "outputs": [],
   "source": [
    "parts = community.best_partition(G, resolution=1.0)\n",
    "num_parts = np.max(list(parts.values()))\n",
    "\n",
    "communities = [[] for i in range(num_parts+1)]\n",
    "for k, v in parts.items():\n",
    "    communities[v].append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D9qjoRqMop4U",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Zafarbenie vrcholov a hrán určíme podľa komunít, do ktorých patria:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QWjrx0rXri_t"
   },
   "outputs": [],
   "source": [
    "colors = np.asarray([to_hex(color_list[parts.get(node)]) for node in G.nodes()])\n",
    "cmap = lambda c: color_list[c]\n",
    "\n",
    "edge_colors = [to_hex(\n",
    "    mix_colors([cmap(parts.get(src)), cmap(parts.get(dest))]),\n",
    "      keep_alpha=True) for src, dest, w in G.edges(data='weight')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C4mW72mOojBG",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "#### 2-rozmerné rozmiestnenie vrcholov\n",
    "\n",
    "Keďže graf zobrazujeme v 2-rozmernom diagrame, musíme všetkým jeho vrcholom pochopiteľne priradiť nejaké pozície. Ideálne by bolo urobiť to tak, aby sa úzko súvisiace vrcholy nachádzali blízko seba a aby sa hrany vzájomne príliš neprekrývali. Existuje mnoho metód na určenie rozloženia vrcholov. Nie všetky z nich však poskytujú dobré výsledky v prípade, že je graf zložitý a obsahuje veľký počet spojení. Pri určovaní rozloženia vrcholov preto mierne zneužijeme UMAP – metódu pôvodne určenú na znižovanie rozmeru dát – a rozloženie určíme pomocou nej.\n",
    "\n",
    "Jedným zo vstupov metódy UMAP sú vzdialenosti medzi bodmi. V našom prípade vytvoríme maticu vzdialeností invertovaním váh hrán (počtov spoločných výskytov). UMAP bude potom združovať vrcholy, ktoré sa často vyskytujú v tom istom kontexte.\n",
    "\n",
    "Aby sme zabezpečili, že budú vrcholy z tej istej komunity držať pokope, zvolíme počiatočné pozície všetkých vrcholov tak, že komunitám sa priradia rovnako vzdialené body pozdĺž obvodu kružnice (všetky vrcholy patriace do tej istej komunity sú na začiatku na tej istej pozícii).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "t0yZTVVVojBI",
    "outputId": "9cd02d75-cd30-4bd0-f2a7-5725586f756a"
   },
   "outputs": [],
   "source": [
    "# set up the distance matrix for UMAP\n",
    "num_nodes = G.number_of_nodes()\n",
    "dist_mat = np.zeros([num_nodes, num_nodes])\n",
    "max_weight = edges[\"weight\"].max()\n",
    "\n",
    "for n1, n2, w in G.edges(data=\"weight\"):\n",
    "    invdist = w / max_weight\n",
    "    dist_mat[n1, n2] += invdist\n",
    "    dist_mat[n2, n1] += invdist\n",
    "dist_mat = dist_mat.max() - dist_mat\n",
    "\n",
    "# set up the initial node positions for UMAP\n",
    "num_communities = len(communities)\n",
    "community_angles = np.asarray(\n",
    "    [ic*2*3.14/num_communities for ic in range(num_communities)])\n",
    "community_centers = np.stack(\n",
    "    [np.sin(community_angles) * 100, np.cos(community_angles) * 100], axis=1)\n",
    "\n",
    "init = np.zeros([num_nodes, 2])\n",
    "for c, mem in enumerate(communities):\n",
    "    for n in mem:\n",
    "        init[n] = community_centers[c]\n",
    "\n",
    "# run UMAP\n",
    "umap = UMAP(\n",
    "    metric='precomputed',\n",
    "    init=init,\n",
    "    min_dist=10,\n",
    "    spread=50,\n",
    ")\n",
    "\n",
    "pos = umap.fit_transform(dist_mat)\n",
    "pos = {ipos: p for ipos, p in enumerate(pos)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VVEdcgO7ojBT",
    "tags": [
     "sk"
    ]
   },
   "source": [
    "#### Vykreslenie grafu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2d48RYBpx5m"
   },
   "source": [
    "A nakoniec nezostáva už nič iné než vykresliť samotný graf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "qCi-QWbhr_Oy",
    "outputId": "10e83ede-68bd-4dbf-a4b3-fb7622739f4d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[14, 10])\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(\n",
    "     G,\n",
    "     pos = pos,\n",
    "     node_size = sizes,\n",
    "     node_color = colors,\n",
    "     linewidths = 1,\n",
    ")\n",
    "\n",
    "# edges\n",
    "edge_collection = nx.draw_networkx_edges(\n",
    "    G,\n",
    "    pos,\n",
    "    width = widths,\n",
    "    edge_color = edge_colors,               \n",
    ")\n",
    "\n",
    "# transparency of edges\n",
    "edge_alpha_colors = [tuple(col[:3]) + (al,) for al, col\n",
    "                        in zip(edge_alpha, edge_collection.get_colors())]\n",
    "edge_collection.set_color(edge_alpha_colors)\n",
    "\n",
    "# edge labels\n",
    "text_collection = nx.draw_networkx_labels(G, pos,\n",
    "    labels = {node[0]: node[1].replace(\" \", \"\\n\")\n",
    "                  for node in G.nodes(data=\"label\")})\n",
    "\n",
    "# font sizes\n",
    "for node, textobj in text_collection.items():\n",
    "    rank = np.reshape([page_rank[node]], (1, -1))\n",
    "    textobj.set_fontsize(font_scaler.transform(rank)[0])\n",
    "\n",
    "# minor re-styling of the plot\n",
    "plt.gca().collections[0].set_edgecolor(\"k\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F6uO6fJoPLiF"
   },
   "source": [
    "## Sentimentálny kontext postáv\n",
    "\n",
    "Metódy analýzy textu nám umožňujú odhadovať aj sentiment textu (pozitívny, negatívny a pod.). Jednoduché prístupy ku analýze sentimentu implementuje napríklad balíček `TextBlob`. Presnejšie výsledky by bolo možné získať napríklad pomocou niektorej z metód založených na hlbokom učení, ale aj výsledky získané pomocou tejto metódy by mali na hrubú analýzu stačiť.\n",
    "\n",
    "Skúsme teda extrahovať kontexty, v ktorých sa mená jednotlivých postáv vyskytujú, a určiť ich sentiment. V grafe si potom zobrazíme s akým prevládajúcim sentimentom sa jednotlivé postavy v knihe spájajú.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TZ7ZwK-jPLiS"
   },
   "outputs": [],
   "source": [
    "# context radius\n",
    "blob_radius = 10\n",
    "\n",
    "names = []\n",
    "sentiments = []\n",
    "num_occurences = []\n",
    "\n",
    "# we extract context and accumulate their polarities\n",
    "for ent, occurences in translated_occurences.items():\n",
    "    sentiment = 0\n",
    "    \n",
    "    for occ in occurences:\n",
    "        sentiment += TextBlob(\" \".join(chunk_list[occ-blob_radius:occ+blob_radius])).polarity\n",
    "        \n",
    "    sentiment /= len(occurences)\n",
    "    \n",
    "    names.append(ent)\n",
    "    sentiments.append(sentiment)\n",
    "    num_occurences.append(len(occurences))\n",
    "\n",
    "# we track the accumulated sentiment but also the number of occurrences\n",
    "data = np.asarray([sentiments, num_occurences]).transpose()\n",
    "entity_sentiments = pd.DataFrame(data, columns=[\"sentiment\", \"occurences\"], index=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMc3QlPussUy"
   },
   "source": [
    "Vo výslednom grafe zobrazíme len entity, ktoré sa vyskytujú väčší počet krát. Entity s menším počtom výskytov okrem toho budeme vizualizovať transparentnejšou farbou, aby sme ich odlíšili.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "BfmxrvpSPLiZ",
    "outputId": "fb4f8c19-d7a0-498e-d862-0742ab4d60c3"
   },
   "outputs": [],
   "source": [
    "s = entity_sentiments[entity_sentiments[\"occurences\"] > 25]\n",
    "s_occ = minmax_scale(np.log(s[\"occurences\"].values), (0.3, 1.0))\n",
    "s_vals = s[\"sentiment\"].values\n",
    "s_index = s[\"sentiment\"].index\n",
    "\n",
    "abs_max = np.abs(s_vals).max()\n",
    "norm = plt.Normalize(vmin=-abs_max, vmax=abs_max)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(s_vals)), s_vals,\n",
    "  color=[(0, 0, 1, occ) if val >= 0 else \n",
    "         (1, 0, 0, occ) for val, occ in zip(s_vals, s_occ)]\n",
    ")\n",
    "\n",
    "plt.xticks(range(len(s_vals)), s_index, rotation=90)\n",
    "plt.ylabel(\"sentiment\")\n",
    "\n",
    "plt.subplots_adjust(left=0.12, right=0.9, top=0.9, bottom=0.3)\n",
    "plt.grid(linestyle='--')\n",
    "plt.gca().set_axisbelow(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vff_XszeojCC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "4_book_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
