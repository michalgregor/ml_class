{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad03dce",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "**POZNÁMKA: Tento notebook je určený pre platformu Google Colab. Je však možné ho spustiť (možno s drobnými úpravami) aj ako štandardný Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15fc464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/gym_plannable.git\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/rl_tabular.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ffd961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "%matplotlib inline\n",
    "from rl_tabular import ActionValueTable, StateValueTable\n",
    "from rl_tabular.maze_env_plots import (\n",
    "    Plotter, plot_action_values, plot_state_values)\n",
    "from rl_tabular import EpsGreedyPolicy, ReplayBuffer, QLearning\n",
    "from rl_tabular import ExponentialSchedule\n",
    "from rl_tabular import qtable_control\n",
    "from rl_tabular import Trainer, seed\n",
    "from gym_plannable.env import MazeEnv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1625d",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "## Q učenie\n",
    "\n",
    "Ďalej sa budeme venovať TD (temporal difference) prístupu známemu ako Q učenie. Q učenie je bezmodelové a učí sa optimálnu **hodnotovú funkci akcií** , takže model sa nevyžaduje ani na riadenie agenta (na rozdiel od TD učenia s hodnotovou funkciou stavov).\n",
    "\n",
    "Napokon, Q učenie je metóda neviazaná na stratégiu (off-policy) takže sa dokáže učiť aj z dát nazbieraných pomocou inej stratégie – to nám umožní používať ho s opätovným prehrávaním skúseností.\n",
    "\n",
    "### Q učenie s fixnou postupnosťou akcií\n",
    "\n",
    "V prvom kroku sa budeme sústrediť na samotné Q učenie a nebudeme riešiť prieskum. Preto v nasledujúcej bunke definujeme fixnú postupnosť akcií, ktorá agenta dostane z počiatočného do cieľového stavu. Jedinou úlohou nášho agenta bude naučiť sa postupnosť akcií potom, ako ju pár ráz pozoruje. Uvidíme, ako dobre to bude fungovať.\n",
    "\n",
    "---\n",
    "### Úloha 1: Implementujte pravidlo Q učenia\n",
    "\n",
    "**V nasledujúcej bunke doplňte implementáciu pravidla Q učenia.** \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta &= r_{t+1} + \\gamma \\underset{a}{\\max} \\ Q(s_{t+1}, a)-Q(s_{t},a_{t}) \\\\\n",
    "Q(s_{t},a_{t}) &\\leftarrow Q(s_{t},a_{t}) + \\alpha \\delta\n",
    "\\end{aligned}\n",
    "$$\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376d48a",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "actions = [0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 3, 3, 3, 3]\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "\n",
    "# create the environment\n",
    "env = MazeEnv() # we do not enable rendering here; we'll do it manually\n",
    "\n",
    "# set up plotting\n",
    "plotter = Plotter(env, ActionValueTable, StateValueTable, figsize=[8, 4])\n",
    "\n",
    "# set up the value function\n",
    "qtable = ActionValueTable(env.action_space.n)\n",
    "\n",
    "# the training loop\n",
    "step = 0\n",
    "for episode in range(20):\n",
    "    obs = env.reset()\n",
    "        \n",
    "    for a in actions:\n",
    "        # apply the action and observe the effect\n",
    "        obs_next, reward, done, _ = env.step(a)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # compute td: a, reward, qtable, gamma, obs, obs_next, np.max\n",
    "        td = # -----\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        qtable[obs, a] += alpha * td\n",
    "        \n",
    "        # book keeping\n",
    "        obs = obs_next\n",
    "        step += 1\n",
    "\n",
    "        # for the first 3 episodes, we also do step-wise plots\n",
    "        if episode < 3: plotter.plot(qtable, qtable.to_state_values())\n",
    "\n",
    "        # if the environment is done, conclude the episode\n",
    "        if done: break\n",
    "\n",
    "    print(f\"Episode {episode} finished after {step} steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4adac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_action_values(\n",
    "    qtable, plotter.states, env=env, render_agent=False\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plot_state_values(\n",
    "    qtable.to_state_values(), plotter.states, env=env, render_agent=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13142ab0",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Čo vidno z vizualizácie, je, že v prípade klasického Q učenia agent aktualizuje len hodnotu predposledného stavu. To dáva zmysel, pretože keď sa počítajú hodnoty predchádzajúcich stavov, ešte sa nevie, že agent získa za posledný stavový prechod odmenu.\n",
    "\n",
    "Na druhej strane je zrejmé, že je to veľmi nepraktické – znamená to, že celú sekvenciu je potrebné zopakovať približne 20 ráz (približne dĺžka postupnosti) kým sa ju agent celú naučí – t.j. kým sa hodnota prešíri pozdĺž celej cesty z cieľového až do počiatočného stavu.\n",
    "\n",
    "A aj to platí len za predpokladu, že agent bude každý raz pozorovať tú istú postupnosť akcií – pričom v bežných podmienkach by agent zakaždým preskúmaval inú cestu. Práve na tento aspekt prieskumu sa pozrieme teraz.\n",
    "\n",
    "### Q učenie s $\\varepsilon$-greedy prieskumom\n",
    "\n",
    "V predošlom príklade sme používali v každej epizóde tú istú postupnosť akcií. Teraz budeme postupovať trochu realistickejším spôsobom: použijeme $\\varepsilon$-greedy stratégiu s $\\varepsilon = 0.1$, aby sme robili aj určitý prieskum.\n",
    "\n",
    "Na tento experiment využijeme hotové implementácie stratégie aj Q učenia. Vizualizácie budú ukazovať oboje: hodnotovú funkciu akcií – kde **zafarbené štvorčeky**  vizualizujú **cestu, ktorú si agent zvolil**  v poslednej epizóde – a hodnotovú funkciu stavov. \n",
    "\n",
    "*Všimnite si riadok `seed(1)`. Tento fixuje jadro náhodného generátora tak, aby sme dostali tie isté výsledky pri každom spustení bunky. Ak chcete vidieť aj iné spôsoby, ako by učenie mohlo prebiehať, môžete tento riadok zakomentovať.* \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f4128",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "env = MazeEnv(\n",
    "    show_path=True,\n",
    "    show_path_kw=dict(show_arrows=False, show_visited=True)\n",
    ")\n",
    "\n",
    "# set up plotting\n",
    "plotter = Plotter(env, ActionValueTable, \n",
    "    (StateValueTable, {'render_kwargs': {'skip': {'player_logger'}}}),\n",
    "     figsize=[8, 4], render_agent=False\n",
    ")\n",
    "\n",
    "qtable = ActionValueTable(env.action_space.n)\n",
    "algo = QLearning(qtable, alpha=0.5, gamma=0.9)\n",
    "policy = EpsGreedyPolicy(qtable, env.action_space.n, epsilon=0.1)\n",
    "trainer = Trainer(\n",
    "    algo, policy, verbose=5, on_end_episode=[\n",
    "        lambda *args: plotter.plot(qtable, qtable.to_state_values())]\n",
    ")\n",
    "\n",
    "trainer.train(env, max_episodes=50, max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84773a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv(show_path=True)\n",
    "qtable_control(env, qtable, render=False, max_steps=100)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fdc7f2",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Ako vidno, agent sa pohybuje náhodne kým neobjaví cieľový stav – od tej chvíle sa začína učiť cestu, ktorá k nemu vedie. Vďaka $\\varepsilon$-greedy stratégii stále robí aj trochu prieskumu: niekedy sa odchýli od toho, čo sa už naučil, vďaka čomu môže získať pre zmenu aj iné skúsenosti.\n",
    "\n",
    "Platí však, že pri $\\varepsilon = 0.1$ sa aj po veľmi veľkom počte epizód v skutočnosti naučí len jednu cestu do cieľového stavu a pozná len hodnoty stavov, ktoré ležia veľmi blízko nej.\n",
    "\n",
    "---\n",
    "### Úloha 2:\n",
    "\n",
    "**Ako ďalšiu úlohu skúste experiment spustiť znovu s inými hodnotami $\\varepsilon$. Sledujte čo sa mení keď sa $\\varepsilon$ posúva bližšie k 1. Akú veľkú časť stavového priestoru agent preskúma? Aký je celkový počet krokov, ktoré agent vykoná v rámci predpísaných 50-tich epizód?** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee19466",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb267cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv(show_path=True)\n",
    "qtable_control(env, qtable, max_steps=100)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ac215d",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Q učenie a opätovné prehrávanie skúseností\n",
    "\n",
    "Ďalším problémom, ktorému sa musíme venovať, je nízka vzorková účinnosť nášho algoritmu. Spomeňte si, že sme tú istú cestu museli prejsť veľa krát, pretože agent si z nej každý raz zapamätal len jeden ďalší krok. Musí predsa existovať spôsob, ako sa naučiť celú cestu bez toho, aby ju agent musel vidieť viac než raz.\n",
    "\n",
    "V skutočnosti existuje hneď niekoľko spôsobov, ako to docieliť – my budeme používať techniku známu ako opätovné prehrávanie skúseností (experience replay), kde si všetko, čo agent pozoroval, zaznamenáme a potom si časti týchto skúseností budeme viac ráz opätovne prehrávať. Tým spôsobom budeme vlastne vedieť prešíriť hodnotu prechodu do cieľového stavu späť ku počiatočnému stavu aj ak sme celú postupnosť akcií videli len raz.\n",
    "\n",
    "#### Prehrávacia pamäť (replay buffer)\n",
    "\n",
    "V našom prípade nebude treba prehrávaciu pamäť celú implementovať – mamé takú implementáciu už pripravenú. Pozrieme sa však teraz na jej rozhranie. Pri konštrukcii pamäte špecifikujeme jej maximálnu veľkosť (`max_size`). Keď sa dosiahne kapacita pamäte, nové skúsenosti začnú v pamäti nahrádzať tie najstaršie. Väčšinou však budeme mať priestoru dostatok – maximálna veľkosť sa typicky nastavuje na veľmi veľkú hodnotu takže sa skúsenosti často nevymazávajú vôbec.\n",
    "\n",
    "Okrem maximálnej veľkosti pamäte nastavujeme aj predvolenú veľkosť dávky (`batch_size`). Keď zavoláme metódu `.sample()`, dostaneme dávku s `batch_size` náhodne vybranými stavovými prechodmi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(max_size=10000, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e29a7",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Aby sme v prehrávacej pamäti zaznamenali stavový prechod, môžeme zavolať `replay_buffer.add(obs, a, reward, obs_next, done, info)`, kde `a` je akcia a `obs` je predprechodové pozorovanie, zatiaľ čo `obs_next` je pozorovanie po stavovom prechode.\n",
    "\n",
    "---\n",
    "#### Úloha 3: Zaznamenanie prechodov\n",
    "\n",
    "**Spustite prostredie a náhodne voľte akcie. Pomocou metódy `replay_buffer.add` zaznamenajte 1000 prechodov do prehrávacej pamäte.** \n",
    "\n",
    "Tipy:\n",
    "\n",
    "* Spomeňte si, že `env.step(action)` navracia n-ticu `(observation, reward, done, info)`.\n",
    "* Spomeňte si tiež, že keď `done` má hodnotu true, aktuálna epizóda skončila a treba začať novú epizódu volaním `obs = env.reset()`.\n",
    "* Náhodné akcie možno z priestoru akcií vzorkovať pomocou volania `env.action_space.sample()`.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9693341e",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(max_size=10000, batch_size=100)\n",
    "\n",
    "env = MazeEnv()\n",
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "for step in range(1000):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----\n",
    "    \n",
    "    \n",
    "    \n",
    "    obs = obs_next\n",
    "        if terminated or truncated: obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f361079",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, a, reward, obs_next, done, info = replay_buffer.sample()\n",
    "\n",
    "print(f\"The replay buffer contains {len(replay_buffer)} samples.\")\n",
    "print(f\"The batch contains {len(obs)} samples.\\n\")\n",
    "\n",
    "print(f\"obs: {obs[:3]} ...\")\n",
    "print(f\"actions: {a[:3]} ...\")\n",
    "print(f\"obs_next: {obs_next[:3]} ...\")\n",
    "print(f\"done: {done[:3]} ...\")\n",
    "print(f\"info: {info[:3]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec4b3d",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "#### Q učenie s prehrávacou pamäťou\n",
    "\n",
    "Ďalej budeme znovu experimentovať s Q učením, pričom teraz už budeme používať aj $\\varepsilon$-greedy stratégiu, aj prehrávaciu pamäť. Znovu použijeme už hotovú implementáciu danej metódy.\n",
    "\n",
    "Čo by sme mali vidieť je, že hodnoty sa budú šíriť omnoho rýchlejšie, pretože tie isté skúsenosti sa prehrávajú znovu a znovu. Okrem toho by malo byť vidno, že sa aktualizujú aj hodnoty stavov, ktoré neboli navštívené v rámci ostatnej epizódy. Pri hodnote $\\varepsilon = 0.1$ však agent stále nepreskúma adekvátne všetky oblasti stavového priestoru.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "env = MazeEnv(\n",
    "    show_path=True,\n",
    "    show_path_kw=dict(show_arrows=False, show_visited=True)\n",
    ")\n",
    "\n",
    "plotter = Plotter(env, ActionValueTable, \n",
    "    (StateValueTable, {'render_kwargs': {'skip': {'player_logger'}}}),\n",
    "     figsize=[8, 4], render_agent=False\n",
    ")\n",
    "\n",
    "qtable = ActionValueTable(env.action_space.n)\n",
    "algo = QLearning(qtable, alpha=0.5, gamma=0.9)\n",
    "policy = EpsGreedyPolicy(qtable, env.action_space.n, epsilon=0.1)\n",
    "replay_buffer = ReplayBuffer(max_size=10000, batch_size=100)\n",
    "\n",
    "trainer = Trainer(\n",
    "    algo, policy, verbose=5, replay_buffer=replay_buffer,\n",
    "    on_end_episode=[lambda *args: plotter.plot(qtable, qtable.to_state_values())],\n",
    ")\n",
    "\n",
    "trainer.train(env, max_episodes=20, max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db67a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv(show_path=True)\n",
    "qtable_control(env, qtable, max_steps=100)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd829d36",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Žíhanie miery prieskumu\n",
    "\n",
    "Ako sme videli, ak používame prinízku hodnotu $\\varepsilon$, vedie to k nedostatočnému prieskumu. Ak na druhej strane nastavíme $\\varepsilon$ na privysokú hodnotu, správanie sa agenta bude nestále aj potom, ako sa priblížil ku skutočnej hodnotovej funkcii akcií. Výsledkom je, že sa môže vzdať veľmi veľkého množstva odmien, epizóda môže trvať zbytočne veľa krokov atď.\n",
    "\n",
    "Je preto rozumné mieru prieskumu žíhať: aby agent realizoval veľa prieskumu na začiatku, ale aby sa miera prieskumu postupne znižnovala takže sa agent postupom času bude blížiť ku optimálnej stratégii, využívať a zdokonaľovať nadobudnuté poznatky.\n",
    "\n",
    "Častým spôsobom, ako mieru prieskumu žíhať, je predpísať jej exponenciálny rozvrh. Tu na to použijeme triedu `ExponentialSchedule`, v rámci ktorej môžeme špecifikovať:\n",
    "\n",
    "* Počiatočnú hodnotu parametra;\n",
    "* Konečnú hodnotu parametra;\n",
    "* Krok, kedy má začať žíhanie;\n",
    "* Krok, kedy sa má žíhanie ukončiť.\n",
    "V našom prípade bude žíhanie výchádzať z čísla epizódy (`method='episode'`) a nie z počtu krokov. Počet krokov by však bolo možné použiť takisto a dokonca je to pravdepodobne bežnejšia voľba. Takto môže náš exponenciálny rozvrh vyzerať:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f181b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_schedule = ExponentialSchedule(\n",
    "    None,\n",
    "    init_val=1.0, final_val=0.1,\n",
    "    first_step=5, final_step=18,\n",
    "    method='episode'\n",
    ")\n",
    "\n",
    "steps = range(eps_schedule.final_step+5)\n",
    "eps = [eps_schedule.get_value(s) for s in steps]\n",
    "plt.plot(steps, eps)\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"epsilon\")\n",
    "plt.grid(ls='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11852c7",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Keď sme si rozvrh zadefinovali, zaregistrujeme ho v rámci callback-u `on_begin_step`, aby sa $\\varepsilon$ podľa neho priebežne aktualizoval. Pozrime sa teraz, aký efekt to bude mať.\n",
    "\n",
    "Keďže v rámci niekoľkých počiatočných epizód držíme $\\varepsilon$ na hodnote $1$, agent sa vtedy bude správať úplne náhodným spôsobom. To by nám malo pomôcť nazbierať počas tých epizód veľa skúseností. Následne sa už bude $\\varepsilon$ postupne znižovať.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e334d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "env = MazeEnv(\n",
    "    show_path=True,\n",
    "    show_path_kw=dict(show_arrows=False, show_visited=True)\n",
    ")\n",
    "\n",
    "plotter = Plotter(env, ActionValueTable, \n",
    "    (StateValueTable, {'render_kwargs': {'skip': {'player_logger'}}}),\n",
    "     figsize=[8, 4], render_agent=False\n",
    ")\n",
    "\n",
    "qtable = ActionValueTable(env.action_space.n)\n",
    "algo = QLearning(qtable, alpha=0.5, gamma=0.9)\n",
    "policy = EpsGreedyPolicy(qtable, env.action_space.n)\n",
    "\n",
    "eps_schedule = ExponentialSchedule(\n",
    "    policy.set_epsilon,\n",
    "    init_val=1.0, final_val=0.1,\n",
    "    first_step=5, final_step=18,\n",
    "    method='episode'\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer(max_size=10000, batch_size=100)\n",
    "\n",
    "trainer = Trainer(\n",
    "    algo, policy, replay_buffer, verbose=5,\n",
    "    on_begin_step=[eps_schedule],\n",
    "    on_end_episode=[lambda *args: plotter.plot(qtable, qtable.to_state_values())],\n",
    ")\n",
    "\n",
    "trainer.train(env, max_episodes=20, max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv(show_path=True)\n",
    "qtable_control(env, qtable, max_steps=100)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a0ec31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "b08d363ebb8492a302c7076da18bf168d910622d9da13f07c6e53914cde27110"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
