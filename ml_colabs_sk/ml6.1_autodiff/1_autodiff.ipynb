{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "**POZNÁMKA: Tento notebook je určený pre platformu Google Colab. Je však možné ho spustiť (možno s drobnými úpravami) aj ako štandardný Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!apt install libgraphviz-dev pkg-config # to fix broken installation of pygraphviz\n",
    "!{sys.executable} -m pip install pygraphviz==1.7\n",
    "!{sys.executable} -m pip install git+https://gitlab.com/michalgregor/ani_torch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "from ani_torch import TorchGraph, trackable_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# hide a PYDEV warning triggered by the use of sys.gettrace in Google Colab\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='PYDEV DEBUGGER WARNING:.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "## Automatická diferenciácia pomocou PyTorch\n",
    "\n",
    "Ak vezmeme do úvahy, že väčšina súčasných metód strojového učenia je založená na optimalizácii a mnoho populárnych optimalizačných metódy využíva gradient (vrátane tých, čo sa využívajú v hlbokom učení), potrebujeme byť gradient schopní čo najjednoduchšie a najefektívnejšie vypočítať.\n",
    "\n",
    "Automatická diferenciácia (autodiff; v teórii umelých neurónových sietí aj pod názvom metóda spätného šírenia chyby: backprop), je metóda, ktorá si pri výpočte gradientu zostaví graf výrazu, ktorý potom spustí dopredne (na výpočet výstupu) a spätne (na prešírenie gradientov z výstupu späť na vstup). Autodiff vie preto vypočítať gradient za cenu približne dvoch dopredných behov. Tento prístup je neporovnateľne efektívnejší než metódy, o ktorých sme hovorili doteraz: numerická diferenciácia a symbolická diferenciácia.\n",
    "\n",
    "### Výpočtový graf a gradient\n",
    "\n",
    "V nástroji PyTorch sa výpočtový graf konštruuje automaticky, spustením štandardného imperatívneho kódu, ale na špeciálnych objektoch. Namiesto klasických polí sa používajú PyTorch tenzory. Rovnako namiesto `numpy` operácií ako sú `np.cos` alebo `np.exp` sa požívajú PyTorch ekvivalenty `torch.cos` a `torch.exp`. Inak bude kód vyzerať prakticky rovnako.\n",
    "\n",
    "Začneme definovaním jednoduchej PyTorch funkcie, ktorá navráti $\\cos(ax + c)$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, a, c):\n",
    "    y = torch.sin(a*x + c)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Aby sme funkciu mohli spustiť, potrebujeme si už len vytvoriť PyTorch tenzory. Tie sa dajú vytvoriť konverziou zo štandardných Python-ových dátových typov, prípadne aj numpy polí. Aby sme mohli určiť gradient vo vzťahu ku jednotlivým vstupom, musíme však podstúpiť dva kroky: zabezpečiť, aby mali tenzory float-ový typ a aby mali parameter `requires_grad` nastavený na `True`. Druhá podmienka vyplýva zo snahy vyhnúť sa nepotrebným výpočtom: málokedy je totiž potrebné určiť gradient vo vzťahu ku všetkým premenným.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2, dtype=float, requires_grad=True)\n",
    "a = torch.tensor(3, dtype=float, requires_grad=True)\n",
    "c = torch.tensor(4, dtype=float, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Ďalej funkciu spustíme na našich tenzoroch a výstup si uložíme. Môžeme ho tiež priamo vypísať.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = func(x, a, c)\n",
    "print(y.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Na spustenie spätného behu, ktorým sa vypočítajú gradienty, stačí zavolať funkciu `y.backward()`. Gradienty sa tým prešíria na naše vstupné tenzory a vieme k nim pristupovať cez atribút `.grad`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(a.grad)\n",
    "print(c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Vizualizácia výpočtového grafu\n",
    "\n",
    "Ďalej použijeme pomocnú knižnicu na zobrazenie výpočtového grafu. Táto knižnica nie je súčasťou PyTorch: použijeme ju len na lepšiu ilustráciu toho, ako automatická diferenciácia funguje. Jediné čo treba urobiť je, že z našej funkcie a niekoľkých vstupných hodnôt vytvoríme objekt typu `TorchGraph` (vstupy môžu byť čísla alebo numpy polia – do PyTorch tenzorov sa obalia automaticky).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = TorchGraph(func, [2, 3, 4])\n",
    "graph.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "#### Vizualizácia dopredného a spätného behu\n",
    "\n",
    "Čo je ešte dôležitejšie, vieme pomocou animovaného obrázka vizualizovať dopredný a spätný beh autodiff-u. To nám umožní vytvoriť vizuálne vysvetlenia toho, ako spätné šírenie gradientov funguje.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.animate(direction=\"forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.animate(direction=\"backward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Šírenie gradientu pre niekoľko častých prípadov\n",
    "\n",
    "Zrejme najjednoduchším spôsobom ako porozumieť fungovaniu metódy autodiff, je prejsť si niekoľko častých prípadov ako sú ščítavanie, násobenie a pod. a vysvetliť ako sa v nich gradient šíri.\n",
    "\n",
    "#### Sčítavanie: distribúcia gradientu\n",
    "\n",
    "Najjednoduchším prípadom je zrejme sčítavanie: gradient z výstupu sa jednoducho distribuuje do oboch vstupných vetiev.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_add(a, b):\n",
    "    y = a + b\n",
    "    return y\n",
    "\n",
    "graph = TorchGraph(func_add, [2, 3])\n",
    "graph.plot(with_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = TorchGraph(func_add, [2, 3], [2])\n",
    "graph.plot(with_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "#### Súčin: výmena a násobenie\n",
    "\n",
    "Pri súčine sa jednoducho medzi sebou vymenia vstupy z dopredného behu (a samozrejme sa násobia gradientmi z výstupu ako to vyplýva z reťazového pravidla).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_mult(a, b):\n",
    "    y = a * b\n",
    "    return y\n",
    "\n",
    "graph = TorchGraph(func_mult, [2, 3])\n",
    "graph.plot(with_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = TorchGraph(func_mult, [2, 3], [2])\n",
    "graph.plot(with_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "#### Vetvenia: akumulácia gradientov\n",
    "\n",
    "Kedykoľvek sa v grafe vyskytne vetvenie a tá istá premenná sa použije viackrát, gradienty zo všetkých vetiev sa v spätnom behe akumulujú.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_branch(x):\n",
    "    y1 = torch.sqrt(x)\n",
    "    y2 = torch.sqrt(x)\n",
    "    return y1, y2\n",
    "\n",
    "graph = TorchGraph(func_branch, [4], [4, 8])\n",
    "graph.plot(with_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "#### Operátor `max`: gradientný prepínač\n",
    "\n",
    "Operátor `max` sa často používa ako podvzorkovacia operácia v hlbokých konvolučných sieťach. Ako sa cezeň šíria gradienty? Je zrejmé, že výstup operátora závisí len od najväčšieho vstupu. Na ten vstup sa prešíri celý gradient z výstupu. Gradienty vo vzťahu ku ostatným vstupom sú nulové: ich zmena nemá na výstup žiadny vplyv.\n",
    "\n",
    "Dalo by sa samozrejme namietať, že hodnoty ostatných vstupov budú mať vplyv na výstup ak niektorá z nich narastie natoľko, že sa stane najväčšou. Musíme však pamätať na to, že pri výpočte gradientov nás zaujímajú nekonečne malé zmeny and nekonečne malá zmena vstupu nespôsobí, že hodnota jedného vstupu prekročí hodnotu iného.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_branch(a, b):\n",
    "    y = torch.max(a, b)\n",
    "    return y\n",
    "\n",
    "graph = TorchGraph(func_branch, [2, 5], [2])\n",
    "graph.plot(with_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Definícia nových operácií\n",
    "\n",
    "Aby sme ešte úplnejšie pochopili, ako autodiff funguje, implementujeme si vlstnú novú operáciu: sigmoidnú funkciu. Jej matematická definícia je nasledovná:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation}\n",
    "a jej derivácia je:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\n",
    "\\end{equation}\n",
    "Našu novú funkciu zadefinujeme ako podtriedu `torch.autograd.Function`. Definujeme v nej \n",
    "dve statické metódy (ak nerozumiete, čo to znamená, netrápte sa: len pridajte na príslušné miesto dekorátor `@staticmethod`):\n",
    "\n",
    "* **forward:**  táto metóda realizuje dopredný beh;\n",
    "* **backward:**  táto metóda realizuje spätné šírenie gradientov z výstupov našej funkcie na jej vstupy.\n",
    "Je zrejmé, že výstup dopredného behu by sa dal v našom prípade opakovane použiť pri výpočte spätného behu, takže by sa nemusela opakovanie počítať tá istá výpočtovo náročná nelineárna funkcia. Výstup z dopredného behu si môžeme uložiť v kontextovom objekte `ctx` pomocou metódy `ctx.save_for_backward`.\n",
    "\n",
    "Napokon dekorujeme aj samotnú triedu pomocou dekorátora `@trackable_function`. Tento dekorátor nie je súčasťou nástroja PyTorch: pridávame ho, aby sa naša nová funkcia dala vizualizovať. Tiež ju pomocou `\"name = $\\sigma$\"` pomenúvame: jej názov sa bude vizualizovať ako $\\sigma$ a nie ako `sigmoid`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trackable_function\n",
    "class Sigmoid(torch.autograd.Function):\n",
    "    name = \"$\\sigma$\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        y = 1 / (1 + torch.exp(-x))\n",
    "        ctx.save_for_backward(y)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        y, = ctx.saved_tensors\n",
    "        grad_input = y * (1 - y) * grad_output\n",
    "        return grad_input\n",
    "\n",
    "sigmoid = Sigmoid.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_sigmoid(x):\n",
    "    y = sigmoid(x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = TorchGraph(func_sigmoid, [2])\n",
    "graph.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "---\n",
    "### Úloha 1: Použitie autodiff-u na funkciu\n",
    "\n",
    "**Aplikujte autodiff na nasledujúcu funkciu:** \n",
    "$$\n",
    "y = a \\sin(bx) + c\n",
    "$$\n",
    "\n",
    "**pričom** \n",
    "$$\n",
    "a=5, b=4, c=7, x=2.\n",
    "$$\n",
    "**Aký je gradient voči $x$?** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def func(x, a, b, c):\n",
    "    \n",
    "    \n",
    "    # ---\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "# ---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a351393365bb1b108989afa08de3243f72f5e58927baf5d192f3cca79a41cbc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
