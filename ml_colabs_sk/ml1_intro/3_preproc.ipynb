{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "**POZNÁMKA: Tento notebook je určený pre platformu Google Colab. Je však možné ho spustiť (možno s drobnými úpravami) aj ako štandardný Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bs97NTOSoeVF"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0JZDvAVnoop7",
    "outputId": "b53da33f-e828-4921-d40e-a1c5f9a9a50a"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "DATA_HOME = \"https://github.com/michalgregor/ml_notebooks/blob/main/data/{}?raw=1\"\n",
    "\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(DATA_HOME.format(\"titanic.zip\"), directory=\"data/titanic\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "## Predspracovanie dát\n",
    "\n",
    "V predchádzajúcich príkladoch sme pracovali s dátovou množinou Iris. Keďže táto dátová množina obsahuje len 4 stĺpce a dáta v nich sú numerické a s približne rovnakou škálou, nebolo ich potrebné žiadnym zvláštnym spôsobom predspracovať.\n",
    "\n",
    "V praxi sa takéto prípady vyskytujú veľmi zriedkakedy. Príprava dátovej množiny väčšinou zaberie omnoho viac vývojárskeho času než aplikácia a ladenie samotného modelu. Dáta treba typicky vyčistiť, ošetriť chýbajúce hodnoty, vhodne preškálovať, v niektorých prípadoch je potrebné prekódovať kategorické premenné a pod.\n",
    "\n",
    "V tomto notebook-u si ukážeme, ako fungujú niektoré základné typy predspracovania a ako sa dá fáza predspracovania navrhnúť tak, aby sa dal celý postup ľahko reprodukovať pre nové dáta.\n",
    "\n",
    "Celkový postup pri tréningu modelu sa dá zhrnúť takto:\n",
    "\n",
    "* Načítame dátovú množinu.\n",
    "* Oddelíme tréningové a testovacie dáta (ak ešte nie sú rozdelené).\n",
    "* Vyčístíme a predspracujeme ju, napr.:* Ošetríme chýbajúce hodnoty.\n",
    "* Preškálujeme numerické dáta do vhodných rozsahov.\n",
    "* Prekódujeme kategorické premenné z textovej do číselnej reprezentácie.\n",
    "\n",
    "* Natrénujeme model na tréningových dátach.\n",
    "* Otestujeme zovšeobecnenie na testovacích dátach.\n",
    "### Načítanie dátovej množiny Titanic\n",
    "\n",
    "Ako príklad na predspracovanie použijeme známu dátovú množinu [Titanic](https://www.kaggle.com/c/titanic). Dátová množina obsahuje údaje o pasažieroch z Titanicu. Úlohou je predikovať, ktorí haváriu prežili a ktorí nie. Aby sme získali predstavu, s akými údajmi budeme približne pracovať, zobrazme si najprv stručný opis dát zo súboru `description.txt`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/titanic/description\", \"r\") as file:\n",
    "    print(\"\".join(file.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Ďalej načítajme z CSV súboru samotnú dátovú množinu a rozdeľme si ju na tréningovú a testovaciu časť. Stratifikujeme podľa triedy (či sa pasažier zachránil alebo nie):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/titanic/train.csv\")\n",
    "df_train, df_test = train_test_split(df, test_size=0.25,\n",
    "                     stratify=df[\"Survived\"], random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Jednoduché predspracovanie\n",
    "\n",
    "#### Škálovanie numerických vstupov\n",
    "\n",
    "Pre mnohé metódy strojového učenia je vhodné numerické dáta najprv preškálovať do nejakého štandardného rozsahu – inak môže metóda dátam, ktoré majú väčšiu relatívnu škálu, prikladať väčšiu váhu, čo typicky nie je žiaduce. Existuje viacero typov takého škálovania, často sa používa napríklad:\n",
    "\n",
    "* Škálovanie do rozsahu od 0 po 1 (dá sa použiť `sklearn.preprocessing.MinMaxScaler`);\n",
    "* Štandardizácia (stredná hodnota sa posunie na nulu a rozptyl preškáluje na 1; `sklearn.preprocessing.StandardScaler`);\n",
    "* ...\n",
    "Ukážme si príklad štandardizácie (iné typy škálovania sa používajú obdobným spôsobom) – dajme tomu, že chceme štandardizovať stĺpec `Fare`. V rámci balíčka `scikit-learn`, s ktorým budeme pracovať, sa všetky podobné operácie realizujú pomocou štandardného rozhrania – tzv. transformátorov. Každý transformátor sa najprv skonštruuje a potom sa dá naladiť podľa dát pomocou metódy `fit`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[['Fare']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Transformované dáta je možné získať pomocou metódy `transform`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_scaled = scaler.transform(df_train[['Fare']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Ak chceme dáta použiť na naladenie transformátora, ale zároveň chceme tie isté dáta aj preškálovať, je k dispozícii aj kombinovaná funkcia `fit_transform` – v našom prípade bude teda lepšie použiť tú:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "fare_scaled = scaler.fit_transform(df_train[['Fare']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Zobrazme si teraz časť pôvodného a transformovaného stĺpca, aby sme videli, či náš transformátor funguje:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    np.hstack([df_train[['Fare']], fare_scaled]),\n",
    "    columns=[\"Fare\", \"Fare Scaled\"]\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Môžeme sa tiež pozrieť na stredné hodnoty a rozptyly, aby sme zistili, či sa naozaj zmenili tak, ako sme predpokladali:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean of fare: {}\\nVariance of fare: {}\".format(\n",
    "    np.mean(df_train['Fare']),\n",
    "    np.var(df_train['Fare'])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean of scaled fare: {}\\nVariance of scaled fare: {}\".format(\n",
    "    np.mean(fare_scaled),\n",
    "    np.var(fare_scaled)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "#### Prekódovanie kategorických premenných\n",
    "\n",
    "V dátových množinách sa často vyskytujú kategorické premenné, ktoré nadobúdajú určitý pomerne malý počet diskrétnych hodnôt, reprezentovaných textovými reťazcami. V našom prípade patria medzi takéto premenné napr. `Embarked` (prístav, kde cestujúci nastúpil na loď) a `Sex` (pohlavie cestujúceho).\n",
    "\n",
    "Takéto premenné môže byť (v závislosti od použitej metódy) potrebné transformovať z textových reťazcov na numerické identifikátory (každej textovej hodnote sa priradí nejaké číslo). V Python-e to je možné realizovať úplne obdobne ako preškálovanie numerického atribútu – iba sa použije iný transformátor: `OrdinalEncoder`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenc = OrdinalEncoder()\n",
    "sex_encoded = ordenc.fit_transform(df_train[[\"Sex\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Znovu si pre porovnanie zobrame pôvodný aj prekódovaný stĺpec:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    np.hstack([df_train[[\"Sex\"]], sex_encoded]),\n",
    "    columns=[\"Sex\", \"Sex Encoded\"]\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Ako vidno, pohlavie `female` bolo prekódované ako 0 a pohlavie `male` ako 1. Overiť si to môžeme aj podľa poradia jednotlivých označení v nasledujúcom zozname:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenc.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Ošetrenie chýbajúcich hodnôt\n",
    "\n",
    "Keď sa pokúsime vyššie uvedené prístupy aplikovať na ďalšie stĺpce z dátovej množiny, zistíme, že to nie vždy funguje. Pri dátových množinách sa totiž často stáva, že niektoré údaje chýbajú (v niektorých riadkoch nie sú vyplnené všetky stĺpce). Pre väčšinu metód strojového učenia to predstavuje problém a chýbajúce hodnoty treba nejakým spôsobom ošetriť. Existujú v zásade tri skupiny prístupov ako sa s chýbajúcimi hodnotami vysporiadať:\n",
    "\n",
    "* Príslušné riadky z dátovej množiny vypustiť.\n",
    "* Chýbajúce hodnoty podľa nejakého pravidla doplniť (angl. imputation).\n",
    "* Chýbajúce hodnoty ponechať, ak sa s nimi vie vysporiadať priamo metóda strojového učenia (napr. niektoré implementácie rozhodovacích stromov).\n",
    "Úplné vypustenie riadkov sa robí väčšinou v prípade, keď sa dá predpokladať, že riadok obsahuje málo užitočných informácií (napr. chýbajú skoro všetky hodnoty) alebo keď je dát také veľké množstvo, že neúplné záznamy nie je vôbec potrebné použiť (čo sa stáva zriedkavo).\n",
    "\n",
    "Doplnenie chýbajúcich hodnôt môže byť rôzne zložité:\n",
    "\n",
    "* Veľmi jednoduché – napr. chýbajúce hodnoty sa doplnia priernou alebo najčastejšou hodnotou z daného stĺpca.\n",
    "* Veľmi zložité – napr. sa na iných stĺpcoch natrénuje celý model a ten predikuje chýbajúce hodnoty.\n",
    "* Stredne zložité...\n",
    "My si ukážeme len jeden triviálny spôsob doplnenia pre numerické a jeden pre kategorické dáta (v oboch prípadoch použijeme triedu `SimpleImputer`), ale v balíčku `scikit-learn` a inde sa dajú nájsť aj ďalšie (napr. `sklearn.impute.IterativeImputer`).\n",
    "\n",
    "#### Detekcia chýbajúcich hodnôt\n",
    "\n",
    "Predtým, ako prejdeme ku samotnému dopĺňaniu chýbajúcich hodnôt, ukážme, ako sa dajú chýbajúce hodnoty detegovať. Balíček `pandas` má na ten účel fukciu `.isnull()`, ktorá nám pre každú bunku vráti či v nej chýba hodnota alebo nie:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Age\"].isnull()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Ak chceme vedieť, či v danom stĺpci chýba aspoň jedna hodnota, môžeme zreťaziť funkciu `.isnull()` s funkciou `.any()`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Age\"].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Fare\"].isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "As we can see, some values are missing in the `Age` column, but in the `Fare` column we have values for all passengers.\n",
    "\n",
    "#### Trivial Imputation of Numeric Values\n",
    "\n",
    "A trivial way to impute missing numeric values is to use the `SimpleImputer` transformer. This transformer will replace missing values with the average for that column by default. It is, however, possible to use different strategies as well:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_impute = SimpleImputer()\n",
    "age_imputed = num_impute.fit_transform(df_train[[\"Age\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Výsledok bude vyzerať nasledovne:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    np.hstack([df_train[['Age']], age_imputed]),\n",
    "    columns=[\"Age\", \"Age Imputed\"]\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "#### Doplnenie kategorických atribútov\n",
    "\n",
    "Na doplnenie chýbajúcich hodnôt pre kategorické atribúty môžeme znovu použiť transformátor `SimpleImputer` – budeme ho len odlišne parametrizovať. Ak chceme napríklad chýbajúce hodnoty doplniť najčastejšou hodnotou atribútu:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_impute = SimpleImputer(strategy=\"most_frequent\")\n",
    "embarked_imputed = cat_impute.fit_transform(df_train[[\"Embarked\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Alternatívne môžeme kategorickému atribútu pridať novú hodnotu, ktorú nazveme `MISSING`. Tá bude indikovať, že hodnota chýbala:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_impute = SimpleImputer(strategy='constant', fill_value='MISSING')\n",
    "embarked_imputed = cat_impute.fit_transform(df_train[[\"Embarked\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Opakovateľnosť predspracovania\n",
    "\n",
    "Prirodzene, že keď navrhneme fázu predspracovania, boli by sme radi, keby sme ju mohli následne rovnako aplikovať aj na testovacie dáta a neskôr, po nasadení modelu, na všetky nové dáta. Jedna vec, na ktorú si pritom treba dať pozor je, že parametre transformátorov ladíme na dátach. Na predspracovanie testovacích a ďalších dát musíme použiť rovnako naladené transformátory, inak dostaneme iné výsledky a náš model nebude správne fungovať. Mohlo by sa napríklad stať, že tá istá kategorická hodnota sa v rámci trénovacej množiny prekóduje ako 3 a v rámci testovacej ako 1.\n",
    "\n",
    "#### Nesprávny spôsob predspracovania\n",
    "\n",
    "Ukážme si malý príklad nesprávneho prípadu predspracovania. Dajme tomu, že autor kódu chcel dosiahnuť opakovateľnosť a preto kód na predspracovanie obalil do funkcie, ktorú volá najprv z tréningovými a neskôr s testovacími dátami. Zabudol však, že vnútri funkcie sa ladia parametre transformátorov, ktoré sa naladia zakaždým inak.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    num_impute = SimpleImputer()\n",
    "    age_imputed = num_impute.fit_transform(df[['Age']])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    age_scaled = scaler.fit_transform(age_imputed)\n",
    "    \n",
    "    return age_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_preproc = preprocess(df_train)\n",
    "df_test_preproc = preprocess(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Výstup bude vyzerať nasledovne. Malo by byť z neho vidno, že hodnota 26 sa v každom prípade prekódovala na iné číslo, čo je samozrejme neprijateľné.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    np.hstack([df_train[['Age']], df_train_preproc]),\n",
    "    columns=[\"Age\", \"Scaled Age\"]\n",
    ").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    np.hstack([df_test[['Age']], df_test_preproc]),\n",
    "    columns=[\"Age\", \"Scaled Age\"]\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "#### Správny postup\n",
    "\n",
    "Správne sa musí ten istý transformátor, naladený na tréningových dátach, aplikovať aj na testovacie dáta. Dalo by sa to realizovať napríklad takto:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, params=None):\n",
    "    if params is None:\n",
    "        params = {}\n",
    "        \n",
    "        params[\"num_impute\"] = SimpleImputer()\n",
    "        age_imputed = params[\"num_impute\"].fit_transform(df[['Age']])\n",
    "        \n",
    "        params[\"scaler\"] = StandardScaler()\n",
    "        age_scaled = params[\"scaler\"].fit_transform(age_imputed)\n",
    "        \n",
    "    else:\n",
    "        age_imputed = params[\"num_impute\"].transform(df[['Age']])\n",
    "        age_scaled = params[\"scaler\"].transform(age_imputed)\n",
    "        \n",
    "    return age_scaled, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_preproc, params = preprocess(df_train)\n",
    "df_test_preproc, params = preprocess(df_test, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Keďže oba transformátory sme si tento raz uložili a na testovacie dáta aplikovali už len pomocou funkcie `transform`, výsledky by tento raz mali byť správne.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    np.hstack([df_train[['Age']], df_train_preproc]),\n",
    "    columns=[\"Age\", \"Scaled Age\"]\n",
    ").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    np.hstack([df_test[['Age']], df_test_preproc]),\n",
    "    columns=[\"Age\", \"Scaled Age\"]\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "#### Jednoduchší prístup: scikit-learn pipelines\n",
    "\n",
    "Problém takéhoto prístupu ku predspracovaniu je, že v praxi je predspracovanie často dosť komplikované. Uchovávať si manuálne všetky transformátory, ktoré boli v rámci neho použité, a následne ich opätovne rovnakým spôsobom aplikovať je pomerne prácná úloha – a dá sa pri nej ľahko pomýliť. Preto si v ďalšom notebook-u ukážeme ako sa dá tento postup automatizovať pomocou konceptu tzv. **pipelines**  – tiež z balíčka `scikit-learn`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "1_pipelines.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
