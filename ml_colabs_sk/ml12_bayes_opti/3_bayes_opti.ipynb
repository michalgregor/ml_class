{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "**POZNÁMKA: Tento notebook je určený pre platformu Google Colab. Je však možné ho spustiť (možno s drobnými úpravami) aj ako štandardný Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from IPython.display import HTML\n",
    "from IPython.utils.capture import capture_output\n",
    "from matplotlib.animation import FuncAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(\"https://www.dropbox.com/s/u8u7vcwy3sosbar/titanic.zip?dl=1\", directory=\"data/titanic\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Auxiliary Functions -- { display-mode: \"form\" }\n",
    "\n",
    "# fix the seed of the random generator\n",
    "# np.random.seed(4)\n",
    "\n",
    "# GP-based Bayes opti\n",
    "def random_point(lbound=0, ubound=5):\n",
    "    return np.random.uniform(0, 5)\n",
    "    \n",
    "def next_point(gp, ybest, acquisition_func):\n",
    "    x = random_point()\n",
    "    \n",
    "    best_score = np.inf\n",
    "    num_restarts = 10\n",
    "    \n",
    "    for i in range(num_restarts):\n",
    "        res = minimize(lambda xx: -acquisition_func(gp,\n",
    "                           xx.reshape([-1, 1]), ybest).item(),\n",
    "                       random_point(),\n",
    "                       method='L-BFGS-B',\n",
    "                       bounds=[[0, 5]])\n",
    "\n",
    "        if res.fun < best_score:\n",
    "            best_score = res.fun\n",
    "            x = res.x\n",
    "\n",
    "    return x\n",
    "\n",
    "# plotting\n",
    "def plot_distro(\n",
    "    x, gp, intervals=True, ax=None,\n",
    "    xlabel=\"x\", ylabel=\"y\", return_preds=False\n",
    "):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    y_mean, y_std = gp.predict(x.reshape((-1, 1)), return_std=True) \n",
    "    y_mean, y_std = y_mean.reshape(-1), y_std.reshape(-1)\n",
    "    \n",
    "    ax.plot(x, y_mean, 'k', lw=3, zorder=9)\n",
    "    \n",
    "    if intervals:\n",
    "        ax.fill_between(x, y_mean - 3*y_std, y_mean + 3*y_std,\n",
    "                         alpha=0.2, color='b')\n",
    "    \n",
    "    ax.set_xlim(np.min(x), np.max(x))\n",
    "    ax.set_ylim(-5, 5)\n",
    "    ax.grid(ls='--')\n",
    "    \n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "\n",
    "    if return_preds:\n",
    "        return y_mean, y_std\n",
    "    \n",
    "def plot_distro_func_data(\n",
    "    x, gp=None, X=None, Y=None, func=None,\n",
    "    intervals=True, ax=None, return_preds=False\n",
    "):\n",
    "    preds = None\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if not gp is None:\n",
    "        preds = plot_distro(\n",
    "            x, gp, intervals=intervals, ax=ax, return_preds=return_preds\n",
    "        )\n",
    "        \n",
    "    if not func is None:\n",
    "        ax.plot(x, func(x), '--', linewidth=3)\n",
    "    \n",
    "    if not X is None and not Y is None:\n",
    "        ax.scatter(X, Y, c='r', s=50, zorder=10, edgecolors=(0, 0, 0))\n",
    "    \n",
    "    return preds\n",
    "\n",
    "def bayes_opti_anim(fig, gp, x, func, acquisition_func,\n",
    "                      X=None, Y=None, num_opti_steps=10, ax=None,\n",
    "                      save_figures=True, save_plain=True,\n",
    "                      save_plain_nofunc=True, save_nofunc=True):\n",
    "    with capture_output() as io:\n",
    "        if ax is None:\n",
    "            ax = fig.gca()\n",
    "            \n",
    "        ax.clear()\n",
    "        plot_distro_func_data(x, gp, func=func, ax=ax)\n",
    "        plt.show()\n",
    "        if save_figures:\n",
    "            fig.savefig(\"output/gp_opti_init.svg\",\n",
    "                        bbox_inches=\"tight\", pad_inches=0)\n",
    "        yield\n",
    "\n",
    "        if X is None: X = []\n",
    "        if Y is None: Y = []\n",
    "\n",
    "        x0 = random_point()\n",
    "        xbest = x0\n",
    "        ybest = func(x0)\n",
    "\n",
    "        for s in range(num_opti_steps):\n",
    "            nx = next_point(gp, ybest, acquisition_func)\n",
    "            ny = func(nx)\n",
    "\n",
    "            if ny < ybest:\n",
    "                xbest = nx\n",
    "                ybest = ny\n",
    "\n",
    "            X.append(nx)\n",
    "            Y.append(ny)\n",
    "\n",
    "            gp.fit(np.reshape(X, (-1, 1)),\n",
    "                   np.reshape(Y, (-1, 1)))\n",
    "            \n",
    "            if save_plain:\n",
    "                ax.clear()\n",
    "                plot_distro_func_data(x, gp, func=func, X=X, Y=Y, ax=ax,\n",
    "                                      intervals=False)\n",
    "                fig.savefig(\"output/gp_opti_plain_{}.svg\".format(s),\n",
    "                            bbox_inches=\"tight\", pad_inches=0)\n",
    "                \n",
    "            if save_plain_nofunc:\n",
    "                ax.clear()\n",
    "                plot_distro_func_data(x, gp, X=X, Y=Y, ax=ax,\n",
    "                                      intervals=False)\n",
    "                fig.savefig(\"output/gp_opti_plain_nofunc_{}.svg\".format(s),\n",
    "                            bbox_inches=\"tight\", pad_inches=0)\n",
    "                \n",
    "            if save_nofunc:\n",
    "                ax.clear()\n",
    "                plot_distro_func_data(x, gp, X=X, Y=Y, ax=ax)\n",
    "                fig.savefig(\"output/gp_opti_nofunc_{}.svg\".format(s),\n",
    "                            bbox_inches=\"tight\", pad_inches=0)\n",
    "\n",
    "            ax.clear()\n",
    "            plot_distro_func_data(x, gp, func=func, X=X, Y=Y, ax=ax)\n",
    "\n",
    "            plt.show()\n",
    "            if save_figures:\n",
    "                fig.savefig(\"output/gp_opti_{}.svg\".format(s),\n",
    "                            bbox_inches=\"tight\", pad_inches=0)\n",
    "            yield\n",
    "\n",
    "        ax.clear()\n",
    "        x = np.linspace(0, 5, 100)\n",
    "        plot_distro_func_data(x, gp, func=func, X=X, Y=Y, ax=ax)\n",
    "        ax.scatter([xbest], [ybest], c='cyan', marker='x', linewidth=5,\n",
    "                    s=100, zorder=10, edgecolors=(0, 0, 0))\n",
    "        plt.show()\n",
    "        if save_figures:\n",
    "            fig.savefig(\"output/gp_opti_final.svg\",\n",
    "                        bbox_inches=\"tight\", pad_inches=0)\n",
    "        yield\n",
    "        \n",
    "    print(\"solution:\", xbest, ybest)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "## Bayesovská optimalizácia\n",
    "\n",
    "### Intuícia za bayesovskou optimalizáciou\n",
    "\n",
    "Existuje veľké množstvo rozličných optimalizačných metód. Niektoré z nich, ako napr. metóda klesajúceho gradientu, sú založené na informácii o gradientoch, niektoré dokonca na deriváciách vyššieho rádu: napr. Newtonova metóda a jej mnohé aproximácie. Iné triedy optimalizačných metód zase stavajú na prehľadávaní a používajú rôzne šikovné triky, aby boli výpočtovo únosné. Ďalšia trieda zase pracuje s populáciami kandidátskych riešení a používa rôzne metaheuristické prístupy ako je napr. napodobňovanie evolúcie či iných prirodzených fenoménov.\n",
    "\n",
    "Nech si však už zvolíme ktorúkoľvek z týchto metód, existuje trieda úloh, ktoré je mimoriadne náročné vyriešiť v rozumnom čase: úlohy, kde je extrémne náročné vyhodnotiť účelovú funkciu. Nanešťastie takéto úlohy tvoria v praxi nemalú časť optimalizačných úloh. Vyskutujú sa prakticky neustále aj v kontexte strojového učenia, pretože metódy majú hyperparametre, ktoré treba typicky ladiť, ak má metóda disiahnuť optimálne výsledky. Na to, aby sa vyhodnotila vhodnosť určitej sady hyperparametrov, je však typicky potrebné natrénovať od začiatku celý model strojového učenia (čo napr. v prípade hlbokého učenia, môže niekedy trvať dni alebo dokonca týždne) a otestovať ho.\n",
    "\n",
    "#### Optimalizácia s náhradnou funkciou\n",
    "\n",
    "V takých prípadoch je nevyhnutné zabezpečiť, aby sme boli z každého vyhodnotenia účelovej funkcie schopní získať čo najviac informácie. Na tento by sme mohli použiť body, v ktorých už bola účelová funkcia vyhodnotená, a zostrojiť pomocou nich **náhradný model**  (surrogate model): funkciu, ktorú by sme potom mohli optimalizovať namiesto pôvodnej účelovej funkciu. Toto je to, čo robia metódy **optimalizácie s náhradnou funkciou**  (surrogate optimization methods).\n",
    "\n",
    "#### Bayesovský prístup ku optimalizácii s náhradnou funkciou\n",
    "\n",
    "Je tu ale ďalšia otázka. Ako vlastne vyberieme body, v ktorých účelovú funkciu vyhodnotníme, aby sme získali dáta na konštrukciu náhradného modelu? Mohli by sme azda jednoducho zvoliť zopár rovnomerne náhodných vzoriek z definičného oboru účelovej funkcie a použiť tie.\n",
    "\n",
    "Náš náhradný model však bude len taký dobrý, ako vzorky, ktoré sme si náhodne zvolili. Môže byť stále príliš nákladné zostrojiť dostatočne dobrý náhradný model a v tom prípade sa môže stať, že proces optimalizácie nájde spôsoby, ako využiť chyby modelu, aby získal vynikajúce, ale nerealistické výsledky.\n",
    "\n",
    "Našťastie však existuje lepší spôsob: náhradný model môžeme iteratívne vylepšovať. Môžeme si zvoliť prvý bod (alebo množinu bodov) rovnomerne náhodne, zostaviť okolo nich model a potom využiť informácie z neho na výber bodov, ktoré použijeme v ďalšom kroku. Začnime tým, že si vytvoríme gaussovský proces – budeme sa usilovať minimalizovať funkciu $\\sin([x-2.5]^2)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return np.sin((x - 2.5) ** 2)\n",
    "\n",
    "kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 1.0))\n",
    "gp = GaussianProcessRegressor(kernel=kernel)\n",
    "\n",
    "x = np.linspace(0, 5, 100)\n",
    "plot_distro_func_data(x, gp, func=func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Čiarkovaná čiara zodpovedá našej účelovej funkcii. Hrubá čierna čiara predstavuje aktuálnu strednú hodnotu predikovanú naším GP. Ako zvyčajne, vyfarbená plocha zodpovedá intervalu $\\pm3\\sigma$ od strednej hodnoty.\n",
    "\n",
    "Keďže sme zatiaľ nezískali žiadne pozorovania, máme len apriórne rozdelenie. Nemáme dôvod myslieť si, že tá alebo oná oblasť má vyššiu pravdepodobnosť, že obsahuje minimum. Vyberme si teda počiatočný bod rovnomerne náhodne, ako sme povedali.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [np.random.uniform(0, 5)]\n",
    "Y = [func(xx) for xx in X]\n",
    "\n",
    "gp.fit(np.reshape(X, (-1, 1)),\n",
    "       np.reshape(Y, (-1, 1)))\n",
    "\n",
    "x = np.linspace(0, 5, 100)\n",
    "plot_distro_func_data(x, gp, func=func, X=X, Y=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Zaujímavé samozrejme je, že akonáhle sme si zvolili prvý bod, už máme k dispozícii pozorovanie, ktoré môže informovať náš výber ďalšieho bodu.\n",
    "\n",
    "### Kompromis medzi prieskumom a úžitkom\n",
    "\n",
    "Keď vyberáme, ktoré ďalšie body vyhodnotiť, budeme sa snažiť udržať rovnováhu medzi dvoma podcieľmi:\n",
    "\n",
    "* **Prieskum:**  Snaha znížiť neurčitosť v oblastiach, ktoré ešte nie sú dostatočne preskúmané – aby sme sa dozvedeli či je pravdepodobné, že by mohli obsahovať minimum alebo ich môžeme vylúčiť.\n",
    "* **Úžitok:**  Snaha sústrediť sa na oblasti, o ktorých už veríme, že by mohli obsahovať minimum (t.j. o ktorých už máme dôkazy, že tam účelová funkcia nadobúda nízke hodnoty).\n",
    "To, čo sme práve opísali, sa nazýva **kompromis medzi prieskumom a úžitkom**  (the exploration vs. exploitation trade-off) a dá sa s ním stretnúť aj v iných oblastiach, napr. v učení s odmenou (reinforcement learning).\n",
    "\n",
    "Krásnou vlastnosťou bayesovských metód ako sú gaussovské procesy je, že pracujeme s celým aposteriórnym rozdelením namiesto toho, aby sme predikovali len jednu najpravdepodobnejšiu hodnotu. Vďaka tomu vieme explicitne sledovať neurčitosť a teda aj priamo udrživať rovnováhu medzi priekumom a úžitkom.\n",
    "\n",
    "Celkovo teda platí, že sa budeme na jednej strane snažiť dostať čím bližšie k minimu a na druhej strane čo najviac znížiť neurčitosť.\n",
    "\n",
    "### Optimalizácia\n",
    "\n",
    "Komponent bayesovskej optimalizácie, ktorý nám pomáha zvoliť nasledujúci bod, sa nazýva **akvizičná funkcia** . Keď už máme akvizičnú funkciu k dispozícii, môžeme na nájdenie jej optima voči náhradnému modelu použiť ľubovoľnú existujúcu optimalizačnú metódu. Jednou z najpopulárnejších akvizičných funkcií je **očakávané zlepšenie**  (expected improvement), ktoré je definované takto [[jones]](#jones):\n",
    "\n",
    "$$\n",
    "EI(\\mathbf{x}) \\equiv \\mathbb{E} \\left[ \\max(f_{\\min} - f(\\mathbf{x}), 0) \\right].\n",
    "$$\n",
    "kde $f(\\mathbf{x})$ vyjadruje hodnotu náhradnej fukcie v bode $\\mathbf{x}$ (náhodná premenná), preto potrebujeme pracovať s očakávaním; $f_{\\min}$ je najlepšia (najnižšia) hodnota, ktorú sa zatiaľ podarilo nájsť; a zlepšenie musí byť nezáporné, preto oprátor $\\max$.\n",
    "\n",
    "Očakávané zlepšenie teda vyjadruje presne to, čo napovedá jeho názov: očakávanú hodnotu zlepšenia; to znamená, rozdiel medzi najlepšou doteraz nájdenou hodnotou a hodnotou v bode $\\mathbf{x}$. Ak $f(\\mathbf{x})$ je gaussovké, t.j.:\n",
    "$f(\\mathbf{x}) \\sim \\mathcal{N}(\\mu(\\mathbf{x}), \\sigma(\\mathbf{x}))$\n",
    "(čo je v prípade gaussovských procesov samozrejme pravda), očakávané zlepšenie sa dá vyjadriť aj pohodlnejším spôsobom [[jones]](#jones):\n",
    "\n",
    "$$\n",
    "EI(\\mathbf{x}) = \\left(\n",
    "    f_{\\min} - \\mu(\\mathbf{x})\n",
    "\\right)\\;\n",
    "\\Phi \\left(\n",
    "    \\frac{\n",
    "        f_{\\min} - \\mu(\\mathbf{x})\n",
    "    }{\n",
    "        \\sigma(\\mathbf{x})\n",
    "    }\n",
    "\\right)\n",
    "+ \\sigma(\\mathbf{x}) \\phi \\left(\n",
    "    \\frac{\n",
    "        f_{\\min} - \\mu(\\mathbf{x})\n",
    "    }{\n",
    "        \\sigma(\\mathbf{x})\n",
    "    }\n",
    "\\right)\n",
    "$$\n",
    "kde $\\phi$ je gaussovská hustota pravdepodobnosti (PDF) a $\\Phi$ je gaussovská (kumulatívna) distribučná funkcia (CDF).\n",
    "\n",
    "Aby sme získali väčšiu kontrolu nad kompromisom medzi prieskumom a úžitkom, môžeme od rozdielu $f_{\\min} - \\mu(\\mathbf{x})$  odčítať ešte parameter $\\xi$ [[brochu]](#brochu):\n",
    "\n",
    "$$\n",
    "EI(\\mathbf{x}) = \\left(\n",
    "    f_{\\min} - \\mu(\\mathbf{x}) - \\xi\n",
    "\\right)\\;\n",
    "\\Phi \\left(\n",
    "    \\frac{\n",
    "        f_{\\min} - \\mu(\\mathbf{x}) - \\xi\n",
    "    }{\n",
    "        \\sigma(\\mathbf{x})\n",
    "    }\n",
    "\\right)\n",
    "+ \\sigma(\\mathbf{x}) \\phi \\left(\n",
    "    \\frac{\n",
    "        f_{\\min} - \\mu(\\mathbf{x}) - \\xi\n",
    "    }{\n",
    "        \\sigma(\\mathbf{x})\n",
    "    }\n",
    "\\right)\n",
    "$$\n",
    "Rozdiel $f_{\\min} - \\mu(\\mathbf{x})$ je výkonnostný člen, ktorý určuje, aký dobrý je bod $\\mathbf{x}$ – o koľko lepší sa javí byť (v zmysle strednej hodnoty $f(\\mathbf{x}$) oproti najlepšej doteraz nájdenej hodnote. Zvyšok kritéria máme na to, aby sme do úvahy vzali aj rozptyl (neurčitosť). Odčítaním $\\xi$ teda znižujeme váhu výkonnostného členu a tým podporujeme prieskum.\n",
    "\n",
    "Zdá sa, že vo väčšine prípadov funguje dobre hodnota $\\xi = 0.01$, prípadne škálovaná rozptylom signálu. Žíhaním $\\xi$ (aby sa podporil prieskum na začiatku a úžitok ku koncu) sa prekvapivo autorom nepodarilo dosiahnuť dobré empirické výsledky [[jones]](#jones).\n",
    "\n",
    "---\n",
    "### Úloha 1: Implementujte očakávané zlepšenie\n",
    "\n",
    "**Implementujte akvizičnú funkciu očakávané zlepšenie prepísaním nižšie uvedených vzorcov do zdrojového kódu v Python-e. Použite rozhranie funkcie predpísané v nasledujúcej bunke.* \n",
    "\n",
    "Poznámka: na výpočet gaussovskej PDF a CDF môžete použiť funkcie `norm.pdf` a `norm.cdf`.\n",
    "\n",
    "$$\n",
    "EI(\\mathbf{x}) = \\left(\n",
    "    f_{\\min} - \\mu(\\mathbf{x}) - \\xi\n",
    "\\right)\\;\n",
    "\\Phi \\left(\n",
    "    \\frac{\n",
    "        f_{\\min} - \\mu(\\mathbf{x}) - \\xi\n",
    "    }{\n",
    "        \\sigma(\\mathbf{x})\n",
    "    }\n",
    "\\right)\n",
    "+ \\sigma(\\mathbf{x}) \\phi \\left(\n",
    "    \\frac{\n",
    "        f_{\\min} - \\mu(\\mathbf{x}) - \\xi\n",
    "    }{\n",
    "        \\sigma(\\mathbf{x})\n",
    "    }\n",
    "\\right)\n",
    "$$\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def expected_improvement(gp, x, fmin, explo_rate=0.01):\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu = mu.reshape(-1); sigma = sigma.reshape(-1)\n",
    "    \n",
    "\n",
    "    \n",
    "    # ---\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Napokon použijeme funkciu `expected_improvement` s preddefinovanou pomocnou funkciou `bayes_opti_anim`, ktorá sa postará o optimalizáciu akvizičnej funkcie vo vzťahu ku náhradnej funkcii pomocou metódy LBFGS a následne iteratívne aktualizuje náhradnú funkciu pomocou novozískaných bodov. Výsledky sú prezentované formou animovaného obrázka. Mali by sme vidieť, ako sa účelová funkcia postupne preskúma a nájde sa vhodné minimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 1.0))\n",
    "gp = GaussianProcessRegressor(kernel=kernel)\n",
    "\n",
    "fig = plt.figure()\n",
    "x = np.linspace(0, 5, 100)\n",
    "frames = lambda: bayes_opti_anim(fig, gp, x, func=func,\n",
    "               acquisition_func=expected_improvement,\n",
    "               num_opti_steps=10)\n",
    "\n",
    "anim = FuncAnimation(fig, func=lambda x: None,\n",
    "                     interval=200, frames=frames)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Vizualizácia očakávaného zlepšenia\n",
    "\n",
    "Aby sme získali o trochu lepšiu intuíciu o tom, čo naša akvizičná funkcia (očakávané zlepšenie) vyjadruje, vizualizujme si ju pre jednoduchý GP podmienený pár dátovými bodmi. Miera prieskumu je nastavená na $\\xi=0.01$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [0.5, 3.5]\n",
    "Y = [func(xx) for xx in X]\n",
    "\n",
    "kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 1.0))\n",
    "gp = GaussianProcessRegressor(kernel=kernel)\n",
    "gp.fit(np.reshape(X, (-1, 1)),\n",
    "       np.reshape(Y, (-1, 1)))\n",
    "\n",
    "x = np.linspace(0, 5, 100)\n",
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "explo_rate=0.01\n",
    "ac = expected_improvement(gp, x, np.min(Y), explo_rate=explo_rate)\n",
    "\n",
    "plot_distro_func_data(x, gp, X=X, Y=Y, ax=axes[0])\n",
    "axes[1].plot(x, ac)\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"EI(x)\")\n",
    "plt.grid(ls='--')\n",
    "\n",
    "fig.savefig(\"output/expected_improvement.svg\",\n",
    "            bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Porozumenie funkcii očakávaného zlepšenia\n",
    "\n",
    "Teraz sa pokúsme porozumieť trochu lepšie matematike, na ktorej je založené očakávané zlepšenie:\n",
    "\n",
    "$$\n",
    "EI(\\mathbf{x}) \\equiv \\mathbb{E} \\left[ \\max(f_{\\min} - f(\\mathbf{x}) - \\xi, 0) \\right].\n",
    "$$\n",
    "Prejdime si ju celú postupne.\n",
    "\n",
    "#### Samotné zlepšenie\n",
    "\n",
    "Prvou súčasťou vzorca je samotné zlepšenie vyjadrené ako $f_{\\min} - f(\\mathbf{x})$. Čím väčší tento rozdiel je, tým lepšie je $f(\\mathbf{x})$ než predošlá najlepšia hodnota.\n",
    "\n",
    "#### Ohraničenie zlepšenia zdola\n",
    "\n",
    "Člen $\\max(\\text{improvement}, 0)$ ohraničuje zlepšenie zdola na nule. Znamená to v podstate, že ignorujeme všetky prípady, kedy je $f(\\mathbf{x})$ v skutočnosti horšie než $f_{\\min}$ a považujeme tam hodnotu zlepšenia za 0, hoci inak by bola záporná.\n",
    "\n",
    "Ako ukazuje nasledujúci obrázok, znamená to, že berieme do úvahy len plochu **pod čiarkovanou červenou čiarou**  (v skutočnosti úplne všetko pod čiarkovanou čiarou, ale vyfarbený interval $\\pm 3\\sigma$ obsahuje väčšinu pravdepodobnosti). Toto je jediná oblasť, ktorá má akýkoľvek nenulový príspevok.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Expected improvement + visualization of clipping explo_rate=0.01 -- { display-mode: \"form\" }\n",
    "X = [0.5, 3.5]\n",
    "Y = [func(xx) for xx in X]\n",
    "\n",
    "kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 1.0))\n",
    "gp = GaussianProcessRegressor(kernel=kernel)\n",
    "gp.fit(np.reshape(X, (-1, 1)),\n",
    "       np.reshape(Y, (-1, 1)))\n",
    "\n",
    "x = np.linspace(0, 5, 100)\n",
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "explo_rate=0.01\n",
    "ac = expected_improvement(gp, x, np.min(Y), explo_rate=explo_rate)\n",
    "\n",
    "y_mean, y_std = plot_distro_func_data(x, gp, X=X, Y=Y, ax=axes[0], return_preds=True)\n",
    "axes[1].plot(x, ac)\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"EI(x)\")\n",
    "plt.grid(ls='--')\n",
    "\n",
    "f_min_xi = np.min(Y) - explo_rate\n",
    "axes[0].axhline(y=f_min_xi, c='r', ls='--', zorder=10, label=\"$f_{\\min}$\")\n",
    "axes[0].fill_between(x, np.minimum(y_mean - 3*y_std, f_min_xi), f_min_xi,\n",
    "                     color='r', alpha=0.3, zorder=5, label=\"unclipped area\")\n",
    "axes[0].legend()\n",
    "\n",
    "fig.savefig(\"output/expected_improvement_shaded_001.svg\",\n",
    "            bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "#### Očakávaná hodnota\n",
    "\n",
    "Zvyšok je v skutočnosti už jednoduchý – berieme očakávanú hodnotu zo zlepšení pod čiarkovanou čiarou, takže budú preferované oblasti, kde náš konfidenčný interval siaha dolu k optimálnejším hodnotám.\n",
    "\n",
    "#### Miera prieskumu\n",
    "\n",
    "Aby bolo zrejmejšie, aký vplyv má miera prieskumu $\\xi$, zvýšime ju teraz z $0.01$ na $\\xi = 0.75$. Ako vidno, čiarková čiara sa tým posúva smerom dolu. Toto pomáha prieskumu, pretože body v rámci tohto pridaného okraja sa budú ignorovať, čo samozrejme nahráva oblastiam s vyššou mierou neurčitosti, kde oblasť $\\pm 3 \\sigma$ siaha nižšie a preto sa tak veľmi neoreže.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Expected improvement + visualization of clipping with explo_rate=0.75 -- { display-mode: \"form\" }\n",
    "X = [0.5, 3.5]\n",
    "Y = [func(xx) for xx in X]\n",
    "\n",
    "kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 1.0))\n",
    "gp = GaussianProcessRegressor(kernel=kernel)\n",
    "gp.fit(np.reshape(X, (-1, 1)),\n",
    "       np.reshape(Y, (-1, 1)))\n",
    "\n",
    "x = np.linspace(0, 5, 100)\n",
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "explo_rate=0.75\n",
    "ac = expected_improvement(gp, x, np.min(Y), explo_rate=explo_rate)\n",
    "\n",
    "y_mean, y_std = plot_distro_func_data(x, gp, X=X, Y=Y, ax=axes[0], return_preds=True)\n",
    "axes[1].plot(x, ac)\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"EI(x)\")\n",
    "plt.grid(ls='--')\n",
    "\n",
    "f_min_xi = np.min(Y) - explo_rate\n",
    "axes[0].axhline(y=f_min_xi, c='r', ls='--', zorder=10, label=r\"$f_{\\min} - \\xi$\")\n",
    "axes[0].fill_between(x, np.minimum(y_mean - 3*y_std, f_min_xi), f_min_xi,\n",
    "                     color='r', alpha=0.3, zorder=5, label=\"unclipped area\")\n",
    "axes[0].legend()\n",
    "\n",
    "fig.savefig(\"output/expected_improvement_shaded_075.svg\",\n",
    "            bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "<a id=\"jones\">[jones]</a> Jones, D.R., Schonlau, M. and Welch, W.J., 1998. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13(4), pp.455-492. [https://link.springer.com/content/pdf/10.1023/A:1008306431147.pdf](https://link.springer.com/content/pdf/10.1023/A:1008306431147.pdf)\n",
    "\n",
    "<a id=\"brochu\">[brochu]</a> Brochu, E., Cora, V.M. and De Freitas, N., 2010. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599. [https://arxiv.org/abs/1012.2599](https://arxiv.org/abs/1012.2599)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
