{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 726
    },
    "id": "9mtn2wt7AXaf",
    "outputId": "bc19c4ba-7529-4087-eb3e-5200a48916ca"
   },
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nkv_PMkAAonz"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMay2AYBAbji"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(\"https://github.com/MehmetFiratKomurcu/IMDBReviewClassification/raw/master/imdb_master.csv\", directory=\"data\")\n",
    "nltk.download(['punkt', 'stopwords', 'wordnet'])\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "## Naivný bayesovský klasifikátor\n",
    "\n",
    "V tomto notebook-u sa budeme venovať naivnému bayesovskému klasifikátoru. Budeme ho aplikovať na jednoduchú úlohou spracovania prirodzeného jazyka. Existuje dátová množina filmových recenzií zoscrapovaná, spoločne s numerickými hodnoteniami, zo [stránky IMDB](https://www.imdb.com/). Numerické hodnotenia boli transformované do dvoch tried, ktoré indikujú pozitívnu alebo negatívnu recenziu. Ukážeme, prečo je naivný bayesovský klasifikátor dobrým kandidátom na riešenie takých úloh aj napriek jeho extrémne zjednodušujúcim predpokladom.\n",
    "\n",
    "### Načítanie dátovej množiny\n",
    "\n",
    "Na začiatok načítame dátovú množinu z CSV súboru. Špecifikujeme znakovú sadu ISO-8859-1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "V_jS-zVGAolP",
    "outputId": "17b561c9-d86e-4cb2-fb57-5f59cc610555"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/imdb_master.csv\", encoding=\"ISO-8859-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Dátová množina obsahuje stĺpec, ktorý vzorky rozdeľuje na tréningové a testovacie. Dáta teda rozdelíme podľa neho týmto preddefinovaným spôsobom.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMI1aqzjBHez"
   },
   "outputs": [],
   "source": [
    "df_train = df[df['type'] == 'train']\n",
    "df_test = df[df['type'] == 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Predspracovanie textu\n",
    "\n",
    "Ďalej, keďže naivný bayesovskú klasifikátor nevie pracovať priamo ss textou, budeme každú recenziu potrebovať predspracovať do vektora fixnej dĺžky. Tento proces si prejdeme krok za krokom a pomocou jednej recenzie si budeme ilustrovať, čo sa v ňom presne deje.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = df_train['review'].iloc[2]\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "# Odstránime prípadné HTML tagy pomocou regulárnych výrazov.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_re = re.compile(r\"<[^>]*>\")\n",
    "without_tags = html_re.sub(' ', review)\n",
    "print(without_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "# Odstránime interpunkčné znamienka (náhradou všetkých znakov obsiahnutých v `string.punctuation` prázdnym reťazcom).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_punctuation = without_tags\n",
    "\n",
    "for char in string.punctuation:\n",
    "    without_punctuation = without_punctuation.replace(char, \"\")\n",
    "    \n",
    "print(without_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "# Transformujeme všetky písmená na malé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_case = without_punctuation.lower()\n",
    "print(lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "# Reťazec rozdelíme na bielych znakoch, aby sme získali jednotlivé slová.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lower_case.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "# Odstránime stop slová (pomocné slová ako \"and\", \"or\", \"the\" a pod.) a čokoľvek, čo sa neskladá výlučne z písmen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "only_useful_words = [w for w in words if w.isalpha() and not w in stop_words]\n",
    "print(only_useful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "# Slová transformujeme do kánonickej podoby pomocou lemmatizácie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "canonical = [lemmatizer.lemmatize(w) for w in only_useful_words]\n",
    "print(canonical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "# Výsledné slová spojíme späť dokopy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproced_text = \" \".join(canonical)\n",
    "print(preproced_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Definujeme funkciu `preproc_text` s tou istou logikou, ktorú následne aplikujeme na každú recenziu v dátovej množine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTyDwYm0X3-Q"
   },
   "outputs": [],
   "source": [
    "#@title -- function preproc_text -- { display-mode: \"form\" }\n",
    "html_re = re.compile(r\"<[^>]*>\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preproc_text(text):\n",
    "    text = html_re.sub(' ', text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    for char in string.punctuation:\n",
    "        text = text.replace(char, \"\")\n",
    "\n",
    "    # transform all to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # split on whitespace\n",
    "    words = text.split()\n",
    "\n",
    "    # filter out anything that is not exclusively\n",
    "    # made of letters or that is in stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if w.isalpha() and not w in stop_words]\n",
    "    \n",
    "    # lemmatize the words, turning them into canonical forms\n",
    "    canonical = [lemmatizer.lemmatize(w) for w in words]\n",
    "    \n",
    "    # join the words back together\n",
    "    preproced_text = \" \".join(canonical)\n",
    "    return preproced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieZ4kVuUiOv7"
   },
   "outputs": [],
   "source": [
    "reviews_train = [preproc_text(text) for text in df_train['review']]\n",
    "reviews_test = [preproc_text(text) for text in df_test['review']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Transformácia textu na vektor fixnej dĺžky\n",
    "\n",
    "#### Bag of words (batoh slov)\n",
    "\n",
    "Hoci sme už na texte vykonali veľa predspracovania, stále ho máme vo forme reťazca a nie vektora fixnej veľkosti. Ako ho teda vektorizujeme? Jeden spôsob je vytvoriť reprezentáciu **bag of words**  (batoh slov): jednoducho spočítať kooľko krát sa každé slovo v recenzii nachádza. Toto realizuje trieda `CountVectorizer` z balíčka scikit learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "bag_of_words = count_vectorizer.fit_transform(reviews_train)\n",
    "bag_of_words.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Bag of words je obrovská matica: s jedným stĺpcom pre každé unikátne slovo. Práve preto sme venovali toľko úsilia transformácii slov na ich kánonické formy: inak by matice boli ešte niekoľkonásobne väčšie. Platí tiež, že každá recenzia obsahuje len zlomok všetkých možných slov a väčšina prvkov teda bude nulová. Z tohto dôvodu sa bag of words reprezentácie ukladajú vo forme riedkych (sparse) matíc: uložia sa len hodnoty nenulových prvkov.\n",
    "\n",
    "#### Bag of N-grams (batoh n-gramov)\n",
    "\n",
    "Vo všeobecnosti platí, že okrem prítomnosti jednotlivých slov nás zaujíma aj ich vzájomné poradie alebo ich špecifické kombinácie. Aby sme vedeli tieto aspekty zachytiť, môžeme použiť takzvané **n-gramy** : budeme sa na slová pozerať v rámci ich n-slovných komntextov a počítať ich výskyty namiesto výskytov slov. Takto by sme to realizovali pre 2-gramy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "grams_2 = count_vectorizer.fit_transform(reviews_train)\n",
    "grams_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "Všimnite si, že matica má teraz omnoho viac stĺpcov než predtým. Je to samozrejme preto, že 2-gramových kombinácií je omnoho viac než samotných jednotlivých slov. Našťastie nie všetky kombinácie slov sa v textoch vyskytujú, takže počet 2-gramov nebude druhou mocninou počtu slov.\n",
    "\n",
    "Ak by sme chceli sledovať výskyty jednotlivých slov aj 2-gramov, môžeme tiež špecifikovať `ngram_range=(1, 2)` – takto aj naozaj budeme robiť.\n",
    "\n",
    "#### TF-IDF\n",
    "\n",
    "Napokon treba brať do úvahy to, že existujú bežné slová (n-gramy), ktoré sa vyskytujú vo veľmi veľkom počte dokumentov. Intuícia hovorí, že tieto budú pri rozlišovaní medzi triedami asi menej užitočné a preto nechceme, aby mali pri predikcii disproporčne vysoký vplyv. Preto namiesto jednoduchých počtov výskytov vypočítame **TF-IDF** : t.j. **term-frequency times inverse document-frequency**  (frekvenciu pojmu krát inverznú dokumentovú frekvenciu). Nahrubo povedané, frekvenciu (počet výskytov) každého pojmu budeme deliť celkovým počtom dokumentov, v ktorých sa vyskytuje.\n",
    "\n",
    "Presnejšie povedané, ak frekvenciu (počet výskytov) pojmu $t$ v dokumente $d$ označíme $\\text{tf}(t, d)$, inverzná dokumentová frekvencia je definovaná takto [TfidfTransformer](#TfidfTransformer):\n",
    "\n",
    "$$\n",
    "\\text{idf}(t) = \\log \\left[ \\frac{1 + n}{1 + \\text{df}(t)} \\right] + 1,\n",
    "$$\n",
    "kde $n$ je celkový počet dokumentov a $\\text{df}(t)$ je počet dokumentov obsahujúcich pojem $t$. TF-IDF je potom jednoducho [TfidfTransformer](#TfidfTransformer):\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\cdot \\text{idf}(t).\n",
    "$$\n",
    "Na to, aby sme TF-IDF získali V Python-e, použijeme z balíčka scikit learn jednoducho namiesto triedy `CountVectorizer` triedu `TfidfVectorizer`. Použime teda teraz `TfidfVectorizer` na transformáciu našich recenzií na `X_train` a `X_test`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9Ljum26gBMj"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "X_train = vectorizer.fit_transform(reviews_train)\n",
    "Y_train = df_train['label']\n",
    "\n",
    "X_test = vectorizer.transform(reviews_test)\n",
    "Y_test = df_test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Tréning modelu\n",
    "\n",
    "Teraz, keď sme predspracovali dáta, môžeme na nich natrénovať model. Ako sme videli, TF-IDF vektory sú dosť veľké: pri ich ukladaní dokonca preferujeme použitie riedkych matíc. Dôvod prečo naivný bayesovský klasifikátor nie je zlou voľbou pre takéto úlohy (napriek dosť extrémnym zjednodušujúcim predpokladom), je, že by bolo o dosť ťažšie natrénovať na dátach takých rozmerov komplexnejší model. V minulosti, s menej výkonným hardvérom, to často nebolo realistické, a vyhodnejšie to môže byť v niektorých prípadoch aj dnes – za predpokladu, že sú výsledky dostatočne dobré.\n",
    "\n",
    "Platí tiež, že každá metóda, ktorej tréning na našej dátovej množine má byť rýchly, by mala mať podporu pre riedke matice (napr. rozhodovacie stromy v balíčku scikit learn ju nemajú): ak sa ich bude snažiť konvertovať do hustej reprezentáciu, tréning bude trvať podstatne dlhšie. V balíčku scikit learn však existuje ešte zopár iných jednoduchých metód, ktoré majú podporu pre riedke matice – napríklad logistická regresia. Tie by nemalo byť omnoho ťažšie natrénovať na tých istých dátach.\n",
    "\n",
    "V každom prípade, teraz si pomocou triedy `MultinomialNB` vytvoríme a natrénujeme naivný bayesovský klasifikátor:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sk"
    ]
   },
   "source": [
    "### Testovanie\n",
    "\n",
    "Napokon sa pozrime, akú má náš model správnosť.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)\n",
    "\n",
    "cm = pd.crosstab(Y_test, y_test,\n",
    "                 rownames=['actual'],\n",
    "                 colnames=['predicted'])\n",
    "print(cm, \"\\n\")\n",
    "\n",
    "acc = accuracy_score(Y_test, y_test)\n",
    "print(\"Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "<a id=\"TfidfTransformer\">[TfidfTransformer]</a> sklearn.feature_extraction.text.TfidfTransformer. [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "IMDB_naive_bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
