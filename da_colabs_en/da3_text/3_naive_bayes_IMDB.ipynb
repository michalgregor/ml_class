{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 726
    },
    "id": "9mtn2wt7AXaf",
    "outputId": "bc19c4ba-7529-4087-eb3e-5200a48916ca"
   },
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nkv_PMkAAonz"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMay2AYBAbji"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(\"https://github.com/MehmetFiratKomurcu/IMDBReviewClassification/raw/master/imdb_master.csv\", directory=\"data\")\n",
    "nltk.download(['punkt', 'stopwords', 'wordnet'])\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## The Naïve Bayes Classifier\n",
    "\n",
    "In this notebook we will have a look at the naïve Bayes classifier. We are going to apply it to a simple natural language analysis task. There is a dataset of movie reviews scraped, along with numeric scores, from the [IMDB website](https://www.imdb.com/). The scores were transformed into two classes, indicating a positive or a negative review. We will show why the naïve Bayes classifier is a good candidate for such tasks in spite of its extremely simplifying assumptions.\n",
    "\n",
    "### Loading the Data\n",
    "\n",
    "As the first step, we will load our data from a CSV file. We specify the ISO-8859-1 charset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "V_jS-zVGAolP",
    "outputId": "17b561c9-d86e-4cb2-fb57-5f59cc610555"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/imdb_master.csv\", encoding=\"ISO-8859-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The dataset contains a column dividing the samples into train and test. We will therefore use this column to split the dataset in that predefined way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMI1aqzjBHez"
   },
   "outputs": [],
   "source": [
    "df_train = df[df['type'] == 'train']\n",
    "df_test = df[df['type'] == 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Preprocessing the Text\n",
    "\n",
    "Next, since the Naïve Bayes classifier, cannot handle the text directly, we will need to preprocess each review into a fixed-size vector. We will go over this process step by step using one review to illustrate exactly what happens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = df_train['review'].iloc[2]\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Remove any HTML tags using regular expressions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_re = re.compile(r\"<[^>]*>\")\n",
    "without_tags = html_re.sub(' ', review)\n",
    "print(without_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Remove punctuation (by replacing all characters in `string.punctuation` with an empty string).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_punctuation = without_tags\n",
    "\n",
    "for char in string.punctuation:\n",
    "    without_punctuation = without_punctuation.replace(char, \"\")\n",
    "    \n",
    "print(without_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Transform all to lower case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_case = without_punctuation.lower()\n",
    "print(lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Split to on whitespace to get at individual words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lower_case.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Remove stop words (auxiliary words such as \"and\", \"or\", \"the\", etc.) and anything that is not exclusively made of letters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "only_useful_words = [w for w in words if w.isalpha() and not w in stop_words]\n",
    "print(only_useful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Transform words into canonical forms by lemmatization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "canonical = [lemmatizer.lemmatize(w) for w in only_useful_words]\n",
    "print(canonical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# We join the resulting words back together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproced_text = \" \".join(canonical)\n",
    "print(preproced_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We define function `preproc_text` with the same logic that we will apply to each review in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTyDwYm0X3-Q"
   },
   "outputs": [],
   "source": [
    "#@title -- function preproc_text -- { display-mode: \"form\" }\n",
    "html_re = re.compile(r\"<[^>]*>\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preproc_text(text):\n",
    "    text = html_re.sub(' ', text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    for char in string.punctuation:\n",
    "        text = text.replace(char, \"\")\n",
    "\n",
    "    # transform all to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # split on whitespace\n",
    "    words = text.split()\n",
    "\n",
    "    # filter out anything that is not exclusively\n",
    "    # made of letters or that is in stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if w.isalpha() and not w in stop_words]\n",
    "    \n",
    "    # lemmatize the words, turning them into canonical forms\n",
    "    canonical = [lemmatizer.lemmatize(w) for w in words]\n",
    "    \n",
    "    # join the words back together\n",
    "    preproced_text = \" \".join(canonical)\n",
    "    return preproced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieZ4kVuUiOv7"
   },
   "outputs": [],
   "source": [
    "reviews_train = [preproc_text(text) for text in df_train['review']]\n",
    "reviews_test = [preproc_text(text) for text in df_test['review']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Transforming the Text into a Fixed-Size Vector\n",
    "\n",
    "#### Bag of Words\n",
    "\n",
    "Even though we have now done a lot of preprocessing on the text, it is still a string and not a fixed-size vector. So how do we vectorize it? One way to do this would be to create a **bag of words**  representation: simply count how many times each word is present in a review. This is what scikit-learn's `CountVectorizer` class does. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "bag_of_words = count_vectorizer.fit_transform(reviews_train)\n",
    "bag_of_words.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The bag of words is a huge matrix, with one column per each unique word. This is why we took such care transforming the words into their canonical forms: otherwise the matrices would be even several times larger. Also, given that each review just contains a fraction of all the possible words, most entries will be zeros. For this reason, the bag of words is stored as a sparse matrix: only the non-zero elements are recorded.\n",
    "\n",
    "#### Bag of N-grams\n",
    "\n",
    "Now, in general, apart from the presence of single words, we often care about their relative order and about certain combinations of words as well. To capture this, we can use the so-called **n-grams** : we will look at words in their n-word contexts and count the presence of those instead. Here is how to do that for 2-grams:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "grams_2 = count_vectorizer.fit_transform(reviews_train)\n",
    "grams_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Note how the matrix now has much more columns than before. This is because there are more 2-gram combinations than there are individual words. Thankfully, not all combinations of words occured in the texts, so the number of 2-grams is not the number of words squared.\n",
    "\n",
    "If we want to keep track of individual words as well as 2-grams, we can also specify `ngram_range=(1, 2)` – which we are going to do.\n",
    "\n",
    "#### The TF-IDF\n",
    "\n",
    "Finally, there are common words (n-grams) that occur in a large number of documents. Intuitively, these are probably going to be less useful when differentiating among classes and we do not want them to have a disproportionately large effect on the predictions. To this end, instead of simple counts, we can compute the **TF-IDF** : the **term-frequency times inverse document-frequency** . Roughly, this will divide the frequency (number of occurences) of each term by the total number of documents in which it appears.\n",
    "\n",
    "More precisely, if we denote the frequency (number of occurences) of term $t$ in document $d$ with $\\text{tf}(t, d)$, the inverse document-frequency is defined as [TfidfTransformer](#TfidfTransformer):\n",
    "\n",
    "$$\n",
    "\\text{idf}(t) = \\log \\left[ \\frac{1 + n}{1 + \\text{df}(t)} \\right] + 1,\n",
    "$$\n",
    "where $n$ is the total number of documents and $\\text{df}(t)$ is the number of documents that contain term $t$. The TF-IDF is then simply [TfidfTransformer](#TfidfTransformer):\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\cdot \\text{idf}(t).\n",
    "$$\n",
    "To get the TF-IDF in Python, we can simply use scikit-learn's `TfidfVectorizer` instead of `CountVectorizer`. We will now use `TfidfVectorizer` to transform our reviews into `X_train` and `X_test`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9Ljum26gBMj"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "X_train = vectorizer.fit_transform(reviews_train)\n",
    "Y_train = df_train['label']\n",
    "\n",
    "X_test = vectorizer.transform(reviews_test)\n",
    "Y_test = df_test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Training a Model\n",
    "\n",
    "Now that we have our data preprocessed, we can train a model on them. We have seen that the TF-IDF vectors are quite large: we even prefer to store them using sparse matrices. The reason a naïve Bayes classifier is not a bad fit for such task (in spite of its rather extreme simplifying assumptions), is that it would be quite a bit more difficult to train a complex model on data of such dimensions. In the past, with less powerful hardware, it was often not realistic and even now it may be preferable in some cases, provided the performance is sufficient.\n",
    "\n",
    "Also, any training method that intends to train fast on our dataset, will need to support sparse matrices (scikit learn's decision trees do not, for instance): if it tries to convert them into a dense format, training will take quite a bit longer. However, there is a couple of other simple methods in scikit-learn that support sparse matrices such as logistic regression. Those would probably not be wildly more difficult to train on the same data.\n",
    "\n",
    "In any case, we are now going to create and train a naïve Bayes classifier using class `MultinomialNB`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Testing\n",
    "\n",
    "Finally, let's see how accurate our model is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)\n",
    "\n",
    "cm = pd.crosstab(Y_test, y_test,\n",
    "                 rownames=['actual'],\n",
    "                 colnames=['predicted'])\n",
    "print(cm, \"\\n\")\n",
    "\n",
    "acc = accuracy_score(Y_test, y_test)\n",
    "print(\"Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "<a id=\"TfidfTransformer\">[TfidfTransformer]</a> sklearn.feature_extraction.text.TfidfTransformer. [https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "IMDB_naive_bayes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
