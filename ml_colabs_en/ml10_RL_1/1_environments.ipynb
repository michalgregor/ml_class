{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed78d5f",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de84a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/gym_plannable.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66548ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "%matplotlib inline\n",
    "from gym_plannable.env import MazeEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75876840",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Gym Environments\n",
    "\n",
    "The initial notebook in our section on reinforcement learning (RL), is not going to cover any actual reinforcement learning method – it is going to be about how we represent reinforcement learning problems using the OpenAI Gym interface (which has become something of a standard in recent years).\n",
    "\n",
    "[OpenAI's Gym](https://gym.openai.com/) is a Python package with a number of different RL environments – all provided with the same unified interface. We are going to illustrate the main features of the interface using a simple gridworld environment.\n",
    "\n",
    "### The Interface\n",
    "\n",
    "#### Resetting and Rendering\n",
    "\n",
    "To reset an environment's state and obtain an observation of it, we can call the `reset` method. The method returns two things:\n",
    "\n",
    "* **obs** : the initial observation;\n",
    "* **info** : a dictionary that may carry other environment-specific information;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f90fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b2618f",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "To get visual renderings of the environment's state, we can pass `render_mode='human'` to it upon construction.\n",
    "\n",
    "Different environments support different rendering modes – you can query the environment for modes that it supports using the `.metadata` attribute of the environment's class: e.g. `MazeEnv.metadata` here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv(render_mode='human')\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6caa3b5",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "What we can see in our case, is a simple gridworld environment, where the agent is represented by the blue circle, the start and the goal states are denoted using `S` and `G` respectively and grey squares correspond to walls.\n",
    "\n",
    "#### The Observation Space and the Action Space\n",
    "\n",
    "Every environment has an observation space (the `observations_space` attribute) and an action space (the `action_space` attribute); these are the spaces from which observations and actions are drawn.\n",
    "\n",
    "For instance, in our simple maze, the state is fully described by the agent's position in it (nothing else is subject to change). Given that the agent moves around a $10 \\times 10$ grid, any observation returned by `reset` will have to be a pair of integers in the range of $\\{0, 1, ..., 9 \\}$. This corresponds to the following observation space under OpenAI Gym:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f66e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd24917f",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Our action space consists of four actions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9132c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9087701",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "In our case their meaning is:\n",
    "\n",
    "* **0:**  up;\n",
    "* **1:**  down;\n",
    "* **2:**  left;\n",
    "* **3:**  right;\n",
    "but that meaning is, of course, not part of the action space's definition.\n",
    "\n",
    "To sample a (uniformly) random action from the action space, we can call `sample`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdac535",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a4303",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Taking Actions\n",
    "\n",
    "Actions are taken using the `step` method; the input argument is the action. What `step` returns is a tuple with the following elements:\n",
    "\n",
    "* **obs** : the new observation;\n",
    "* **reward** : the immediate reward associated with the transition;\n",
    "* **terminated** : a boolean flag indicating whether the environment has terminated (i.e. a terminal state has been reached and the environment needs to be reset);\n",
    "* **truncated** : a boolean flag indicating (as opposed to terminated) that the environment has been ended prematurely, e.g. because of an error, because a limit on the number of steps has been reached, etc. Again, the environment will need to be reset in order to move on;\n",
    "* **info** : a dictionary that may carry other environment-specific information;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9113661",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv()\n",
    "env.reset()\n",
    "\n",
    "obs, reward, terminated, truncated, info = env.step(0)\n",
    "\n",
    "env.render()\n",
    "print(f\"observation: {obs}\\nreward: {reward}\\nterminated: {terminated}\\ntruncated: {truncated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052fe4f1",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "---\n",
    "#### Task 1: Getting the Agent to the Goal State\n",
    "\n",
    "**In the cell below, construct a maze environment and feed it a sequence of actions such that the agent traverses a path from the start state to the goal state. Call render at each step so that it is possible to observe the agent's behaviour.**  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be91c1",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "env = MazeEnv(render_mode='human')\n",
    "env.reset()\n",
    "\n",
    "\n",
    "# --- env.step(     )\n",
    "# --- env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10401d0",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Plannable Environments\n",
    "\n",
    "For the gridworld environments that we are going to be using the next few notebooks, we provide an extension to the standard OpenAI Gym interface – plannable environments. In a plannable environment we are essentially given access to a distribution model of the environment: we can query for actions legal in a certain state; we can query for all possible state transitions corresponding to an action and their probabilities; we can simulate the behaviour of the environment for a number of steps without actually changing its state.\n",
    "\n",
    "As we know, a distribution model of this kind is required by dynamic programming methods, but it can also be useful under a number of other approaches, so let us have a look at what the interface looks like.\n",
    "\n",
    "#### Retrieve a Plannable State\n",
    "\n",
    "If an environment is plannable, it is going to provide the `single_plannable_state` method (or the `plannable_state` method, which, however, we would only need if we were working with a multiagent environment). This method returns the current state of the environment as a plannable state:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce511d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv(render_mode='human')\n",
    "env.reset()\n",
    "\n",
    "state = env.single_plannable_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9f2a18",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Legal Actions\n",
    "\n",
    "To inquire about which actions are legal in the current state, one can use the `legal_actions` method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d909f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.legal_actions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a0c009",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Here, strictly speaking, only two actions are legal because the agent is in a corner: it cannot go left or down.\n",
    "\n",
    "#### Simulating Transitions\n",
    "\n",
    "To iterate over all possible next states and their corresponding probabilities, given action `a` one can use the `all_next(a)` method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa4c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = state.legal_actions()[0]\n",
    "\n",
    "for next_state, prob in state.all_next(a):\n",
    "    print(f\"observation {next_state.observation()}; \" + \n",
    "          f\"reward: {next_state.reward()}; probability: {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c68c1ff",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "To sample just a single transition, one can use `next(a)`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = state.legal_actions()[0]\n",
    "next_state = state.next(a)\n",
    "print(f\"observation {next_state.observation()}; \" + \n",
    "      f\"reward: {next_state.reward()}; probability: {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e2880",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### The Done Flag\n",
    "\n",
    "To check whether a state is terminal, one can use the `is_done` method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c5441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.is_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40e1f49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
