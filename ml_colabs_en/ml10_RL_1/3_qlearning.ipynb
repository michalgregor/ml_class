{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c075e2a",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15fc464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/gym_plannable.git\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/rl_tabular.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ffd961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "%matplotlib inline\n",
    "from rl_tabular import ActionValueTable, StateValueTable\n",
    "from rl_tabular.maze_env_plots import (\n",
    "    Plotter, plot_action_values, plot_state_values)\n",
    "from rl_tabular import EpsGreedyPolicy, ReplayBuffer, QLearning\n",
    "from rl_tabular import ExponentialSchedule\n",
    "from rl_tabular import qtable_control\n",
    "from rl_tabular import Trainer, seed\n",
    "from gym_plannable.env import MazeEnv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c250e",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Q-Learning\n",
    "\n",
    "Next we are going to have a look at a temporal difference (TD) approach known as Q-learning. Q-learning is model-free and it learns the optimal **action-value**  function, so no model is required to control the agent either (unlike TD learning with a state-value function).\n",
    "\n",
    "Finally, Q-learning is an off-policy method so it can learn from experience gather using any policy – this will allow us to use it together with experience replay.\n",
    "\n",
    "### Q-learning with a Fixed Action Sequence\n",
    "\n",
    "As the first step, we are going to focus on Q-learning itself and we are going to disregard exploration. To this end, the cell below defines a fixed action sequence, which gets the agent from the start to the goal state. The sole role of our reinforcement learning agent will be to learn the sequence after experiencing it several times. We are going to see how well that is going to work.\n",
    "\n",
    "---\n",
    "### Task 1: Implement the Q-learning Update Rule\n",
    "\n",
    "**In the cell below, fill in the implementation of the Q-learning update rule.** \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta &= r_{t+1} + \\gamma \\underset{a}{\\max} \\ Q(s_{t+1}, a)-Q(s_{t},a_{t}) \\\\\n",
    "Q(s_{t},a_{t}) &\\leftarrow Q(s_{t},a_{t}) + \\alpha \\delta\n",
    "\\end{aligned}\n",
    "$$\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376d48a",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "actions = [0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 3, 3, 3, 3]\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "\n",
    "# create the environment\n",
    "env = MazeEnv() # we do not enable rendering here; we'll do it manually\n",
    "\n",
    "# set up plotting\n",
    "plotter = Plotter(env, ActionValueTable, StateValueTable, figsize=[8, 4])\n",
    "\n",
    "# set up the value function\n",
    "qtable = ActionValueTable(env.action_space.n)\n",
    "\n",
    "# the training loop\n",
    "step = 0\n",
    "for episode in range(20):\n",
    "    obs = env.reset()\n",
    "        \n",
    "    for a in actions:\n",
    "        # apply the action and observe the effect\n",
    "        obs_next, reward, done, _ = env.step(a)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # compute td: a, reward, qtable, gamma, obs, obs_next, np.max\n",
    "        td = # -----\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        qtable[obs, a] += alpha * td\n",
    "        \n",
    "        # book keeping\n",
    "        obs = obs_next\n",
    "        step += 1\n",
    "\n",
    "        # for the first 3 episodes, we also do step-wise plots\n",
    "        if episode < 3: plotter.plot(qtable, qtable.to_state_values())\n",
    "\n",
    "        # if the environment is done, conclude the episode\n",
    "        if done: break\n",
    "\n",
    "    print(f\"Episode {episode} finished after {step} steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4adac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_action_values(\n",
    "    qtable, plotter.states, env=env, render_agent=False\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plot_state_values(\n",
    "    qtable.to_state_values(), plotter.states, env=env, render_agent=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc3fafe",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "What we can observe from the visualization is that with vanilla Q-learning, the agent only updates the value of the second to last state. This makes sense because while computing the values of the previous states, we did not yet know that the agent was going to receive a reward.\n",
    "\n",
    "On the other hand, this is clearly very impractical – it means that we need to repeat the sequence around 20 times (roughly its length) before the agent manages to learn it in its entirety – that is, before the value is propagated all the way from the goal state back to the initial state.\n",
    "\n",
    "And even this is only true under the assumption that the agent is going to see the same sequence of actions every time – when in reality, the agent will actually need to explore different paths. It is this exploration aspect that we are going to have a look at next.\n",
    "\n",
    "### Q-learning with $\\varepsilon$-greedy Exploration\n",
    "\n",
    "In the previous example we were using the same fixed action sequence in every episode. Now we are going to do something a bit more realistic: we are going to use the $\\varepsilon$-greedy policy with $\\varepsilon = 0.1$ to do some exploration.\n",
    "\n",
    "For this experiment we are going to use off-the-shelf implementations of both the policy and of Q-learning. The visualization will show both the action-value function – with **coloured tiles**  visualizing **the path that the agent took**  in the last episode – and the state-value function.\n",
    "\n",
    "*Note the part that says `seed(1)`. This fixes the seed of the random number generator so that we get the same results every time the cell runs. Comment that line out if you want to check out other possible ways this can play out.* \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f4128",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "env = MazeEnv(\n",
    "    show_path=True,\n",
    "    show_path_kw=dict(show_arrows=False, show_visited=True)\n",
    ")\n",
    "\n",
    "# set up plotting\n",
    "plotter = Plotter(env, ActionValueTable, \n",
    "    (StateValueTable, {'render_kwargs': {'skip': {'player_logger'}}}),\n",
    "     figsize=[8, 4], render_agent=False\n",
    ")\n",
    "\n",
    "qtable = ActionValueTable(env.action_space.n)\n",
    "algo = QLearning(qtable, alpha=0.5, gamma=0.9)\n",
    "policy = EpsGreedyPolicy(qtable, env.action_space.n, epsilon=0.1)\n",
    "trainer = Trainer(\n",
    "    algo, policy, verbose=5, on_end_episode=[\n",
    "        lambda *args: plotter.plot(qtable, qtable.to_state_values())]\n",
    ")\n",
    "\n",
    "trainer.train(env, max_episodes=50, max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84773a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv(show_path=True)\n",
    "qtable_control(env, qtable, render=False, max_steps=100)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9fdf90",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "As you can see, the agent is walking around randomly until it discovers the goal state at which point it starts to learn a path leading to it. Thanks to the $\\varepsilon$-greedy policy it also does a bit of exploration: it sometimes deviates from what it has already learnt, which allows it to experience something else for a change.\n",
    "\n",
    "However, with $\\varepsilon = 0.1$ even after taking a number of episodes, it only really learns about a single path to the goal state and about the values of some additional states that lie very close to it.\n",
    "\n",
    "---\n",
    "### Task 2:\n",
    "\n",
    "**As your next task, try re-running the experiment with different values of $\\varepsilon$. Observe what changes as $\\varepsilon$ gets closer to 1. How much of the state space has the agent explored? What is the total number of steps that the agent took within the alloted 50 episodes?** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee19466",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb267cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv(show_path=True)\n",
    "qtable_control(env, qtable, max_steps=100)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b029aeb4",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Q-learning with Experience Replay\n",
    "\n",
    "The next problem that we need to address is the low sample-efficiency of our algorithm. Recall how we needed to traverse the same path a number of times, because each time the agent only learned to remember a single step of it. Surely there must be a way to learn about the entire path after just having seen it once.\n",
    "\n",
    "As it turns out, there are several ways to do just that – we will be using a technique known as experience replay, where we record all that the agent has experienced and then replay bits of that experience multiple times. That way we can actually propagate the value of the goal state transition all the way back to the initial state even if we have only observed the action sequence once.\n",
    "\n",
    "#### The Replay Buffer\n",
    "\n",
    "In our case we do not need to implement a replay buffer from scratch – we have an implementation ready. However, we are going to have a look at its interface. As we construct the buffer, we specify its maximum size (`max_size`). As the capacity of the buffer is reached, new experience starts to replace the oldest experience recorded in the buffer. Usually, though, if there is enough memory, the maximum size is set to a very large value so that experience is often not discarded at all.\n",
    "\n",
    "In addition to the maximum size of the buffer, we can also specify the default batch size (`batch_size`). When we call `.sample()`, we are going to get a batch contanining `batch_size` randomly sampled transitions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(max_size=10000, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adbb2fd",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "To record a transition in the replay buffer, you can call `replay_buffer.add(obs, a, reward, obs_next, done, info)`, where `a` is the action and `obs` is the pre-transition observation, while `obs_next` is the post-transition observation.\n",
    "\n",
    "---\n",
    "#### Task 3: Record Transitions\n",
    "\n",
    "**Run the environment picking actions randomly. Use `replay_buffer.add` to record a 1000 transitions in the replay buffer.** \n",
    "\n",
    "Tips:\n",
    "\n",
    "* Recall that `env.step(action)` returns the `(observation, reward, done, info)` tuple.\n",
    "* Recall also that once `done` is true, the current episode has ended and you need to start a new episode using `obs = env.reset()`.\n",
    "* Recall that you can sample a random action from the action space using `env.action_space.sample()`.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9693341e",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(max_size=10000, batch_size=100)\n",
    "\n",
    "env = MazeEnv()\n",
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "for step in range(1000):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----\n",
    "    \n",
    "    \n",
    "    \n",
    "    obs = obs_next\n",
    "        if terminated or truncated: obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f361079",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, a, reward, obs_next, done, info = replay_buffer.sample()\n",
    "\n",
    "print(f\"The replay buffer contains {len(replay_buffer)} samples.\")\n",
    "print(f\"The batch contains {len(obs)} samples.\\n\")\n",
    "\n",
    "print(f\"obs: {obs[:3]} ...\")\n",
    "print(f\"actions: {a[:3]} ...\")\n",
    "print(f\"obs_next: {obs_next[:3]} ...\")\n",
    "print(f\"done: {done[:3]} ...\")\n",
    "print(f\"info: {info[:3]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc6398b",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Q-learning with a Replay Buffer\n",
    "\n",
    "Next we are going to experiment with Q-learning using both an $\\varepsilon$-greedy policy and a replay buffer. We are again going to be using an off-the-shelf implementation.\n",
    "\n",
    "What you should see is that values propagate much faster because we are replaying the same experience again and again. We should also see that values are even being updated for states that have not been visited during the last episode. However, with $\\varepsilon = 0.1$ the agent still does not always adequately explore all areas of the state space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "env = MazeEnv(\n",
    "    show_path=True,\n",
    "    show_path_kw=dict(show_arrows=False, show_visited=True)\n",
    ")\n",
    "\n",
    "plotter = Plotter(env, ActionValueTable, \n",
    "    (StateValueTable, {'render_kwargs': {'skip': {'player_logger'}}}),\n",
    "     figsize=[8, 4], render_agent=False\n",
    ")\n",
    "\n",
    "qtable = ActionValueTable(env.action_space.n)\n",
    "algo = QLearning(qtable, alpha=0.5, gamma=0.9)\n",
    "policy = EpsGreedyPolicy(qtable, env.action_space.n, epsilon=0.1)\n",
    "replay_buffer = ReplayBuffer(max_size=10000, batch_size=100)\n",
    "\n",
    "trainer = Trainer(\n",
    "    algo, policy, verbose=5, replay_buffer=replay_buffer,\n",
    "    on_end_episode=[lambda *args: plotter.plot(qtable, qtable.to_state_values())],\n",
    ")\n",
    "\n",
    "trainer.train(env, max_episodes=20, max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db67a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv(show_path=True)\n",
    "qtable_control(env, qtable, max_steps=100)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd2b69",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Annealing the Exploration Rate\n",
    "\n",
    "As we have seen before, keeping $\\varepsilon$ too low results in too little exploration. On the other hand, if we keep $\\varepsilon$ at a very high value, the agent will keep behaving erratically even after it has approached the true action-values. As a result, it may sacrifice a lot of reward, take unnecessarily many steps, etc.\n",
    "\n",
    "A sensible thing to do, then, is to anneal the exploration rate: make the agent explore a lot at the beginning, but gradually decrease the exploration rate so that the agent keeps closer to the optimal policy after a while, exploiting and refining the acquired knowledge.\n",
    "\n",
    "A common way to anneal the exploration rate is by prescribing an exponential schedule. Here we are going to use the `ExponentialSchedule` class, where we can specify:\n",
    "\n",
    "* The initial value of the parameter;\n",
    "* The final value of the parameter;\n",
    "* The step to start the annealing at;\n",
    "* The step to stop the annealing at.\n",
    "In our case, the annealing will be based on the episode number (`method='episode'`) and not the step number. However, using the step number is also possible and probably even more common. Here is what an exponential schedule can look like:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f181b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_schedule = ExponentialSchedule(\n",
    "    None,\n",
    "    init_val=1.0, final_val=0.1,\n",
    "    first_step=5, final_step=18,\n",
    "    method='episode'\n",
    ")\n",
    "\n",
    "steps = range(eps_schedule.final_step+5)\n",
    "eps = [eps_schedule.get_value(s) for s in steps]\n",
    "plt.plot(steps, eps)\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"epsilon\")\n",
    "plt.grid(ls='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed96d5",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Having defined the schedule, we are now going to register it as a `on_begin_step` callback so that the $\\varepsilon$ keeps getting updated according to the schedule. Let's see what effect that is going to have.\n",
    "\n",
    "Given that we keep $\\varepsilon$ at $1$ for the first few episodes, the agent is going to behave in a completely random way then. This should help it acquire a lot of experience over those episodes. Afterwards, $\\varepsilon$ is going to decrease gradually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e334d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "env = MazeEnv(\n",
    "    show_path=True,\n",
    "    show_path_kw=dict(show_arrows=False, show_visited=True)\n",
    ")\n",
    "\n",
    "plotter = Plotter(env, ActionValueTable, \n",
    "    (StateValueTable, {'render_kwargs': {'skip': {'player_logger'}}}),\n",
    "     figsize=[8, 4], render_agent=False\n",
    ")\n",
    "\n",
    "qtable = ActionValueTable(env.action_space.n)\n",
    "algo = QLearning(qtable, alpha=0.5, gamma=0.9)\n",
    "policy = EpsGreedyPolicy(qtable, env.action_space.n)\n",
    "\n",
    "eps_schedule = ExponentialSchedule(\n",
    "    policy.set_epsilon,\n",
    "    init_val=1.0, final_val=0.1,\n",
    "    first_step=5, final_step=18,\n",
    "    method='episode'\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer(max_size=10000, batch_size=100)\n",
    "\n",
    "trainer = Trainer(\n",
    "    algo, policy, replay_buffer, verbose=5,\n",
    "    on_begin_step=[eps_schedule],\n",
    "    on_end_episode=[lambda *args: plotter.plot(qtable, qtable.to_state_values())],\n",
    ")\n",
    "\n",
    "trainer.train(env, max_episodes=20, max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv(show_path=True)\n",
    "qtable_control(env, qtable, max_steps=100)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a0ec31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "b08d363ebb8492a302c7076da18bf168d910622d9da13f07c6e53914cde27110"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
