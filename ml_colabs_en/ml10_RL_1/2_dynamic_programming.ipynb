{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd16497",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d85e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/gym_plannable.git\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/rl_tabular.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc7fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "%matplotlib inline\n",
    "from gym_plannable.env import MazeEnv\n",
    "from rl_tabular import StateValueTable, ActionValueTable, collect_states\n",
    "from rl_tabular.maze_env_plots import plot_action_values, plot_state_values\n",
    "from rl_tabular import vtable_control, qtable_control\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cca067",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Dynamic Programming: Value Iteration\n",
    "\n",
    "As we have already learnt, reinforcement learning is all about discovering a policy that maximizes long-term rewards. We also know that one branch of approaches to that problem relies on value functions. \n",
    "\n",
    "A *state-value function*  $V(s)$ expresses how much reward we can expect in the long term after being in state $s$ and following policy $\\pi$ thereafter [[sutton1998]](#sutton1998):\n",
    "\n",
    "$$\n",
    "V^{\\pi}(s) = \\mathbb{E}_{\\pi}\\{ R_{t}|s_{t}=s \\}\n",
    "$$\n",
    "You will also recall that, provided we have a distribution model of the environment and the state-action space is small, we can compute the state-value function using a method known as **value iteration** .\n",
    "\n",
    "We initialize the values of all states to zeros:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da90bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtable = StateValueTable()\n",
    "plot_state_values(vtable);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a13d334",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Then we apply the following rule, iterating over all states $s$ multiple times:\n",
    "\n",
    "$$\n",
    "V_{\\color{red} k+1}(s) = \\max_{a \\in A} \\sum_{s'} P_{ss'}^{a} \\left[r_{ss'}^{a} + \\gamma V_{\\color{red} k}(s') \\right].\n",
    "$$where $P_{ss'}^{a}$ is the probability of going from state $s$ to state $s'$ after taking action $a$; and $r^a_{ss'}$ is the reward received for that same transition.\n",
    "\n",
    "---\n",
    "### Task 1: Value Iteration\n",
    "\n",
    "**In the cell below fill in the value iteration rule.** \n",
    "\n",
    "$$\n",
    "V_{\\color{red} k+1}(s) = \\max_{a \\in A} \\sum_{s'} P_{ss'}^{a} \\left[r_{ss'}^{a} + \\gamma V_{\\color{red} k}(s') \\right].\n",
    "$$\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8a05df",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "env = MazeEnv()\n",
    "env.reset()\n",
    "\n",
    "# we collect all reachable states\n",
    "states = collect_states(env.single_plannable_state())\n",
    "\n",
    "for it in range(15):\n",
    "    print(f\"Iteration {it} started.\")\n",
    "    \n",
    "    for state in states:\n",
    "        if state.is_done():\n",
    "            continue\n",
    "            \n",
    "        legals = state.legal_actions()\n",
    "        maxval = -np.inf\n",
    "\n",
    "        for a in legals:\n",
    "            val = 0\n",
    "\n",
    "            for next_state, prob in state.all_next(a):\n",
    "                r = next_state.reward()                \n",
    "                \n",
    "                \n",
    "                \n",
    "                # r, gamma, prob, vtable, next_state       \n",
    "                val +=     # -----\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "            maxval = max(val, maxval)\n",
    "\n",
    "        vtable[state] = maxval\n",
    "        \n",
    "    plot_state_values(vtable, states, env=env, update_display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c700266",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Controlling the Agent using the State-Value Function\n",
    "\n",
    "If the action space is discrete and relatively small and we know the optimal value function, it is easy to derive the optimal policy from it – we simply iterate over all the actions and pick the one which is most likely to lead to a high-value next state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b067ace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv(render_mode='human', show_path=True)\n",
    "vtable_control(env, vtable, max_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c28cd5c",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Converting Between State and Action-Value Functions\n",
    "\n",
    "Given a model, a state-value function can be converted to an action-value function by applying essentially the same logic that we applied when controlling the agent using the state-value function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd088e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable = vtable.to_action_values(states)\n",
    "plot_action_values(qtable, states, action_spec=env.action_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2593d01",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "It is also possible to convert the action-value function into a state-value function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f6b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtable2 = qtable.to_state_values()\n",
    "plot_state_values(vtable2, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f306a972",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Control using the Action-Value Function\n",
    "\n",
    "Agents are easier to control using the action-value function as opposed to the state-value function. Since the action-value function gives us the value of each action directly, we merely need to pick the action with the maximum value – we no longer need a model to query for possible next states etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea10e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeEnv(render_mode='human', show_path=True)\n",
    "qtable_control(env, qtable, max_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c95e78",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "<a id=\"sutton1998\">[sutton1998]</a> SUTTON, R.S. - BARTO, A.G. Reinforcement Learning: An Introduction. [s.l.]: The MIT Press, 1998. ISBN 0262193981.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae3a8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "b08d363ebb8492a302c7076da18bf168d910622d9da13f07c6e53914cde27110"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
