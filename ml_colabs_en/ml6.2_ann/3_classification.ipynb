{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OXDlX-V-K7K3",
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform, which provides free hardware acceleration. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook, using a local GPU.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "homwYbPNTMuF",
    "outputId": "75f6a3bd-6bae-4664-8276-a2f9d39418d0"
   },
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z5iLk5oyTABe"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "ZFdF9__eTCta",
    "outputId": "5d96a127-97be-41ca-82ae-725c01612110"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(\"https://www.dropbox.com/s/v3ptdkv5fvmx5zk/iris.csv?dl=1\", directory=\"data\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_sRXDh_BK7MN",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Neural Network Classifiers\n",
    "\n",
    "This notebook deals with the application of a neural network constructed using the `PyTorch` python package to a simple classification task. We will show how a network can be created and trained. We will use a very simple architecture â€“ no convolutional layers, batch normalization or anything like that.\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "In this example, we will again be using the Iris dataset, with which we are very familiar by now. We will now load it from the CSV file and split it into the train and test folds:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "g0gqtIoXTEmo",
    "outputId": "c230f6fc-2b77-4231-9a16-375ab9900f7b"
   },
   "outputs": [],
   "source": [
    "#@title -- Loading and Splitting the dataset df_train, df_test -- { display-mode: \"form\" }\n",
    "\n",
    "# we load the data from the CSV\n",
    "df = pd.read_csv(\"data/iris.csv\")\n",
    "display(df.head())\n",
    "\n",
    "# we split it into train and test, stratifying by species\n",
    "df_train, df_test = train_test_split(df, test_size=0.25,\n",
    "                                     stratify=df['species'],\n",
    "                                     random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDhmP7paK7Mm",
    "tags": [
     "en"
    ]
   },
   "source": [
    "As usual, we sort the columns into categorical, numerical and output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wEDV74K8K7Mu"
   },
   "outputs": [],
   "source": [
    "categorical_inputs = []\n",
    "numeric_inputs = list(df.columns[:-1])\n",
    "output = [\"species\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5PONQKCuK7M7",
    "tags": [
     "en"
    ]
   },
   "source": [
    "The preprocessing that we have standardly applied up till now re-encodes categorical attributes into numbers, by assigning a number to each unique value of the attribute (using the `OrdinalEncoder` transformer). In the case of neural networks it will usually be more suitable to use one-hot encoding instead: for each categorical column there will be as many input neurons as there are distinct categorical values and exactly one out of these will be active at any given time. This kind of preprocessing can be achieved using the `OneHotEncoder` transformer. The preprocessing for numeric values can remain unchanged.\n",
    "\n",
    "We do not forget to transform the arrays into PyTorch tensors with appropriate datatypes: 32-bit floats for inputs and long ints for class labels (output). We are also going to pick a device at this point, the same way we did in the previous notebooks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hEWqq4NRK7NH"
   },
   "outputs": [],
   "source": [
    "input_preproc = make_column_transformer(\n",
    "    (make_pipeline(\n",
    "        SimpleImputer(strategy='constant', fill_value='MISSING'),\n",
    "        OneHotEncoder()),\n",
    "     categorical_inputs),\n",
    "    \n",
    "    (make_pipeline(\n",
    "        SimpleImputer(),\n",
    "        StandardScaler()),\n",
    "     numeric_inputs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-C4Q34A1K7NU"
   },
   "outputs": [],
   "source": [
    "output_preproc = OrdinalEncoder()\n",
    "\n",
    "X_train = input_preproc.fit_transform(df_train[categorical_inputs+numeric_inputs])\n",
    "Y_train = output_preproc.fit_transform(df_train[output]).reshape(-1)\n",
    "\n",
    "X_test = input_preproc.transform(df_test[categorical_inputs+numeric_inputs])\n",
    "Y_test = output_preproc.transform(df_test[output]).reshape(-1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "X_train = torch.as_tensor(X_train, dtype=torch.float32).to(device)\n",
    "Y_train = torch.as_tensor(Y_train, dtype=torch.long).to(device)\n",
    "X_test = torch.as_tensor(X_test, dtype=torch.float32).to(device)\n",
    "Y_test = torch.as_tensor(Y_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fx1fXxzoK7Nz",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Creating the Neural Network\n",
    "\n",
    "Our neural network will be very similar to that used for regression. The number of inputs will once again equal the number of columns in our dataset, while the number of outputs will now, of course, equal the number of classes, since the network is going to return their respective probabilities.\n",
    "\n",
    "You will recall that in classifiers, we generally use the softmax function as the ativation function of the output layer. This function makes sure that the outputs of this last layer always sum up to 1 so that they can be interpreted as properly normalized probabilities. It also applies a nonlinear transformation that makes it easier to get probabilities close to 1.\n",
    "\n",
    " **ATTENTION: In the case of the PyTorch framework, the softmax function is part of the ``nn.CrossEntropyLoss'' loss function, so we DO NOT ADD IT AT THE END OF OUR MODEL! We leave the last layer linear.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXI0Wb9dINWw"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.fc1(x)\n",
    "        y = torch.relu(y)\n",
    "        \n",
    "        y = self.fc2(y)\n",
    "        y = torch.relu(y)\n",
    "        \n",
    "        y = self.fc3(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zn0ySi8HK7OY",
    "tags": [
     "en"
    ]
   },
   "source": [
    "---\n",
    "### Task 1: Training the Network\n",
    "\n",
    "**In the cell below, complete the training loop and train the neural network.** \n",
    "\n",
    "The training loop is going to be pretty much the same as for regression, with the exception that now we are going to be using the `nn.CrossEntropyLoss`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kja-0c4MK_BJ",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "num_inputs = X_train.shape[1]\n",
    "num_outputs = len(np.unique(Y_train.cpu()))\n",
    "\n",
    "model = Net(num_inputs, num_outputs)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "# ----\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_train)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.grid(ls='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6lBsnW4MK7O7",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Testing\n",
    "\n",
    "Now we are again ready to test performance. We will again need to remember to put our model into evaluation mode using `model.eval()` first and running the model inside `torch.no_grad()` to skip building the computational graph.\n",
    "\n",
    "Note that what our network predicts are class probabilities. To get at the class labels, we run `argmax` on the probabilities (actually, since our network does not contain the final softmax layer, the values we get here are actually logits, not normalized probabilities, but that makes no difference when looking for the maximum) and thus identify the most probable class.\n",
    "\n",
    "#### On Training Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6hhikUlINIT"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_logit = model(X_train)\n",
    "    y_train = y_train_logit.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "QBg9CnISPHf5",
    "outputId": "24f55332-dddd-4449-f00f-ec941cd2fb81"
   },
   "outputs": [],
   "source": [
    "Y_train_cpu = Y_train.cpu()\n",
    "y_train_cpu = y_train.cpu()\n",
    "\n",
    "cm = pd.crosstab(\n",
    "    output_preproc.inverse_transform(\n",
    "        Y_train_cpu.reshape(-1, 1)).reshape(-1),\n",
    "    output_preproc.inverse_transform(\n",
    "        y_train_cpu.reshape(-1, 1)).reshape(-1),\n",
    "    rownames=['actual'],\n",
    "    colnames=['predicted']\n",
    ")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-OMdQEE4PHpt",
    "outputId": "9d1b90b6-210d-4808-9c6c-f5aaa16257ff"
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(Y_train_cpu, y_train_cpu)\n",
    "print(\"Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dTzTPfMLK7Pl",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### On Testing Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_logit = model(X_test)\n",
    "    y_test = y_test_logit.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "31Ez6D75PSb7",
    "outputId": "76e443f1-7913-418a-9d2c-8c0571f3c105"
   },
   "outputs": [],
   "source": [
    "Y_test_cpu = Y_test.cpu()\n",
    "y_test_cpu = y_test.cpu()\n",
    "\n",
    "cm = pd.crosstab(\n",
    "    output_preproc.inverse_transform(\n",
    "        Y_test_cpu.reshape(-1, 1)).reshape(-1),\n",
    "    output_preproc.inverse_transform(\n",
    "        y_test_cpu.reshape(-1, 1)).reshape(-1),\n",
    "    rownames=['actual'],\n",
    "    colnames=['predicted']\n",
    ")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5IB3nwHnPSgV",
    "outputId": "60a709bb-25ef-427b-bc83-0972862d83ae"
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(Y_test_cpu, y_test_cpu)\n",
    "print(\"Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8709uN5wK7QY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "1_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a351393365bb1b108989afa08de3243f72f5e58927baf5d192f3cca79a41cbc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
