{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WC94NpgjL_Ou",
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform, which provides free hardware acceleration. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook, using a local GPU.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PQYmhASVL_PI",
    "outputId": "fd214f1a-06e7-4763-a46a-f9dbc4a8dbf0"
   },
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "r8y8qp8gL_Pp",
    "outputId": "07b90e01-87fb-45bf-b80f-546a8f765dcc"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from class_utils import error_histogram\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "qWIal29oL_P6",
    "outputId": "756a47ce-2e26-45be-bd1d-449d32cefd47"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(\"https://www.dropbox.com/s/p5q7gzupa2ndw55/sigmoid_regression_data.csv?dl=1\", directory=\"data\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YQSbH9iwL_QK",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Neural Network Based Regression\n",
    "\n",
    "In this notebook we are going to show how a simple neural net created using `PyTorch` can be applied to a regression problem. We are going to construct a very simple multi-layer perceptron, train it and visualize the results.\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "Let's start by defining our regression problem. We will load our dataset from a CSV file â€“ the data consists of noisy samples drawn from a sigmoid (logistic) curve. Given that we have encountered such data in prior notebooks, we are not going to go over the procedure of loading and preprocessing it and so the code of the following cell is hidden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "WyM9YplXL_QU",
    "outputId": "0b29ebc5-a219-45cc-95ec-a64527f0ae39"
   },
   "outputs": [],
   "source": [
    "#@title -- Data Loading and Preprocessing; X_train, Y_train, X_test, Y_test -- { display-mode: \"form\" }\n",
    "df = pd.read_csv(\"data/sigmoid_regression_data.csv\")\n",
    "\n",
    "# we create a discretized version of the y column\n",
    "# to allow for stratification\n",
    "kbins = KBinsDiscretizer(6, encode='ordinal')\n",
    "y_stratify = kbins.fit_transform(df[['y']])\n",
    "\n",
    "# we split the dataset into train and test\n",
    "df_train, df_test = train_test_split(df, stratify=y_stratify,\n",
    "                                 test_size=0.3, random_state=4)\n",
    "\n",
    "# we specify the inputs and the outputs\n",
    "categorical_inputs = []\n",
    "numeric_inputs = ['x']\n",
    "output = ['y']\n",
    "\n",
    "# we create the pipeline\n",
    "input_preproc = make_column_transformer(\n",
    "    (make_pipeline(\n",
    "        SimpleImputer(strategy='constant', fill_value='MISSING'),\n",
    "        OneHotEncoder()),\n",
    "     categorical_inputs),\n",
    "    \n",
    "    (make_pipeline(\n",
    "        SimpleImputer(),\n",
    "        StandardScaler()),\n",
    "     numeric_inputs)\n",
    ")\n",
    "\n",
    "# we fit and apply the pipeline on the train set\n",
    "X_train = input_preproc.fit_transform(df_train[categorical_inputs+numeric_inputs])\n",
    "Y_train = df_train[output].values\n",
    "\n",
    "# we apply the same pipeline to the test set,\n",
    "# taking care to use transform and not fit_transform\n",
    "X_test = input_preproc.transform(df_test[categorical_inputs+numeric_inputs])\n",
    "Y_test = df_test[output].values\n",
    "\n",
    "# we plot the data for visual inspection\n",
    "plt.scatter(X_train, Y_train, marker='x', label=\"training data\")\n",
    "plt.scatter(X_test, Y_test, c='r', label=\"testing data\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(ls='--')\n",
    "plt.legend()\n",
    "plt.savefig(\"output/regression_data.pdf\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OjEM-vSdL_Qj",
    "tags": [
     "en"
    ]
   },
   "source": [
    "In addition to our standard preprocessing, we will also transform the results into datatypes expected by PyTorch, i.e. into PyTorch tensors (similar to `numpy` arrays, but with autodiff support) of 32-bit floats.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D034I7pzL_Qq"
   },
   "outputs": [],
   "source": [
    "X_train = torch.as_tensor(X_train, dtype=torch.float32)\n",
    "Y_train = torch.as_tensor(Y_train, dtype=torch.float32)\n",
    "X_test = torch.as_tensor(X_test, dtype=torch.float32)\n",
    "Y_test = torch.as_tensor(Y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pz5vyIi-K7OG",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Selecting a Device and Transferring our Data\n",
    "\n",
    "Our neural net can run on several different kinds devices. By default, everythings runs on the processor (CPU), but PyTorch also supports ceratain kinds of graphical cards (GPUs), the use of which can speed up the computations very significantly. There are also other special devices such as TPUs, FPGAs, etc., but to run your models on those, you will generally need some kinds of extensions to PyTorch.\n",
    "\n",
    "Let us now specify what kind of device we want to use: let's say that we want to use a GPU, if it is available, and the CPU, if it is not. We can check for GPU availability using `torch.cuda.is_available`. Note that on a multi-gpu computer, you can also select which specific GPU or GPUs you want to use, but that is beyond the scope of this notebook.\n",
    "\n",
    "Here we are merely going to select `\"cuda\"` (the GPU, so named after the CUDA framework from Nvidia) if `torch.cuda.is_available()` is true and `\"cpu\"` otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pz5vyIi-K7OG",
    "tags": [
     "en"
    ]
   },
   "source": [
    "When using a certain device to run our model, we need to make sure to also transfer our data to that device's memory. This is easily done using the `.to(device)` method provided by PyTorch tensors. To transfer our data now into our selected device, we can run:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to(device)\n",
    "Y_train = Y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "Y_test = Y_test.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GC4NtVMbL_Q2",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Creation of the Neural Network and Training\n",
    "\n",
    "In order to create our neural net, we will inherit from the base class `nn.Module`. All layers with learnable parameters are created in the constructor and assigned as attributes to our network. The way in which the layers connect to each other and compute the output from the input is defined in method `forward`. A neural net must have a certain fixed number of input and output neurons. The number of inputs will, of course, equal the number of columns in our `X_train`, while the number of outputs will equal the number of columns in our `Y_train`.\n",
    "\n",
    "You'll recall that in neural networks built for regression, we usually **leave the last layer linear**  (without an activation function) so that it can produce unbounded outputs and does not have to learn to invert the effect of a non-linear activation function when its shape is not a good match for the regression task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oANQWkmvL_Q-"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.fc1(x)\n",
    "        y = torch.relu(y)\n",
    "        \n",
    "        y = self.fc2(y)\n",
    "        y = torch.relu(y)\n",
    "        \n",
    "        y = self.fc3(y)        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zn0ySi8HK7OY",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Now we are ready to construct our model. Note that the model also needs to be transferred to our device of choice, which is done in exactly the same way we employed with the data: by calling `.to(device)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = X_train.shape[1]\n",
    "num_outputs = Y_train.shape[1]\n",
    "\n",
    "model = Net(num_inputs, num_outputs)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zn0ySi8HK7OY",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Running the Network\n",
    "\n",
    "If we did everything correctly, we should now be able to run our data through the model. Let's try that with the first 5 rows from `X_train`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(X_train[:5, ...])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zn0ySi8HK7OY",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Tensors, Gradients, Detaching\n",
    "\n",
    "You may have noticed the `grad_fn` in the printout of our tensor. As mentioned before, PyTorch tensors have built-in support for autodiff. When you run operations on them, an on-the-fly computational graph is being built, which can then be backpropagated through.\n",
    "\n",
    "If you are going to do some further operations with your tensors that are not part of the training process, such as logging loss values, doing plotting, etc., it is a good idea to extract the data and get rid of the computational graph before you do anything else. You can do this using `.detach()`; you will see that the `grad_fn` part will be gone when you display the tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zn0ySi8HK7OY",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Converting to NumPy\n",
    "\n",
    "To convert your tensor into a `numpy` array, you can run `.numpy()` on it. Since the tensor can have gradient info attached, it is generally a good idea to call `.detach` first. Furthermore, the tensor can be on a different device, so to be safe, you'll usually also want to call `.cpu()` to transfer it back to the CPU first.\n",
    "\n",
    "I.e. this is the fool-proof way of converting from PyTorch tensors to numpy arrays:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_np = y.detach().cpu().numpy()\n",
    "y_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zn0ySi8HK7OY",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Similarly, if your tensor contains a scalar, you can extract it simply by calling `.item()`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scalar = y.mean()\n",
    "s = y_scalar.detach().cpu().item()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zn0ySi8HK7OY",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Running without Gradients\n",
    "\n",
    "When you are running your model outside of training, you usually won't need the autodiff support and the computational graph. In such cases, what you want to do is turn the computational graph off, since building it involves some computational overhead. To do this, you can put your PyTorch calls under a `torch.no_grad()` context, e.g.:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y = model(X_train[:5, ...])\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zn0ySi8HK7OY",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Note how the tensor doesn't have `grad_fn` now even though we didn't run `.detach()` on it. This is because the `torch.no_grad()` context prevented the computation graph from being built in the first place.\n",
    "\n",
    "#### Train Mode vs. Eval Mode\n",
    "\n",
    "PyTorch has a number of special layers that behave differently during training than they do during inference. For instance, there is the dropout layer, which â€“ during training â€“ keeps randomly turning off some portion of the layer's neurons to help prevent the network from overfitting. During inference this behaviour is, of course, deactivated since one doesn't want to interfere with the quality of predictions.\n",
    "\n",
    "To support both these cases, then, PyTorch models have two distinct modes:\n",
    "\n",
    "* **Training Mode:**  When training the model, you put it into training mode by calling `model.train()`;\n",
    "* **Evaluation Mode:**  When running inference, you put it into evaluation mode by calling `model.eval()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# during training:\n",
    "model.train()\n",
    "y = model(X_train[:5, ...])\n",
    "\n",
    "# during inference:\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y = model(X_test[:5, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zn0ySi8HK7OY",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### The Training Loop\n",
    "\n",
    "After this, the only thing that remains is to run the training. In PyTorch, this is relatively verbose: we need to construct a loss function, an optimizer and write the entire training loop from scratch. However, this is an approach that allows a lot of flexibility, which is going to be very useful when constructing and training more complex models.\n",
    "\n",
    "In later examples, we will show how to train on mini-batches and we may even enhance our training loop with more fancy features such as learning rate scheduling, early stopping, loading data on the fly and augmenting it, etc. For now, however, we are going to keep things simple. Since our data is tiny, we are going to do training in full-batch mode, i.e. feed all our training data into the model at once.\n",
    "\n",
    "#### Constructing the Optimizer\n",
    "\n",
    "We are going to use `Adam` as our optimizer of choice. When constructing it, we need to specify:\n",
    "\n",
    "* what parameters it will be optimizing â€“ we specify `model.parameters()`, i.e. parameters of our model;\n",
    "* what its learning rate is going to be.\n",
    "#### Constructing the Loss Function\n",
    "\n",
    "For the loss function, we are going to go with the **mean squared error** , which is a common choice for regression problems. We can construct it simply using PyTorch's `nn.MSELoss`.\n",
    "\n",
    "The rest of the code for the training loop is going to explained through comments in the following code cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_train = [] # we will store the training loss here for plotting\n",
    "\n",
    "# we are going to train for a number of  epochs\n",
    "for epoch in range(1000):\n",
    "    # we put the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # we run our data through the model\n",
    "    y = model(X_train)\n",
    "\n",
    "    # we measure the loss and record it\n",
    "    loss = criterion(y, Y_train)\n",
    "    loss_train.append(loss.detach().item())\n",
    "\n",
    "    # we clear any gradients that have been\n",
    "    # computed in the previous iteration\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # we backpropagate the loss\n",
    "    loss.backward()\n",
    "\n",
    "    # we update the weights using the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    # we print a progress report every now and then\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"epoch {epoch}, loss: {np.mean(loss_train[-20:])}\")\n",
    "\n",
    "print(f\"epoch {epoch}, loss: {np.mean(loss_train[-20:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vrjov_H4L_Rm",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Testing\n",
    "\n",
    "Now that we have trained our model, we are ready to test its performance. We remember to put our model into evaluation mode using `model.eval()` first and running the model inside `torch.no_grad()` to skip building the computational graph.\n",
    "\n",
    "To evaluate, we are going to compute the MSE, the MAE and display our usual histogram of errors on a standardized scale.\n",
    "\n",
    "#### On Training Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_cpu = Y_train.cpu()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_cpu = model(X_train).cpu()\n",
    "\n",
    "# we compute and display the MSE and the MAE\n",
    "mse = mean_squared_error(Y_train_cpu, y_train_cpu)\n",
    "print(\"MSE = {}\".format(mse))\n",
    "\n",
    "mae = mean_absolute_error(Y_train_cpu, y_train_cpu)\n",
    "print(\"MAE = {}\".format(mae))\n",
    "\n",
    "# we display the error histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "error_histogram(Y_train_cpu, y_train_cpu, Y_fit_scaling=Y_train_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dTzTPfMLK7Pl",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### On Testing Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_cpu = Y_test.cpu()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_cpu = model(X_test).cpu()\n",
    "\n",
    "# we compute and display the MSE and the MAE\n",
    "mse = mean_squared_error(Y_test_cpu, y_test_cpu)\n",
    "print(\"MSE = {}\".format(mse))\n",
    "\n",
    "mae = mean_absolute_error(Y_test_cpu, y_test_cpu)\n",
    "print(\"MAE = {}\".format(mae))\n",
    "\n",
    "# we display the error histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "error_histogram(Y_test_cpu, y_test_cpu, Y_fit_scaling=Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dTzTPfMLK7Pl",
    "tags": [
     "en"
    ]
   },
   "source": [
    "These results indicate that the model works quite well â€“ the errors are low on both the train and the test set. Since we are working with 2D data, let's also plot the points in the original space.\n",
    "\n",
    "We may still observe minor artifacts in some parts of the curve, but the overall shape should be captured reasonably well, if our results are good on the train and the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "UyT19LaJL_R3",
    "outputId": "1d1008ac-dd73-48d3-e6fe-f83511e1670e"
   },
   "outputs": [],
   "source": [
    "#@title -- Regression Curve vs. Data -- { display-mode: \"form\" }\n",
    "x_min = min(torch.min(X_train), torch.min(X_test))\n",
    "x_max = max(torch.max(X_train), torch.max(X_test))\n",
    "xx = torch.linspace(x_min, x_max, 250).reshape((-1, 1))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    yy = model(xx.to(device))\n",
    "    yy = yy.cpu()\n",
    "\n",
    "plt.scatter(X_train.cpu(), Y_train.cpu(), marker='x', label=\"training data\")\n",
    "plt.scatter(X_test.cpu(), Y_test.cpu(), c='r', label=\"testing data\")\n",
    "\n",
    "plt.plot(xx, yy, label=\"regression curve\", c='k')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(ls='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"output/regression.pdf\", bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dFb8rM1eL_SB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "3_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a351393365bb1b108989afa08de3243f72f5e58927baf5d192f3cca79a41cbc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
