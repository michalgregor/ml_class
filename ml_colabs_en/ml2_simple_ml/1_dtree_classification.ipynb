{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform, which provides free hardware acceleration. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook, using a local GPU.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bs97NTOSoeVF"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from class_utils import show_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0JZDvAVnoop7",
    "outputId": "b53da33f-e828-4921-d40e-a1c5f9a9a50a"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "DATA_HOME = \"https://github.com/michalgregor/ml_notebooks/blob/main/data/{}?raw=1\"\n",
    "\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(DATA_HOME.format(\"titanic.zip\"), directory=\"data/titanic\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Decision Trees for Classification\n",
    "\n",
    "We will now show how to apply a decision tree classifier to the [Titanic](https://www.kaggle.com/c/titanic) dataset. Given that we have already explored this dataset in a previous example and we already know how to preprocess it, we will not repeat the exercise. The code necessary to load and preprocess the data is in the next cell and it is hidden for conciseness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Preprocessing the Data -- { display-mode: \"form\" }\n",
    "df = pd.read_csv(\"data/titanic/train.csv\")\n",
    "df_train, df_test = train_test_split(df, test_size=0.25,\n",
    "                     stratify=df[\"Survived\"], random_state=4)\n",
    "\n",
    "categorical_inputs = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
    "numeric_inputs = [\"Age\", \"SibSp\", 'Parch', 'Fare']\n",
    "class_names = [\"died\", \"survived\"]\n",
    "\n",
    "output = \"Survived\"\n",
    "\n",
    "input_preproc = make_column_transformer(\n",
    "    (make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OrdinalEncoder()),\n",
    "     categorical_inputs),\n",
    "    \n",
    "    (make_pipeline(\n",
    "        SimpleImputer(),\n",
    "        StandardScaler()),\n",
    "     numeric_inputs)\n",
    ")\n",
    "\n",
    "X_train = input_preproc.fit_transform(df_train[categorical_inputs+numeric_inputs])\n",
    "Y_train = df_train[output].values.reshape(-1)\n",
    "\n",
    "X_test = input_preproc.transform(df_test[categorical_inputs+numeric_inputs])\n",
    "Y_test = df_test[output].values.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Training\n",
    "\n",
    "Literally the only thing that we need to change at this point w.r.t. the previous example is to use a `DecisionTreeClassifier` instead of the `KNeighborsClassifier`. The rest of the code can stay the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Testing\n",
    "\n",
    "The code to test the model can be copied verbatim as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)\n",
    "\n",
    "cm = pd.crosstab(Y_test, y_test,\n",
    "                 rownames=['actual'],\n",
    "                 colnames=['predicted'])\n",
    "print(cm, \"\\n\")\n",
    "\n",
    "acc = accuracy_score(Y_test, y_test)\n",
    "print(\"Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The accuracy achieved by our decision tree classifier is not great. In fact, chances are that it will be lower than that achieved in our KNN example. This is very suspicious indeed and it might mean that our model has overfitted. To check whether this is the case, we should test our model on the training set. If the results are much better, that will indicate overfitting and we will need to modify the hyperparameters of the decision tree so as to decrease its capacity and get a model that generalizes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = model.predict(X_train)\n",
    "\n",
    "acc_train = accuracy_score(Y_train, y_train)\n",
    "print(\"Accuracy = {}\".format(acc_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "As it turns out, the accuracy on the training set is indeed much higher, which indicates overfitting. We can also visualize the resulting tree to examine how complex it is. We will use an auxiliary function `show_tree`. The tree is likely to be quite complex and difficult to read.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(model,\n",
    "  feature_names=categorical_inputs+numeric_inputs,\n",
    "  class_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Tuning the Hyperparameters to Do More Pruning\n",
    "\n",
    "As the next step we will show how to tune hyperparameters of the decision tree to make it simpler and to prevent it from overfitting. This is achieved using pruning, which can come in two different flavors:\n",
    "\n",
    "* **pre-pruning** : as the tree is grown, new branches are prevented from being formed unless some pre-specified criteria are met;\n",
    "* **post-pruning** : the tree is grown fully and branches are removed from it afterwards.\n",
    "In this example we will only be using pre-pruning and its parameters will be specified in the constructor of the `DecisionTreeClassifier`.\n",
    "\n",
    "#### Using Cross-Validation\n",
    "\n",
    "When tuning the hyperparameters, we will need a way to determine which parameters work. We cannot test each different setting on the testing set: recall that we are only allowed to use it once â€“ to test the final model.\n",
    "\n",
    "We basically have 2 options:\n",
    "\n",
    "* To split the dataset into 3 parts: the training set, the validation set and the testing set (the validation set would be used to tune the hyperparameters and the testing set would be used at the end to verify that the final model generalizes).\n",
    "* To use cross-validation: The training set would be split into $k$ folds and then the model would be trained on $k-1$ folds and tested on the remaining fold. This would be repeated for all combinations of folds and the results would be averaged.\n",
    "Since decision trees are cheap to train and our dataset is not too large, in the present example we will use cross-validation. Let us have a look at how it is applied in `scikit-learn`. We will use the `sklearn.model_selection.cross_validate` function and specify `cv=5`, which means that there will be $k = 5$ folds. The function will return the accuracies on all of the folds. We will compute the mean of these and use that as an indicator of how well our model is doing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(model, X_train, Y_train, cv=5)['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Changing the Hyperparameters\n",
    "\n",
    "Now let's do some actual hyperparameter tuning. To see what we can change when constructing the decision tree, we will have a look at its documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DecisionTreeClassifier.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The minimum number of samples for a leaf (`min_samples_leaf`) seems like a good candidate: if we make a prediction based on a very small amount of samples, it is likely not to be representative. You can, of course, also try to experiment with other parameters such as the maximum depth of the tree and so on.\n",
    "\n",
    "---\n",
    "#### Task 1: Tune `min_samples_leaf`\n",
    "\n",
    "**In the cell below, experiment with different values of `min_samples_leaf` and select try to maximize the cross-validation accuracy. Also observe the effect of the hyperparameter on the structure of the tree.** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(\n",
    "    \n",
    "    \n",
    "    min_samples_leaf=    # ------\n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "acc = cross_validate(model, X_train, Y_train, cv=5)['test_score'].mean()\n",
    "print(\"Cross-validation accuracy = {}\".format(acc))\n",
    "\n",
    "# we need to fit the model before we plot it\n",
    "model.fit(X_train, Y_train)\n",
    "show_tree(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Testing the Tuned Tree\n",
    "\n",
    "Having tuned the hyperparameters, we can now verify how well our final model actually generalizes. We re-train the model with the best hyperparameters on the entire training set and evaluate the accuracy on the testing set. The results should be substantially better now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)\n",
    "accuracy_score(Y_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "1_pipelines.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
