{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "\n",
    "# install deps for explain manually, since pdpbox requires\n",
    "# an ancient version of matplotlib as a dep\n",
    "!{sys.executable} -m pip install --no-deps pdpbox lime eli5\n",
    "!{sys.executable} -m pip install class_utils@git+https://github.com/michalgregor/class_utils.git\n",
    "#!{sys.executable} -m pip install class_utils[explain]@git+https://github.com/michalgregor/class_utils.git\n",
    "\n",
    "!{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from class_utils import Explainer\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    COLAB_MODE = True\n",
    "except:\n",
    "    COLAB_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "DATA_HOME = \"https://github.com/michalgregor/ml_notebooks/blob/main/data/{}?raw=1\"\n",
    "\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(DATA_HOME.format(\"titanic.zip\"), directory=\"data/titanic\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Interpreting Models\n",
    "\n",
    "There is a lot of applications, where it is not enough to train a model and compute predictions. We need to be able to interpret the model and to explain why a certain prediction was made. Such interpretability is a vital prerequisite of safe and trustworthy machine learning and artificial intelligence – it helps to verify that a system is not biased and that its predictions are not based on protected attributes such as race. In some countries, interpretability is even required by law: In the EU, for instance, whenever an automatic system makes decisions about humans they have the right to an explanation.\n",
    "\n",
    "There are models, which have some inherent interpretability. Decision trees are a good example: a tree is essentially just a collection of rules. We can plot it and read through it. With most kinds of models, however, this is not possible. Even with decision trees it becomes harder with increasing size and as soon as we form an ensemble of decision trees, it is simply not practicable anymore.\n",
    "\n",
    "In this notebook we will showcase a few generic methods that help to interpret predictions of arbitrary models. We will again start by loading and preprocessing the [Titanic](https://www.kaggle.com/c/titanic) dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/titanic/train.csv\")\n",
    "df_train, df_test = train_test_split(df, test_size=0.25,\n",
    "                     stratify=df[\"Survived\"], random_state=4)\n",
    "\n",
    "categorical_inputs = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
    "numeric_inputs = [\"Age\", \"SibSp\", 'Parch', 'Fare']\n",
    "output = \"Survived\"\n",
    "\n",
    "class_names = [ \"died\", \"survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_preproc = make_column_transformer(\n",
    "    (make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OrdinalEncoder()),\n",
    "     categorical_inputs),\n",
    "    \n",
    "    (make_pipeline(\n",
    "        SimpleImputer(),\n",
    "        StandardScaler()),\n",
    "     numeric_inputs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = input_preproc.fit_transform(df_train[categorical_inputs+numeric_inputs])\n",
    "Y_train = df_train[output].values.reshape(-1)\n",
    "\n",
    "X_test = input_preproc.transform(df_test[categorical_inputs+numeric_inputs])\n",
    "Y_test = df_test[output].values.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We will extract the imputers from our pipeline. These are important: they will be used later when constructing an explainer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_imputer = input_preproc.transformers_[0][1][0]\n",
    "numeric_imputer = input_preproc.transformers_[1][1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We will next train an XGBoost model on the data and compute its accuracy on the test set just to make sure that everything works correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)\n",
    "accuracy_score(Y_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We create an explainer: an auxiliary object that will allow us to create the explanations. When constructing the explainer, it is crucial to use **the same kind of imputation of missing values**  that we used to train our model. Otherwise we will be explaining different samples than the model would normally see.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = Explainer(\n",
    "    model, df_train,\n",
    "    categorical_inputs,\n",
    "    categorical_imputer,\n",
    "    numeric_inputs,\n",
    "    numeric_imputer,\n",
    "    input_preproc,\n",
    "    class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Permutation Importance\n",
    "\n",
    "The first thing that it might be useful to know is the relative influence of individual features on the prediction (feature importance). One well-known way to compute the importance of a feature is to permute its column in the dataset (to shuffle it so that it gets out of order) and observe how that affects the predictions. If they change very severely the feature was presumably very important to the prediction. If they only change slightly or not at all, the importance of the feature is probably negligible. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = explainer.permutation_importance(df_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "In Titanic, for instance, column \"Sex\" seems to be the most important by far. This indicates that men and women have very different rates of survival.\n",
    "\n",
    "### Partial Dependence Plots\n",
    "\n",
    "To investigate the influence of a feature on the prediction in more detail, we can use the so called partial dependence plots. These are formed by systematically changing a single feature and observing how that influences the results. Let us see what the partial dependence on column \"Sex\" then. Recall that we are predicting whether a person survived (1) or not (0). A positive number means that a feature contributes to survival and a negative number indicates that it contributes to death.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.pdp_plot(df_test, \"Sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "It is obvious that being male can significantly decrease the chance of survival as far as our classifier is concerned. However, there is a lot more variance than for women – this means that at least for some men there would still be at least a reasonable chance of survival.\n",
    "\n",
    "If we explore the PD plot for \"Fare\", we should see a positive relationship: higher fares generally meant a better chance of survival.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.pdp_plot(df_test, \"Fare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The plots do not have to be monotonous. For age, for example, the situation is a bit more nuanced – although there is a chance that this is due to noise in the data: the effect is not that pronounced.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.pdp_plot(df_test, \"Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### LIME: Local Interpretations\n",
    "\n",
    "Finally, we might be interested in local interpretations: when given a particular sample, we might want to know the influence that each feature had on the prediction. There is a method called LIME (Local Interpretable Model-agnostic Explanations), which provides this kind of explanation by fitting a local linear model around the prediction. This makes the approach model-agnostic: it works with any kind of model.\n",
    "\n",
    "To experiment with LIME, we will pick some sample from the dataset and have it explained. We will see which features have a positive and a negative influence on a particular prediction and what the magnitude of that influence is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain(df_test.iloc[2])\n",
    "exp.show_in_notebook(show_all=True, colab_mode=COLAB_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_pyplot_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
