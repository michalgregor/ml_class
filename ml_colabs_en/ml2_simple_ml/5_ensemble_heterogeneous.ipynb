{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "olympic-knowing",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install lightgbm\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "DATA_HOME = \"https://github.com/michalgregor/ml_notebooks/blob/main/data/{}?raw=1\"\n",
    "\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(DATA_HOME.format(\"titanic.zip\"), directory=\"data/titanic\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-terrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Loading and Preprocessing the Data -- { display-mode: \"form\" }\n",
    "df = pd.read_csv(\"data/titanic/train.csv\")\n",
    "df_train, df_test = train_test_split(df, test_size=0.25,\n",
    "                     stratify=df[\"Survived\"], random_state=4)\n",
    "\n",
    "categorical_inputs = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
    "numeric_inputs = [\"Age\", \"SibSp\", 'Parch', 'Fare']\n",
    "\n",
    "output = \"Survived\"\n",
    "\n",
    "input_preproc = make_column_transformer(\n",
    "    (make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OrdinalEncoder()),\n",
    "     categorical_inputs),\n",
    "    \n",
    "    (make_pipeline(\n",
    "        SimpleImputer(),\n",
    "        StandardScaler()),\n",
    "     numeric_inputs)\n",
    ")\n",
    "\n",
    "X_train = input_preproc.fit_transform(df_train[categorical_inputs+numeric_inputs])\n",
    "Y_train = df_train[output].values.reshape(-1)\n",
    "\n",
    "X_test = input_preproc.transform(df_test[categorical_inputs+numeric_inputs])\n",
    "Y_test = df_test[output].values.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-stuff",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Heterogeneous Ensembles\n",
    "\n",
    "In the previous notebook we have explored methods to train homogeneous ensembles: i.e. ensembles made of several models of the same class (decision trees in our case). These methods were able to set up and train all the models automatically.\n",
    "\n",
    "In this notebook we are going to explore heterogeneous ensembles, which are going to need a bit more work: we will need to create each model separately and then join them into an ensemble using some kind of meta-classifier such as `sklearn.ensemble.VotingClassifier` (or `sklearn.ensemble.VotingRegressor` for regression). The reward for this extra work, however, should be better performance: heterogeneous models tend to make very different errors so a heterogeneous ensemble can often achieve better generalization than a homogeneous one.\n",
    "\n",
    "### An Ensemble Using `VotingClassifier`\n",
    "\n",
    "We will now briefly illustrate how to use `VotingClassifier` on our task. We will start by creating a list of models that we want to form an ensemble from. We can first create each of them separately and use cross-validation to do a little hyperparameter tuning on them like we did in the previous notebooks.\n",
    "\n",
    "We are going to use a small auxiliary function that will display the cross-validation results for both: the test folds and the train folds so that we can distinguish overfitting from underfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossval(model):\n",
    "    scores = cross_validate(model, X_train, Y_train, cv=10, return_train_score=True)\n",
    "    display(Markdown(\"train: {:.5f}; **test: {:.5f}**\".format(\n",
    "        scores['train_score'].mean(),\n",
    "        scores['test_score'].mean()\n",
    "    )))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb7ff6",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "---\n",
    "#### Task 1: Tuning Hyperparams for Each Classifier Separately\n",
    "\n",
    "**In the cells below, try experimenting with the classifiers' hyperparameters to find a setting which does reasonably well in cross-validation.**  Aid: You can run ?NameOfTheClassifier to display the classifier's docstring.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef1a5ff",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "dtree_model = DecisionTreeClassifier(\n",
    "    \n",
    "    # ---\n",
    "    \n",
    ")\n",
    "crossval(dtree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab8fc6f",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "lgbm_model = LGBMClassifier(\n",
    "    \n",
    "    # ---\n",
    "    \n",
    ")\n",
    "crossval(lgbm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f0da63",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(\n",
    "    \n",
    "    # ---\n",
    "    \n",
    ")\n",
    "crossval(knn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff3225e",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "svc_model = svm.SVC(\n",
    "    \n",
    "    # ---\n",
    "    \n",
    ")\n",
    "crossval(svc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ea8ff",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression(\n",
    "    \n",
    "    # ---\n",
    "    \n",
    ")\n",
    "crossval(logistic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    (\"dtree\", dtree_model),\n",
    "    (\"lgbm\", lgbm_model),\n",
    "    (\"knn\", knn_model),\n",
    "    ('svc', svc_model),\n",
    "    ('logistic', logistic_model)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-brake",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We pass the list to `VotingClassifier`. We can also specify the voting mode and other parameters, the meaning of which can be found in the documentation. Having constructed the classifier, we train it. This will get all the contained models trained on the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VotingClassifier(estimators)\n",
    "crossval(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afc7bc3",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### An Ensemble Using `StackingClassifier`\n",
    "\n",
    "As a further alternative, you could use stacking instead of voting. There you would first train a bunch of models and then you would add their outputs to the dataset as further columns. Finally, you would stack another classifier on top â€“ i.e. train it on the full dataset including the new columns.\n",
    "\n",
    "This second 2nd-level model can make use of 1st-level models' predictions, e.g. it could figure out which models might be best at predicting for this kind of sample and weight the predictions accordingly, etc.\n",
    "\n",
    "Here we are going to construct a `StackingClassifier` with our bunch of estimators at the 1st level and a logistic regression model at the 2nd level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackingClassifier(\n",
    "    estimators,\n",
    "    final_estimator=LogisticRegression(C=10),\n",
    "    cv=10\n",
    ")\n",
    "crossval(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-september",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Testing\n",
    "\n",
    "Now select the best ensemble and test it on our testing set. With any luck its performance should be better than any of the component models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898c49b",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "\n",
    "accuracy_score(Y_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aed9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a351393365bb1b108989afa08de3243f72f5e58927baf5d192f3cca79a41cbc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
