{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkj4rzVsmjut",
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform, which provides free hardware acceleration. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook, using a local GPU.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install torchinfo\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y7-4YhzosUs8"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from class_utils.pytorch_utils import BestModelCheckpointer, freeze_except_last\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchinfo\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "uxPlOyi3sUo_",
    "outputId": "0c8b07dc-86a0-4c7e-e27e-8ee3eb135dff"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(\"https://www.dropbox.com/s/w4pg809npvatye0/food5v2.zip?dl=1\", directory=\"data/food5v2\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bm8Dnrk5OC-l",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Transfer Learning\n",
    "\n",
    "In this notebook we will use the **Food 5**  dataset to illustrate transfer learning. The dataset is a downsized version of the [Food 11](https://www.kaggle.com/vermaavi/food11) dataset. For a different transfer learning example, you can also see tensorflow.js's [interactive demo](https://storage.googleapis.com/tfjs-examples/webcam-transfer-learning/dist/index.html).\n",
    "\n",
    "Transfer learning is a very useful technique. Under ordinary circumstance deep learning requires a huge amount of data and computation. If we intend to apply it to a small dataset we will typically not be able to achieve good generalization. The problem is connected to the fact that a small dataset typically cannot sufficiently cover all the possible variations of samples that a model can encounter. In the case of image recognition, for instance, there is virtually an infinite number of variations that a photo of a dog can take: the environment, the lighting, the breed of the dog, the angle – these and other aspects can all change. A small dataset is very unlikely to cover such complex space sufficiently.\n",
    "\n",
    "One of the solutions that allow us to apply deep learning to small datasets even in spite of these problems is **transfer learning** . Under this technique the neural network is first pre-trained on a large, more general dataset (for image recognition this tends to be the ImageNet dataset). The network uses this dataset to learn what natural images look like and how they need to be preprocessed. Once this pre-training is complete, the dataset is then further trained for the specific target task.\n",
    "\n",
    "### The Overall Procedure\n",
    "\n",
    "The overall procedure for transfer learning in image recognition:\n",
    "\n",
    "* Pre-train a network on ImageNet.\n",
    "\n",
    "\n",
    "* Remove one or several of the final layers (the top of the network) and replace them with new layers. The new output layer will now have as many outputs as there are classes in the dataset.\n",
    "\n",
    "\n",
    "* The weights of the pre-trained layers are frozen. Only the new layers are trained using the target dataset.\n",
    "\n",
    "\n",
    "* One the new layers have been trained we can (an optional step) unfreeze the weights of the pre-trained layers as well and fine-tune the network as a whole. We will need to use a significantly lower learning rate. This is so that we do not destroy the pre-trained layers by doing excessively aggressive updates, but also because when the pre-trained layers can be modified, the risk of overfitting tends to increase.\n",
    "\n",
    "\n",
    "### Preparation of the Dataset\n",
    "\n",
    "As usual, let us start by preparing our dataset. For most image recognition tasks the dataset will be too large to fit into memory at once. We will therefore typically not attempt to load all the data at once and we'll use the `DataSet` and `DataLoader` abstraction from `PyTorch`. In the present case, our data comes pre-split into the train, validation and test folds, with each stored in a separate folder. The folders are structured so that each class has its own subfolder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "p-Wiw8QHmjy5",
    "outputId": "02439698-ecbd-4fa9-899f-79b226db318e"
   },
   "outputs": [],
   "source": [
    "!ls data/food5v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vt3NIEDKmjzC",
    "outputId": "f5d12ecb-9032-4f29-a142-0b834ea0c2ed"
   },
   "outputs": [],
   "source": [
    "!ls data/food5v2/training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mVCdEtsFmjzP",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Given that our data has this structure, we can use the `ImageFolder` dataset class from `torchvision.datasets`.\n",
    "\n",
    "Each image will need to be pre-processed before it is fed into the neural network: it will need to be resized, cropped and normalized in the same way it was done when the network was pre-trained. We will use a pre-trained ResNet50 with `IMAGENET1K_V2` weights. So let's first see what the preprocessing procedure that these weights were trained with looks like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "weights = models.ResNet50_Weights.IMAGENET1K_V2\n",
    "image_transforms = weights.transforms()\n",
    "image_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mVCdEtsFmjzP",
    "tags": [
     "en"
    ]
   },
   "source": [
    "This is actually pretty simple. We are going to base two different preprocessing procedures for our data on it. The first one is just going to reproduce `image_transforms` shown above. The second one, however, is going to do **data augmentation**  – it will contain a couple of randomized steps that are going to modify the image every time that it is loaded. This is going to add more variety to our training set. The network is essentially never going to see the exact same image twice. In practice, data augmentation pipelines can be a lot more elaborate, applying rotation, zoom, channel shift and a bunch of other transformations to the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6P8XQEMCsgVX"
   },
   "outputs": [],
   "source": [
    "normal_preproc = transforms.Compose([\n",
    "    transforms.Resize(image_transforms.resize_size),\n",
    "    transforms.CenterCrop(image_transforms.crop_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(image_transforms.mean, image_transforms.std)\n",
    "])\n",
    "\n",
    "augment_preproc = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_transforms.crop_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(image_transforms.mean, image_transforms.std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndwMqTUOR71l",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Next we can construct the `ImageFolder` datasets themselves. We specify the paths to the individual folds of our dataset as well as the way in which the images should be preprocessed for each fold. We will use the normal pipeline for validation and testing data and the pipeline with augmentation for training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEMPcRIits1V"
   },
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(\n",
    "    \"data/food5v2/training\",\n",
    "    augment_preproc\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "valid_dataset = ImageFolder(\n",
    "    \"data/food5v2/validation\",\n",
    "    normal_preproc\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_dataset = ImageFolder(\n",
    "    \"data/food5v2/testing\",\n",
    "    normal_preproc\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwehgtZCmjz5",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Displaying a Few Samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "QgmXBE6mmj0B",
    "outputId": "b563f713-82be-4675-ac26-94a979daa74a"
   },
   "outputs": [],
   "source": [
    "#@title -- Display Data Samples --\n",
    "disp_dataset = ImageFolder(\n",
    "    \"data/food5v2/training\",\n",
    "    transforms.ToTensor()\n",
    ")\n",
    "loader = DataLoader(disp_dataset, batch_size=1, shuffle=True)\n",
    "loader_iter = iter(loader)\n",
    "\n",
    "num_rows = 4; num_cols = 4\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 8))\n",
    "\n",
    "for row in axes:\n",
    "    for ax in row:\n",
    "        sample = next(loader_iter)[0][0].numpy().transpose((1, 2, 0))\n",
    "        ax.imshow(sample)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2aLHWrpgOJOz",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Loading the Pre-Trained Network\n",
    "\n",
    "We load a pre-trained ResNet50 network. The weights pre-trained on ImageNet will download automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "80e98bc88f374275bedb82737935ab4d",
      "5a1034ec61384e90a57f3ca9f57e4bb6",
      "c8f6ae12235d4dfa864de5114a6e2bf5",
      "f5e66d097cd94fcd92c0167d0baac031",
      "d6e67658bddb41a1b6e1261a63603158",
      "d6995b2241ae46cca768574f8cccf2d6",
      "3db5a994401949858a1507e673cc87c9",
      "172ff158eadb4cd6bb7d13eb0b99bcf3"
     ]
    },
    "colab_type": "code",
    "id": "iSHPQsGVmj0v",
    "outputId": "9659afdc-3d49-4d41-85ae-a58f401e579f"
   },
   "outputs": [],
   "source": [
    "model = models.resnet50(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2aLHWrpgOJOz",
    "tags": [
     "en"
    ]
   },
   "source": [
    "To get a feeling for what our architecture looks like, we are going to use function `torchinfo.summary`. This is going to give us info about the hierarchical structure of our network, including all its submodules and individual layers. The summary at the very bottom also shows how many trainable parameters there are.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PpgGwXhmj05",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Modifying the Network\n",
    "\n",
    "#### Replacing the Final Layer\n",
    "\n",
    "To adapt our neural network to the new classification task, we are going to replace the last layer (the fully-connected linear layer `model.fc`) with a new module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tmi9KFLfBIqZ"
   },
   "outputs": [],
   "source": [
    "class ModelTop(nn.Module):\n",
    "    def __init__(self, num_features, num_outputs):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(num_features, num_outputs)\n",
    "\n",
    "    def set_dropout(self, p):\n",
    "        self.dropout.p = p\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = torch.flatten(x, 1)\n",
    "        y = self.dropout(y)\n",
    "        y = self.fc(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-lut1-SsStUA"
   },
   "outputs": [],
   "source": [
    "num_features = model.fc.in_features\n",
    "top = ModelTop(num_features=num_features, num_outputs=10)\n",
    "model.fc = top\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ovrd5B8mj1V",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Training the New Layers\n",
    "\n",
    "In our training loop, we are going to use best model checkpointing: we are going to monitor the validation loss and every time it improves, we are going to save our model. Then at the end of training, we are going to restore the best saved model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ovrd5B8mj1V",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Freezing the Pretrained Layers\n",
    "\n",
    "Recall that at first, we only want to train our new top layers and leave the pre-trained layers as they are. In our case we will therefore need to freeze all layers except last. We are going to use a predefined auxiliary function to do that, but internally, it just goes over the layers and set the `requires_grad` flag for all their parameters to a corresponding value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ey8iP-hWY8bi"
   },
   "outputs": [],
   "source": [
    "freeze_except_last(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ovrd5B8mj1V",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Let's display the summary of our model again to make sure that everything worked as intended. We should see that the number of trainable parameters is now substantially lower (just the parameters of the final layer in our new module) and there is a lot of untrainable (frozen) parameters. Note also that for the frozen layers, the number of parameters is now shown in parentheses – this way you can check whether you have frozen the correct layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ovrd5B8mj1V",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Even though we are just using a single linear layer for 10 classes on top of the network, there is still a bunch of trainable parameters: 20 490 if you are using ResNet50. This is a huge amount given that we only have 200 samples in our train set. Dropout should help with generalization, but even so we cannot expect miracles. It wouldn't be difficult to get more data for this kind of task – it is just that we do not want the training in the notebook to take too long so we are working with a tiny dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ovrd5B8mj1V",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Training the New Layers\n",
    "\n",
    "In our training loop, we are going to use best model checkpointing: we are going to monitor the validation loss and every time it improves, we are going to save our model. Then at the end of training, we are going to restore the best saved model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "schedule = ExponentialLR(optimizer, gamma=0.9)\n",
    "checkpointer = BestModelCheckpointer(checkpoint_path=\"output/best_model.pt\")\n",
    "loss_train = []\n",
    "loss_valid = []\n",
    "\n",
    "for epoch in range(30):\n",
    "    epoch_train_loss = []\n",
    "    epoch_valid_loss = []\n",
    "\n",
    "    model.train()\n",
    "    for X_batch, Y_batch in train_dataloader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        Y_batch = Y_batch.to(device)\n",
    "        \n",
    "        y_batch = model(X_batch)\n",
    "        loss = criterion(y_batch, Y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss.append(loss.item())\n",
    "\n",
    "    loss_train.append(np.mean(epoch_train_loss))\n",
    "\n",
    "    model.eval()\n",
    "    for X_batch, Y_batch in valid_dataloader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        Y_batch = Y_batch.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_batch = model(X_batch)\n",
    "            loss = criterion(y_batch, Y_batch)\n",
    "\n",
    "        epoch_valid_loss.append(loss.item())\n",
    "\n",
    "    loss_valid.append(np.mean(epoch_valid_loss))\n",
    "    checkpointer(loss_valid[-1], model)\n",
    "    schedule.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"epoch {epoch}, train loss: {np.mean(loss_train[-5:])}, valid loss: {np.mean(loss_valid[-5:])}\")\n",
    "\n",
    "print(f\"epoch {epoch}, loss: {loss_train[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_train, label=\"train\")\n",
    "plt.plot(loss_valid, label=\"valid\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.grid(ls='--')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ovrd5B8mj1V",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Evaluating the Model on the Validation Set\n",
    "\n",
    "Now we are going to load the best saved model back from the checkpoint file and run evaluation. Since we are not done with our model yet, we are only going to be testing it on the **validation set, not on the testing set** .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"output/best_model.pt\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_Y = []\n",
    "eval_y = []\n",
    "\n",
    "model.eval()\n",
    "for X_batch, Y_batch in valid_dataloader:\n",
    "    eval_Y.extend(Y_batch.numpy())\n",
    "    X_batch = X_batch.to(device)\n",
    "    Y_batch = Y_batch.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_batch = model(X_batch)\n",
    "\n",
    "    eval_y.extend(y_batch.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "eval_Y = np.array(eval_Y)\n",
    "eval_y = np.array(eval_y)\n",
    "\n",
    "cm = pd.crosstab(\n",
    "    eval_Y, eval_y,\n",
    "    rownames=['actual'],\n",
    "    colnames=['predicted']\n",
    ")\n",
    "print(cm, '\\n')\n",
    "\n",
    "acc = accuracy_score(eval_Y, eval_y)\n",
    "print(\"Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUbFpxSmmj2K",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Fine-tuning Pre-Trained Weights\n",
    "\n",
    "Once we have trained the new top of the model, it will often make sense to unfreeze a few more layers of the network and continue training. However, one usually lowers the learning rate significantly and often uses a more conservative optimizer such as `SGD` in place of more aggressive optimizers such as `Adam`. This is to ensure that the steps taken by the optimizer do not disrupt the pretrained features and undo the benefits of using transfer learning. Note also that even at this stage, one typically does not unfreeze the entire network.\n",
    "\n",
    "In our case we have very little data and it is unlikely that this fine-tuning stage will actually help to improve results. We can make the attempt though. We'll start by unfreezing the last 5 layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_except_last(model, num_last=5);\n",
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUbFpxSmmj2K",
    "tags": [
     "en"
    ]
   },
   "source": [
    "---\n",
    "### Task 1: Run the Fine-Tuning\n",
    "\n",
    "**Now modify the training loop used earlier to perform the final fine-tuning. Instead of `Adam`, use `SGD` as the optimizer and set the learning rate to a lower value such as `1e-7`. Also modify `checkpoint_path` so that the checkpoints are saved in a different file than before. If the fine-tuned model's performance is not an improvement upon the previous version, restore the previous version's weights from the corresponding checkpoint.** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUbFpxSmmj2K",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Testing the Fine-Tuned Model\n",
    "\n",
    "Now we load back the best version of our fine-tuned model and evaluate it. Chances are that it is not going to do better than the version where we just trained the new layers because we have so little data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"output/best_full_model.pt\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_Y = []\n",
    "eval_y = []\n",
    "\n",
    "model.eval()\n",
    "for X_batch, Y_batch in valid_dataloader:\n",
    "    eval_Y.extend(Y_batch.numpy())\n",
    "    X_batch = X_batch.to(device)\n",
    "    Y_batch = Y_batch.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_batch = model(X_batch)\n",
    "\n",
    "    eval_y.extend(y_batch.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "eval_Y = np.array(eval_Y)\n",
    "eval_y = np.array(eval_y)\n",
    "\n",
    "cm = pd.crosstab(\n",
    "    eval_Y, eval_y,\n",
    "    rownames=['actual'],\n",
    "    colnames=['predicted']\n",
    ")\n",
    "print(cm, '\\n')\n",
    "\n",
    "acc = accuracy_score(eval_Y, eval_y)\n",
    "print(\"Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUbFpxSmmj2K",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We can now also evaluate our model on the test set. On this particular dataset, we can actually expect the results on the test set to be a bit better – by chance, the test fold appears to be a bit less challenging in this case, which can happen when you're working with very small datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_Y = []\n",
    "eval_y = []\n",
    "\n",
    "model.eval()\n",
    "for X_batch, Y_batch in test_dataloader:\n",
    "    eval_Y.extend(Y_batch.numpy())\n",
    "    X_batch = X_batch.to(device)\n",
    "    Y_batch = Y_batch.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_batch = model(X_batch)\n",
    "\n",
    "    eval_y.extend(y_batch.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "eval_Y = np.array(eval_Y)\n",
    "eval_y = np.array(eval_y)\n",
    "\n",
    "cm = pd.crosstab(\n",
    "    eval_Y, eval_y,\n",
    "    rownames=['actual'],\n",
    "    colnames=['predicted']\n",
    ")\n",
    "print(cm, '\\n')\n",
    "\n",
    "acc = accuracy_score(eval_Y, eval_y)\n",
    "print(\"Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QiJ5ur2v9xnz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "7_transfer_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a351393365bb1b108989afa08de3243f72f5e58927baf5d192f3cca79a41cbc4"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "172ff158eadb4cd6bb7d13eb0b99bcf3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3db5a994401949858a1507e673cc87c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a1034ec61384e90a57f3ca9f57e4bb6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80e98bc88f374275bedb82737935ab4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c8f6ae12235d4dfa864de5114a6e2bf5",
       "IPY_MODEL_f5e66d097cd94fcd92c0167d0baac031"
      ],
      "layout": "IPY_MODEL_5a1034ec61384e90a57f3ca9f57e4bb6"
     }
    },
    "c8f6ae12235d4dfa864de5114a6e2bf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6995b2241ae46cca768574f8cccf2d6",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d6e67658bddb41a1b6e1261a63603158",
      "value": 102502400
     }
    },
    "d6995b2241ae46cca768574f8cccf2d6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6e67658bddb41a1b6e1261a63603158": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f5e66d097cd94fcd92c0167d0baac031": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_172ff158eadb4cd6bb7d13eb0b99bcf3",
      "placeholder": "​",
      "style": "IPY_MODEL_3db5a994401949858a1507e673cc87c9",
      "value": " 97.8M/97.8M [00:40&lt;00:00, 2.53MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
