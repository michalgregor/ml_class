{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_S-z0oogD8gL",
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform, which provides free hardware acceleration. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook, using a local GPU.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "rhIBCV6AD4qB",
    "outputId": "ac3604fc-db4d-4f68-e4df-146036123945"
   },
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mi0rP21wD4kf"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "ynFqI0klD4gU",
    "outputId": "a1aef434-4f22-4d1e-dbb8-9d02f3ca0319"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(\"https://www.dropbox.com/s/djnjkz456tbgfnk/lion.png?dl=1\", directory=\"data\")\n",
    "download_file_maybe_extract(\"https://www.dropbox.com/s/ma25i7w3jpqex2a/imagenet_classes?dl=1\", directory=\"data\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y7J1si6ND4Yz"
   },
   "outputs": [],
   "source": [
    "#@title -- Auxiliary Functions -- { display-mode: \"form\" }\n",
    "\n",
    "with open(\"data/imagenet_classes\", \"r\") as file:\n",
    "    classes = [c[:-1] for c in file.readlines()]\n",
    "\n",
    "def decode_proba(proba, top=5):\n",
    "    if isinstance(proba, torch.Tensor):\n",
    "        proba = proba.cpu().numpy()\n",
    "        \n",
    "    proba = proba.ravel()\n",
    "    ind = np.argsort(proba)\n",
    "    \n",
    "    for c in reversed(ind[-top:]):\n",
    "        print(\"{}:\\t{} ({})\".format(\n",
    "            np.array2string(proba[c], precision=5,\n",
    "                            suppress_small=False),\n",
    "            classes[c], c))\n",
    "\n",
    "class PermuteTransform:\n",
    "    def __init__(self, *dims):\n",
    "        self.dims = dims\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x.permute(*self.dims)\n",
    "\n",
    "class UnsqueezeTransform:\n",
    "    def __init__(self, dim=0):\n",
    "        self.dim = dim\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x.unsqueeze(self.dim)\n",
    "\n",
    "class SqueezeTransform:\n",
    "    def __init__(self, dim=0):\n",
    "        self.dim = dim\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x.squeeze(self.dim)\n",
    "\n",
    "class FromTensorTransform:\n",
    "    def __call__(self, x):\n",
    "        return x.detach().cpu().numpy()\n",
    "\n",
    "class ClampTransform:\n",
    "    def __init__(self, min=0., max=1.):\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if not self.min is None:\n",
    "            x = torch.clamp_min(x, self.min)\n",
    "        \n",
    "        if not self.max is None:\n",
    "            x = torch.clamp_max(x, self.max)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ToDeviceTransform:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x.to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zepJcAkZg41q",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Adversarial Examples\n",
    "\n",
    "This notebook shows one relatively simple method for generating adversarial examples.\n",
    "\n",
    "Let us start by loading the 50-layer ResNet architecture pretrained on ImageNet. The network expects 224x224 images at its input and it is able to classify them into 1000 classes (their list is in file data/classes and will also be displayed in the code below).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "41d957b785fc4ab9af193b2be27e9454",
      "c3f5bbcbb9bd447aab1c56e26679f6ab",
      "df4c25528fb54e5cbf042da35cd2482d",
      "53c11550cf774c74b55795d8b728b4f3",
      "741f876d9d4548fb816fb19d88699de2",
      "2018dc42995845e38d8a09e17637e822",
      "dc40b2301a5a476ebff016420975bd58",
      "abdf71e3fc1e46559a213690914fdc74"
     ]
    },
    "colab_type": "code",
    "id": "ESjg0cKOYmgu",
    "outputId": "65987dce-61cb-4910-ca90-f9b4158900ba"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "weights = models.ResNet50_Weights.IMAGENET1K_V1\n",
    "model = models.resnet50(weights=weights).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zepJcAkZg41q",
    "tags": [
     "en"
    ]
   },
   "source": [
    "As usual, we need to make sure that we preprocess input images in a way analogous to that in which they were preprocessed when the chosen weights were trained. In this case, we are just going to replicate the normalization since the adversarial examples generated using our rudimentary method are going to be quite fragile and could be damaged by the cropping and resizing.\n",
    "\n",
    "Also, we are going to need a reverse operation – for when we've come up with an adversarial example and we want to display it. To this end, we are going to define `normalize` and `denormalize` based on the normalization from `weights.transforms()`.\n",
    "\n",
    "Finally, we are going to add some further transforms such as permuting the dimensions to get from (width, height, channels) to (channels, width, height) format, operations `squeeze` and `unsqueeze` to add/remove the batch dimensions and such. This could be done outside of `image_transform` and `image_detransform`, but this way we can keep all the transformations together and make our code a bit clearer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_transforms = weights.transforms()\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=weights_transforms.mean,\n",
    "    std=weights_transforms.std\n",
    ")\n",
    "\n",
    "denormalize = transforms.Normalize(\n",
    "    mean=[-tm/sm for tm, sm in zip(weights_transforms.mean, weights_transforms.std)],\n",
    "    std=[1.0/ts for ts in weights_transforms.std]\n",
    ")\n",
    "\n",
    "image_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    normalize,\n",
    "    UnsqueezeTransform(),\n",
    "    ToDeviceTransform(device)\n",
    "])\n",
    "\n",
    "image_detransform = torchvision.transforms.Compose([\n",
    "    SqueezeTransform(),\n",
    "    denormalize,\n",
    "    PermuteTransform(1, 2, 0),\n",
    "    ClampTransform(0., 1.),\n",
    "    FromTensorTransform()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JmOKRCAphTkV",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Parameters\n",
    "\n",
    "We select the target class here: i.e. the class that we will try to get our image to be misclassifed into.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSpNgmDehX73"
   },
   "outputs": [],
   "source": [
    "# target_class = 231 # collie\n",
    "# target_class = 413 # assault rifle\n",
    "# target_class = 847 # tank\n",
    "target_class = 409 # analog clock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hLlf_JnbiJ3w",
    "tags": [
     "en"
    ]
   },
   "source": [
    "To get the list of all the classes uncomment and run the following cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TIjDOu9dhYBd"
   },
   "outputs": [],
   "source": [
    "# for ic, c in enumerate(classes):\n",
    "#     print(\"{}:\\t{}\".format(ic, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-BgUXNQi3rL",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Loading and Preprocessing the Original Image\n",
    "\n",
    "Next we are going to load and display the original image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "xyArDh9fD36P",
    "outputId": "16a7c1af-f4fc-4a3a-cd32-78a89cd90191"
   },
   "outputs": [],
   "source": [
    "img = plt.imread(\"data/lion.png\")\n",
    "plt.imshow(img); plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_i_aIDJFjVA0",
    "tags": [
     "en"
    ]
   },
   "source": [
    " We will apply the preprocessing that our pretrained neural net expects using function `image_transform`. We will then run the preprocessed image through the net and display the top-5 predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = image_transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_logit = model(img_t)\n",
    "    y_proba = torch.nn.functional.softmax(y_logit, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_proba(y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04aMSI4Mjl1G",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Constructing the Loss Function\n",
    "\n",
    "Our next step will be to construct the loss function that we are going to minimize in order to get our adversarial image. Since it is the adversarial image that we are going to be optimizing, let us create a separate tensor for it. Given that the image is supposed to look like the original image, the sensible thing, of course, is to initialize it by copying the original.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w8A6FP_hhF6r"
   },
   "outputs": [],
   "source": [
    "adv_t = img_t.clone().detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9D677ndMkbx0",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Having created our adversarial tensor, we will also wrap the target class in a tensor (of type `long`) and make sure it is transferred to the correct device.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0st-aMZOhGBe"
   },
   "outputs": [],
   "source": [
    "target_class_t = torch.as_tensor([target_class], dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOp5Iir5lCod",
    "tags": [
     "en"
    ]
   },
   "source": [
    "When computing the loss, we:\n",
    "\n",
    "* Run the adversarial example through the network to compute its output `y`;\n",
    "* We want the input to be misclassified into `target_class_t` so we construct the deception loss as the cross entropy loss with `y` and `target_class_t` as parameters (let us recall that we also use cross entropy when training a network to predict certain classes);\n",
    "* We construct the similarity loss as the $L^1$ distance between the adversarial image and the original image;\n",
    "* We add the two losses up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HLbMN7vNEws9"
   },
   "outputs": [],
   "source": [
    "def compute_loss():\n",
    "    y = model(adv_t)\n",
    "    deception_loss = torch.nn.functional.cross_entropy(y, target_class_t)\n",
    "    similarity_loss = torch.nn.functional.l1_loss(adv_t, img_t)\n",
    "    loss = deception_loss + similarity_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "93aDZc-emC2e",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### The Optimization\n",
    "\n",
    "We will create an optimizer and provide it with the parameters that it is going to be optimizing: tensor `adv_t` in this case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2g3JbgsZmwJ5"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([adv_t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ixIwQt0Omy9E",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We define a function that the optimizer is going to run at each step:\n",
    "\n",
    "* Zero out the gradients from the previous step.\n",
    "* Compute the loss function.\n",
    "* Backpropagate the gradients.\n",
    "The updating of the parameters is, of course, going to be handled by the optimizer itself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nyTg-cozgL9y"
   },
   "outputs": [],
   "source": [
    "def opti_step():\n",
    "    optimizer.zero_grad()\n",
    "    loss = compute_loss()\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dsH0bn6meIK",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We run the optimizer for a couple of epochs and display the losses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "R4CRsaCo48b8",
    "outputId": "e4f04b73-50fd-468b-83dc-4578ed0cdc67"
   },
   "outputs": [],
   "source": [
    "for epoch in range(500):\n",
    "    optimizer.step(opti_step)\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"Epoch {}; loss {}.\".format(epoch, compute_loss().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgeNJcsKnFri",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Displaying the Adversarial Example\n",
    "\n",
    "We will process the resulting adversarial example to transform it from a tensor back to a natural image that can be visualized. We also run the adversarial image through our network to make sure that it really does get misclassified. If everything worked out correctly, the image should now get classified as an analog clock or whatever other target class that we chose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = image_detransform(adv_t)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_logit = model(image_transform(adv))\n",
    "    y_proba = torch.nn.functional.softmax(y_logit, dim=1)\n",
    "\n",
    "decode_proba(y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXi9dXhcn4bt",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We can now plot both: the original image and the adversarial image side by side.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "pNFUSSA-nTbW",
    "outputId": "384ef011-cc83-48c2-9887-24a1cd2d2886"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=[10, 6])\n",
    "\n",
    "axes[0].imshow(img)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title(\"the original image\")\n",
    "\n",
    "axes[1].imshow(adv)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title(\"the adversarial example\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "omNq7w_Ln1m9",
    "tags": [
     "en"
    ]
   },
   "source": [
    "The images are going to be visually indistinguishable. To show that they are really not the same and how they differ, we will compute and display their absolute pixel-wise difference (averaging over the colour channels).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "SX2VWgtx48Qd",
    "outputId": "7a36abdd-4d8f-43bb-ab7d-31ea7d5eab19"
   },
   "outputs": [],
   "source": [
    "diff = np.abs(img - adv).mean(axis=-1)\n",
    "plt.imshow(diff, cmap='Greys')\n",
    "plt.axis('off')\n",
    "plt.colorbar(label=\"pixel-wise difference (range [0, 1])\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_heFhuoZN2iH",
    "tags": [
     "en"
    ]
   },
   "source": [
    "---\n",
    "### Task 1: A Different Image and Target Class\n",
    "\n",
    "**Apply the same procedure to a different image and target class.** \n",
    "\n",
    "Note: New images can be uploaded **directly through the notebook interface**  or alternatively using:\n",
    "\n",
    "```\n",
    "from google.colab import files\n",
    "content_img = files.upload()\n",
    "filename = list(content_img)[0]\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nqCKWHGN2iU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "8_adversarial_examples.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2018dc42995845e38d8a09e17637e822": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41d957b785fc4ab9af193b2be27e9454": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df4c25528fb54e5cbf042da35cd2482d",
       "IPY_MODEL_53c11550cf774c74b55795d8b728b4f3"
      ],
      "layout": "IPY_MODEL_c3f5bbcbb9bd447aab1c56e26679f6ab"
     }
    },
    "53c11550cf774c74b55795d8b728b4f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abdf71e3fc1e46559a213690914fdc74",
      "placeholder": "​",
      "style": "IPY_MODEL_dc40b2301a5a476ebff016420975bd58",
      "value": " 97.8M/97.8M [00:00&lt;00:00, 234MB/s]"
     }
    },
    "741f876d9d4548fb816fb19d88699de2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "abdf71e3fc1e46559a213690914fdc74": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3f5bbcbb9bd447aab1c56e26679f6ab": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc40b2301a5a476ebff016420975bd58": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df4c25528fb54e5cbf042da35cd2482d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2018dc42995845e38d8a09e17637e822",
      "max": 102502400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_741f876d9d4548fb816fb19d88699de2",
      "value": 102502400
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
