{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7h-gdSUXtrq",
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform, which provides free hardware acceleration. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook, using a local GPU.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vt9sKp4g5vQC",
    "outputId": "5345464f-1956-45b0-eda8-e5d07e65630d"
   },
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install captum\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3E0t0wPS5vRa"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "from captum.attr import IntegratedGradients, Saliency, GuidedBackprop\n",
    "from captum.attr import GradientShap\n",
    "from captum.attr import Occlusion\n",
    "from captum.attr import visualization as viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KdwWHj8V5vQo"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "from class_utils.download import download_file_maybe_extract, download_files_maybe_extract\n",
    "DATA_HOME = \"https://github.com/michalgregor/ml_notebooks/blob/main/data/{}?raw=1\"\n",
    "\n",
    "download_files_maybe_extract([\n",
    "    DATA_HOME.format(\"imagenet_classes\"),\n",
    "    DATA_HOME.format(\"images/toucan.png\"),\n",
    "    DATA_HOME.format(\"images/raccoon_example.jpg\"),\n",
    "    DATA_HOME.format(\"images/nails.jpg\"),\n",
    "    DATA_HOME.format(\"images/screws.jpg\"),\n",
    "    DATA_HOME.format(\"images/ambulance.jpg\")\n",
    "], directory=\"data\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aU2ZTD0Q5vR6"
   },
   "outputs": [],
   "source": [
    "#@title -- Auxiliary Functions -- { display-mode: \"form\" }\n",
    "with open(\"data/imagenet_classes\", \"r\") as file:\n",
    "    class_names = [c[:-1] for c in file.readlines()]\n",
    "\n",
    "def resize_image(img, size=224):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size)\n",
    "    ])\n",
    "    return np.asarray(transform(img))\n",
    "\n",
    "def decode_proba(proba, top=5, verbose=True):\n",
    "    with torch.no_grad():\n",
    "        probs, classes = torch.topk(proba, 5)\n",
    "        probs = probs[0]\n",
    "        classes = classes[0]\n",
    "\n",
    "    if verbose:\n",
    "        for p, c in zip(probs.cpu().numpy(),\n",
    "                        classes.cpu().numpy()):\n",
    "            print(\"{}:\\t{} ({})\".format(\n",
    "                np.array2string(p, precision=5),\n",
    "                class_names[c], c))\n",
    "            \n",
    "    return probs, classes\n",
    "\n",
    "vis_kwargs_default = dict(\n",
    "    methods=['original_image', 'alpha_scaling', 'heat_map'],\n",
    "    cmap=\"viridis\",\n",
    "    show_colorbar=True,\n",
    "    signs=['all', 'absolute_value', 'absolute_value'],\n",
    "    fig_size=(12.5, 4)\n",
    ")\n",
    "\n",
    "class ProcImage:\n",
    "    def __init__(self, model, weights, device, verbose=True):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.verbose = verbose\n",
    "\n",
    "        weight_transforms = weights.transforms()\n",
    "        self.normalize = transforms.Normalize(\n",
    "            mean=weight_transforms.mean,\n",
    "            std=weight_transforms.std\n",
    "        )\n",
    "\n",
    "    def preproc_image(self, img, size=224):\n",
    "        if not isinstance(img, Image.Image):\n",
    "            img = torchvision.transforms.functional.to_pil_image(img)\n",
    "\n",
    "        img = img.convert('RGB')\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(size),\n",
    "            transforms.ToTensor(),\n",
    "            self.normalize\n",
    "        ])\n",
    "\n",
    "        tensor = transform(img).unsqueeze(0)\n",
    "        return tensor\n",
    "\n",
    "    def __call__(self, img_path):\n",
    "        img = Image.open(img_path)\n",
    "        img_preproc = self.preproc_image(img)\n",
    "        img_resized = resize_image(img)\n",
    "        \n",
    "        if not self.device is None:\n",
    "            img_preproc = img_preproc.to(self.device)\n",
    "            self.model.to(self.device)\n",
    "\n",
    "        self.model.eval()\n",
    "        img_preproc.requires_grad = True\n",
    "        \n",
    "        output = torch.nn.functional.softmax(self.model(img_preproc), dim=1)\n",
    "        pred_class = decode_proba(output, verbose=True)[1][0]\n",
    "\n",
    "        return img_preproc, img_resized, pred_class\n",
    "\n",
    "def visualize_attrib(attrib, **kwargs):\n",
    "    kwargs_merged = dict(**vis_kwargs_default)\n",
    "    kwargs_merged.update(kwargs)\n",
    "    attrib = np.transpose(attrib.squeeze().cpu().detach().numpy(), (1,2,0))\n",
    "\n",
    "    # workaround for a matplotlib-related bug in captum\n",
    "    visualize_image_attr_multiple(attrib, img_resized, **kwargs_merged)\n",
    "    #viz.visualize_image_attr_multiple(attrib, img_resized, **kwargs_merged)\n",
    "\n",
    "# workaround for a matplotlib-related bug in captum\n",
    "\n",
    "from matplotlib.figure import Figure\n",
    "from captum.attr._utils.visualization import (\n",
    "    _prepare_image, ImageVisualizationMethod, _normalize_attr,\n",
    "    VisualizeSign, make_axes_locatable, LinearSegmentedColormap\n",
    ")\n",
    "\n",
    "def visualize_image_attr(\n",
    "    attr, original_image=None, method=\"heat_map\", sign=\"absolute_value\",\n",
    "    plt_fig_axis=None, outlier_perc=2, cmap=None, alpha_overlay=0.5,\n",
    "    show_colorbar=False, title=None, fig_size=(6, 6), use_pyplot=True\n",
    "):\n",
    "    # Create plot if figure, axis not provided\n",
    "    if plt_fig_axis is not None:\n",
    "        plt_fig, plt_axis = plt_fig_axis\n",
    "    else:\n",
    "        if use_pyplot:\n",
    "            plt_fig, plt_axis = plt.subplots(figsize=fig_size)\n",
    "        else:\n",
    "            plt_fig = Figure(figsize=fig_size)\n",
    "            plt_axis = plt_fig.subplots()\n",
    "\n",
    "    if original_image is not None:\n",
    "        if np.max(original_image) <= 1.0:\n",
    "            original_image = _prepare_image(original_image * 255)\n",
    "    elif ImageVisualizationMethod[method] != ImageVisualizationMethod.heat_map:\n",
    "        raise ValueError(\n",
    "            \"Original Image must be provided for\"\n",
    "            \"any visualization other than heatmap.\"\n",
    "        )\n",
    "\n",
    "    # Remove ticks and tick labels from plot.\n",
    "    plt_axis.xaxis.set_ticks_position(\"none\")\n",
    "    plt_axis.yaxis.set_ticks_position(\"none\")\n",
    "    plt_axis.set_yticklabels([])\n",
    "    plt_axis.set_xticklabels([])\n",
    "    plt_axis.grid(visible=False)\n",
    "\n",
    "    heat_map = None\n",
    "    # Show original image\n",
    "    if ImageVisualizationMethod[method] == ImageVisualizationMethod.original_image:\n",
    "        assert (\n",
    "            original_image is not None\n",
    "        ), \"Original image expected for original_image method.\"\n",
    "        if len(original_image.shape) > 2 and original_image.shape[2] == 1:\n",
    "            original_image = np.squeeze(original_image, axis=2)\n",
    "        plt_axis.imshow(original_image)\n",
    "    else:\n",
    "        # Choose appropriate signed attributions and normalize.\n",
    "        norm_attr = _normalize_attr(attr, sign, outlier_perc, reduction_axis=2)\n",
    "\n",
    "        # Set default colormap and bounds based on sign.\n",
    "        if VisualizeSign[sign] == VisualizeSign.all:\n",
    "            default_cmap = LinearSegmentedColormap.from_list(\n",
    "                \"RdWhGn\", [\"red\", \"white\", \"green\"]\n",
    "            )\n",
    "            vmin, vmax = -1, 1\n",
    "        elif VisualizeSign[sign] == VisualizeSign.positive:\n",
    "            default_cmap = \"Greens\"\n",
    "            vmin, vmax = 0, 1\n",
    "        elif VisualizeSign[sign] == VisualizeSign.negative:\n",
    "            default_cmap = \"Reds\"\n",
    "            vmin, vmax = 0, 1\n",
    "        elif VisualizeSign[sign] == VisualizeSign.absolute_value:\n",
    "            default_cmap = \"Blues\"\n",
    "            vmin, vmax = 0, 1\n",
    "        else:\n",
    "            raise AssertionError(\"Visualize Sign type is not valid.\")\n",
    "        cmap = cmap if cmap is not None else default_cmap\n",
    "\n",
    "        # Show appropriate image visualization.\n",
    "        if ImageVisualizationMethod[method] == ImageVisualizationMethod.heat_map:\n",
    "            heat_map = plt_axis.imshow(norm_attr, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "        elif (\n",
    "            ImageVisualizationMethod[method]\n",
    "            == ImageVisualizationMethod.blended_heat_map\n",
    "        ):\n",
    "            assert (\n",
    "                original_image is not None\n",
    "            ), \"Original Image expected for blended_heat_map method.\"\n",
    "            plt_axis.imshow(np.mean(original_image, axis=2), cmap=\"gray\")\n",
    "            heat_map = plt_axis.imshow(\n",
    "                norm_attr, cmap=cmap, vmin=vmin, vmax=vmax, alpha=alpha_overlay\n",
    "            )\n",
    "        elif ImageVisualizationMethod[method] == ImageVisualizationMethod.masked_image:\n",
    "            assert VisualizeSign[sign] != VisualizeSign.all, (\n",
    "                \"Cannot display masked image with both positive and negative \"\n",
    "                \"attributions, choose a different sign option.\"\n",
    "            )\n",
    "            plt_axis.imshow(\n",
    "                _prepare_image(original_image * np.expand_dims(norm_attr, 2))\n",
    "            )\n",
    "        elif ImageVisualizationMethod[method] == ImageVisualizationMethod.alpha_scaling:\n",
    "            assert VisualizeSign[sign] != VisualizeSign.all, (\n",
    "                \"Cannot display alpha scaling with both positive and negative \"\n",
    "                \"attributions, choose a different sign option.\"\n",
    "            )\n",
    "            \n",
    "            original_image = original_image[..., :3]\n",
    "            plt_axis.imshow(\n",
    "                np.concatenate(\n",
    "                    [\n",
    "                        original_image,\n",
    "                        _prepare_image(np.expand_dims(norm_attr, 2) * 255),\n",
    "                    ],\n",
    "                    axis=2,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise AssertionError(\"Visualize Method type is not valid.\")\n",
    "\n",
    "    # Add colorbar. If given method is not a heatmap and no colormap is relevant,\n",
    "    # then a colormap axis is created and hidden. This is necessary for appropriate\n",
    "    # alignment when visualizing multiple plots, some with heatmaps and some\n",
    "    # without.\n",
    "    if show_colorbar:\n",
    "        axis_separator = make_axes_locatable(plt_axis)\n",
    "        colorbar_axis = axis_separator.append_axes(\"bottom\", size=\"5%\", pad=0.1)\n",
    "        if heat_map:\n",
    "            plt_fig.colorbar(heat_map, orientation=\"horizontal\", cax=colorbar_axis)\n",
    "        else:\n",
    "            colorbar_axis.axis(\"off\")\n",
    "    if title:\n",
    "        plt_axis.set_title(title)\n",
    "\n",
    "    if use_pyplot:\n",
    "        plt.show()\n",
    "\n",
    "    return plt_fig, plt_axis\n",
    "\n",
    "\n",
    "def visualize_image_attr_multiple(\n",
    "    attr, original_image, methods, signs, titles=None,\n",
    "    fig_size=(8, 6), use_pyplot=True, **kwargs\n",
    "):\n",
    "    assert len(methods) == len(signs), \"Methods and signs array lengths must match.\"\n",
    "    if titles is not None:\n",
    "        assert len(methods) == len(titles), (\n",
    "            \"If titles list is given, length must \" \"match that of methods list.\"\n",
    "        )\n",
    "    if use_pyplot:\n",
    "        plt_fig = plt.figure(figsize=fig_size)\n",
    "    else:\n",
    "        plt_fig = Figure(figsize=fig_size)\n",
    "    plt_axis = plt_fig.subplots(1, len(methods))\n",
    "\n",
    "    # When visualizing one\n",
    "    if len(methods) == 1:\n",
    "        plt_axis = [plt_axis]\n",
    "\n",
    "    for i in range(len(methods)):\n",
    "        visualize_image_attr(\n",
    "            attr,\n",
    "            original_image=original_image,\n",
    "            method=methods[i],\n",
    "            sign=signs[i],\n",
    "            plt_fig_axis=(plt_fig, plt_axis[i]),\n",
    "            use_pyplot=False,\n",
    "            title=titles[i] if titles else None,\n",
    "            **kwargs,\n",
    "        )\n",
    "    plt_fig.tight_layout()\n",
    "    if use_pyplot:\n",
    "        plt.show()\n",
    "    return plt_fig, plt_axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEc0N6iSYh2E",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Visual Interpretation of Neural Networks\n",
    "\n",
    "Neural nets with deep learning represent a powerful machine learning paradigm. However, they are not known for their interpretability. Nevertheless, there are a few techniques that can provide some insight into whether a neural net is doing what it should. For tabular data, predictions can, of course, be explained with methods such as LIME. But there is also a couple of good techniques that work for images and we are going to showcase some of them in this notebook.\n",
    "\n",
    "### Loading the Model\n",
    "\n",
    "We will start by loading a pretrained model that we are going to be testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VP0l6CIW5vTB"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = models.ResNet50_Weights.DEFAULT\n",
    "model = models.resnet50(weights=weights).to(device)\n",
    "proc_image = ProcImage(model, weights, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pUgUmzA9_Km",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Saliency\n",
    "\n",
    "We will start with a concept known as visual saliency. The idea is simple: we will just use backpropagation (autodiff) to compute the sensitivity of the prediction to the various input pixels and visualize the result: the saliency map. The pixels in the saliency map will, roughly speaking, light up in proportion to their relevance to the prediction. This allows us to inspect whether the network is paying attention to the correct portions of the image: i.e. that it actually predicts label \"plane\" because it recognizes the plane and not because most of the pixels in the background are blue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IHGiRF25vTT"
   },
   "outputs": [],
   "source": [
    "attributor = Saliency(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mdC2dm0XtuQ",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Next we are going to load an image of a toucan, run it through our model and display the saliency map. We will see that the area around the toucan will light up the most. Other areas will light up too though. As we will see in a bit, in this case, this is more a property of the visualization method than of the model: the standard gradient is not a great indicator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "A4nj4pOQHrMr",
    "outputId": "cd57216b-fa90-4a94-ad9e-232209df6fe5"
   },
   "outputs": [],
   "source": [
    "img_preproc, img_resized, pred_class = proc_image('data/toucan.png')\n",
    "attrib = attributor.attribute(img_preproc, target=pred_class)\n",
    "visualize_attrib(attrib, outlier_perc=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0i0WopP_HrRE",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Guided Backprop\n",
    "\n",
    "The second method we are going to examine will give us much better results. It is called guided backprop and it is actually not that different from saliency: it too relies on a version of the gradient. The difference is that the gradient propagation through the ReLU activation function is modified so that only non-negative gradients get passed through. This effectively ignores signals that contribute negatively to the activation of our class.\n",
    "\n",
    "Since the current version of the `captum` package seems to be having problems with inplace ReLUs due to some changes in `torch` and `torchvision`, we are also going to run over our model first and change the `.inplace` flag of all ReLUs to `False`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzvPDEJPPglp"
   },
   "outputs": [],
   "source": [
    "for module in model.modules():\n",
    "    if isinstance(module, torch.nn.ReLU):\n",
    "        module.inplace=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGq513CKHp9R"
   },
   "outputs": [],
   "source": [
    "attributor = GuidedBackprop(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEQy_XOV9_Kw",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We will again apply our attributor to the toucan image: this time the output should be clearly focused on the toucan's outline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "EJrVVBr-7r9n",
    "outputId": "c7908229-24a7-4902-f383-310c57a11f88"
   },
   "outputs": [],
   "source": [
    "img_preproc, img_resized, pred_class = proc_image('data/toucan.png')\n",
    "attrib = attributor.attribute(img_preproc, target=pred_class)\n",
    "visualize_attrib(attrib, outlier_perc=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAQurIXT9_Kz",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Fine, now let's try the same on a couple more images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oPoL-ycHMeRT",
    "outputId": "2c5ea6ca-cf4b-4754-9d15-65ba93169a44"
   },
   "outputs": [],
   "source": [
    "for img_path in ['data/ambulance.jpg', 'data/screws.jpg', 'data/nails.jpg', 'data/raccoon_example.jpg']:\n",
    "    img_preproc, img_resized, pred_class = proc_image(img_path)\n",
    "    attrib = attributor.attribute(img_preproc, target=pred_class)\n",
    "    visualize_attrib(attrib, outlier_perc=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiEndezsMnPB",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Occlusion\n",
    "\n",
    "There is a number of other approaches based on the gradient: to explore some of them, browse through the documentation of the `captum` package which we are using to produce the visualization in this notebook.\n",
    "\n",
    "We will do just one more experiment – with a visualization method based on a completely different principle: occlusion. The idea is that if a pixel (or a group of pixels) is important to the prediction, then if they are occluded (e.g. set to zeros), this is going to have a very significant impact on the prediction. So if we slide over the image with an occlusion window of some size and observe the impacts, we determine which parts of the image the output is most sensitive to.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1CP7zlO_bwN"
   },
   "outputs": [],
   "source": [
    "attributor = Occlusion(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkBmDURL9_K3",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Let us again try this using our toucan image. We will use a relatively small occlusion window first. As we can see, there is clear focus on the toucan, but some other areas of the image lit up as well – such as some parts of the tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "W_hC6-UrK-M5",
    "outputId": "c6f0d4eb-8e19-477f-a1d3-f6cde3f4affd"
   },
   "outputs": [],
   "source": [
    "img_preproc, img_resized, pred_class = proc_image('data/toucan.png')\n",
    "attrib = attributor.attribute(img_preproc, target=pred_class,\n",
    "                              strides = (3, 20, 20),\n",
    "                              sliding_window_shapes=(3, 30, 30),\n",
    "                              baselines=0)\n",
    "visualize_attrib(attrib, outlier_perc=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pd-ntIgML9ad",
    "tags": [
     "en"
    ]
   },
   "source": [
    "To show the impact of the occlusion window's size, we will run the visualization again with a larger window. Now the area around the toucan's beak seems to be the most relevant one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "LhW_aQXIJAiz",
    "outputId": "1221da9e-6f77-4513-9748-f07fd201cd3f"
   },
   "outputs": [],
   "source": [
    "img_preproc, img_resized, pred_class = proc_image('data/toucan.png')\n",
    "attrib = attributor.attribute(img_preproc, target=pred_class,\n",
    "                              strides = (3, 50, 50),\n",
    "                              sliding_window_shapes=(3, 60, 60),\n",
    "                              baselines=0)\n",
    "visualize_attrib(attrib, outlier_perc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlpHbNGNdKr9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [
    "tEc0N6iSYh2E"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
