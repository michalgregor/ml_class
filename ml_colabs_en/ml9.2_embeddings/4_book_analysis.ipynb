{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UamXwtsxoi6a",
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n",
    "The notebook is inspired by similar analyses from other authors (such as [Character Networks Visualization for Les Misérables](https://studentwork.prattsi.org/infovis/labs/character-networks-visualization-for-les-miserables/) and [Game of Thrones – Co-occurrence Network of Characters](https://studentwork.prattsi.org/infovis/labs/game-of-thrones-co-occurrence-network-of-characters/)). However, the code is original and – unlike the other works – the entire process, including the plotting, is done in Python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 899
    },
    "colab_type": "code",
    "id": "tpza3PdGXK6v",
    "outputId": "f85cf0de-2655-4d75-9e32-917e3c2aac7a"
   },
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install umap-learn python-louvain textblob\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WHqkgEF8PZuX"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import minmax_scale, MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import community # package python-louvain\n",
    "import networkx as nx\n",
    "from umap import UMAP\n",
    "from textblob import TextBlob\n",
    "\n",
    "from IPython.display import display\n",
    "from matplotlib.colors import to_hex\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "PwvKJoC0PZiQ",
    "outputId": "956fb8b6-0256-41ac-cd84-7cb29d20d2c7"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(\"https://www.dropbox.com/s/424vr9du2f480d9/three_musketeers.txt?dl=1\", directory=\"data\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# We also need some data from the nltk package\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zRXssoDSP2G4"
   },
   "outputs": [],
   "source": [
    "#@title -- Auxiliary Functions -- { display-mode: \"form\" }\n",
    "def refine_tags(tree, words, case_sensitive=True, infuse_text=False, tag=None):\n",
    "    class NOTHING: pass\n",
    "    \n",
    "    if isinstance(words, set):\n",
    "        words = {it: tag for it in words}\n",
    "    \n",
    "    if infuse_text:\n",
    "        make_item = lambda word, tag: (word, \"{}:{}\".format(tag, word))\n",
    "    else:\n",
    "        make_item = lambda word, tag: (word, tag)\n",
    "    \n",
    "    if case_sensitive:\n",
    "        normalize = lambda x: x\n",
    "    else:\n",
    "        normalize = lambda x: x.lower()\n",
    "        words = {normalize(k): v for k,v in words.items()}\n",
    "        \n",
    "    for item in tree:\n",
    "        if isinstance(item, tuple):\n",
    "            word, tag = item\n",
    "            words_tag = words.get(normalize(word), NOTHING())\n",
    "            \n",
    "            # not in dict: yield the original item\n",
    "            if isinstance(words_tag, NOTHING):\n",
    "                yield item\n",
    "            # tag is None: do not change the orginal tag\n",
    "            elif words_tag is None:                \n",
    "                yield make_item(word, tag)\n",
    "            # change the tag to words_tag\n",
    "            else:\n",
    "                yield make_item(word, words_tag)\n",
    "        else:\n",
    "            yield nltk.Tree(item.label(), refine_tags(item, words))\n",
    "\n",
    "def refine_chunks(chunked_sents, wordset, grammar, case_sensitive=True,\n",
    "                  infuse_text=False, tag=None):    \n",
    "    refined = (list(refine_tags(sent, wordset,\n",
    "                                case_sensitive=case_sensitive,\n",
    "                                infuse_text=infuse_text, tag=tag))\n",
    "                    for sent in chunked_sents)\n",
    "    rechunked = nltk.RegexpParser(grammar).parse_sents(refined)\n",
    "    return rechunked\n",
    "\n",
    "def mix_colors(colors):\n",
    "    return tuple(np.asarray(colors).mean(axis=0))\n",
    "\n",
    "def compute_cooccurence(entity_occurences, context=15, multi_count=False):\n",
    "    cooccurence = {}\n",
    "\n",
    "    for (ent1, occ1), (ent2, occ2) in itertools.combinations(\n",
    "        entity_occurences.items(), 2\n",
    "    ):\n",
    "        num_occ = 0\n",
    "        jstart = 0\n",
    "        \n",
    "        # for our algorithm to work, we need to make sure the occurrence sequences are sorted\n",
    "        occ1 = sorted(occ1)\n",
    "        occ2 = sorted(occ2)\n",
    "      \n",
    "        for i in range(len(occ1)):\n",
    "            for j in range(jstart, len(occ2)):\n",
    "                if occ1[i] >= occ2[j] - context and occ1[i] <= occ2[j] + context:\n",
    "                    # we have found a co-occurence\n",
    "                    num_occ += 1\n",
    "                    \n",
    "                    if not multi_count:\n",
    "                        # once we get a match, we increment i and set jstart\n",
    "                        # j + 1 so that we do not count the same co-occurrence\n",
    "                        # more than once\n",
    "                        jstart = j + 1\n",
    "                        continue\n",
    "                \n",
    "                elif occ1[i] < occ2[j] - context:\n",
    "                    # we continue with the next i, since occ2[j]\n",
    "                    # will only get larger\n",
    "                    break\n",
    "                    \n",
    "                elif occ1[i] > occ2[j] + context:\n",
    "                    # occ1[i] will only get larger, so we do not need to\n",
    "                    # consider this j in future iterations\n",
    "                    jstart = j + 1\n",
    "\n",
    "        cooccurence[(ent1, ent2)] = num_occ\n",
    "\n",
    "    return cooccurence\n",
    "  \n",
    "color_list = np.array([\n",
    "    (0, 150, 117),\n",
    "    (0, 196, 255),\n",
    "    (115, 192, 0),\n",
    "    (255, 85, 132),\n",
    "    (204, 173, 170),\n",
    "    (155, 116, 216),\n",
    "    \n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "    (150, 150, 150),\n",
    "]) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "opjWkgAYoi88",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Character Networks based on the Number of Co-occurrences\n",
    "\n",
    "In this notebook we are going to perform an interesting analysis, which joins approaches from natural language processing on one hand and from graph theory and network analysis on the other hand. Our goal will be to create a network of characters from a given book, to perform analyses on it and to plot the results. The \"closeness\" of the characters in the network will be determined by their number of co-occurences in the same context. Once we have formed the network, we will show how to compute various things, such as the centrality of the nodes, how to detect communities (groups of characters, which are somehow interrelated), etc. This kind of analysis has a lot of practical applications these days: including social network analysis, for an instance.\n",
    "\n",
    "Our overall procedure will be as follows:\n",
    "\n",
    "* To identify named entities in the text: in our case we will care about the book's characters.\n",
    "* Determine their closeness by counting how many times they co-occur in the same context.\n",
    "* Construct a network using this information.\n",
    "* Analyze the network and visualize the results.\n",
    "### Loading the Text\n",
    "\n",
    "The first step, of course, will be to load the text of the book. Since we are only working with a single book, the volume of our textual data will not be too large and we can afford to read all the data into memory at once. Otherwise we would be forced to split it into smaller chunks and process it piece by piece.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXFQjfIdPLgJ"
   },
   "outputs": [],
   "source": [
    "with open('data/three_musketeers.txt', 'r', encoding=\"utf8\") as f:\n",
    "    sample = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIeBLBxWoi9Z",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Preprocessing the Text\n",
    "\n",
    "The next step is to preprocess the text. We will first apply some very basic standardization: e.g. by replacing various special kinds of quotation marks and other punctuation using some canonical kinds. We will then split the text into individual sentences. The sentences will be tokenized: split into smaller units such as words and phrases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "mZCNsCgNoi9h",
    "outputId": "edec2fe3-74cb-400e-f5c4-9bddcb191479",
    "tags": [
     "en"
    ]
   },
   "outputs": [],
   "source": [
    "# We replace special quotation marks ‘’ using simple quotation marks '.\n",
    "trans_table = str.maketrans(\"‘’\", \"''\")\n",
    "sample = sample.translate(trans_table)\n",
    "\n",
    "# We split the text into sentences.\n",
    "sentences = nltk.sent_tokenize(sample)\n",
    "\n",
    "# We print a few sentences as an example.\n",
    "for sent in sentences[15:18]:\n",
    "    print(sent, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "gaSyVzrhPLge",
    "outputId": "04cbd7eb-0078-4dd9-eae9-3dab829a3833"
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "for tok_sent in tokenized_sentences[15:18]:\n",
    "    print(tok_sent, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xtGtLcpoi-E",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We will then tag all the tokens, which gives us additional information about their role in the sentence – e.g. whether they represent a noun, a verb, and so on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "gmlW5YZHPLgm",
    "outputId": "9b581293-8836-4c49-f847-13c4430f70da"
   },
   "outputs": [],
   "source": [
    "tagged_sentences = nltk.pos_tag_sents(tokenized_sentences)\n",
    "\n",
    "for tag_sent in tagged_sentences[15:18]:\n",
    "    print(tag_sent, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OQNKpvKIoi-V",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We will then try to extract more information about the structure of the text using chunking.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Hf2oqPGPPLgv",
    "outputId": "430d98c4-c49d-40d0-bc63-8bab4c7df5f0"
   },
   "outputs": [],
   "source": [
    "chunked_sentences_sample = nltk.ne_chunk_sents(tagged_sentences[15:18])\n",
    "\n",
    "for chunked_sent in chunked_sentences_sample:\n",
    "    print(chunked_sent, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i4zB71WMoi-m",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Entity Recognition\n",
    "\n",
    "Having applied chunking, we are ready to do named entity recognition. Given that the `nltk` package uses one of the traditional methods, which are not known for their excellent accuracy or robustness, and also given the character of each particular text, it may be necessary to apply some additional manual corrections after entity recognition, or even to go back and preprocess the input data some more to get good results.\n",
    "\n",
    "In any case, we will also need to keep track of the position where each entity occurred, so that we are later able to compute the number of co-occurrences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3nkQXEHoi-t",
    "tags": [
     "en"
    ]
   },
   "outputs": [],
   "source": [
    "# We apply named entity recognition.\n",
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences)\n",
    "\n",
    "# If there is a title before the name of an entity, we will try to pull it in.\n",
    "titles = {\"professor\", \"madame\", \"madam\", \"mr\", \"mr.\", \"mrs\", \"mrs.\",\n",
    "          \"miss\", \"uncle\", \"aunt\", \"lord\"}\n",
    "rechunked_sentences = refine_chunks(chunked_sentences, titles, tag=\"TIT\",\n",
    "                          grammar = r\"\"\"\n",
    "                              PERSON: {<TIT><PERSON>}\n",
    "                          \"\"\",\n",
    "                          case_sensitive=False)\n",
    "\n",
    "# We collect all the occurrences of the individual entities.\n",
    "# We also keep track of the final form of our pre-processed text.\n",
    "entity_occurences = defaultdict(list)\n",
    "chunk_list = []\n",
    "num_chunks = 0\n",
    "\n",
    "for sent_tree in rechunked_sentences:    \n",
    "    for node in sent_tree:\n",
    "        # a regular tagged token\n",
    "        if isinstance(node, tuple):\n",
    "            chunk_list.append(node[0])\n",
    "        \n",
    "        # a sub-tree corresponding to an entity\n",
    "        else:\n",
    "            identifier = \" \".join((leaf[0] for leaf in node.leaves())).strip()    \n",
    "            entity_occurences[identifier].append(num_chunks)\n",
    "            chunk_list.append(identifier)\n",
    "        \n",
    "        num_chunks += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bh7aSXvRoi-9",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Merging Different Names of the Same Entity\n",
    "\n",
    "Naturally, we cannot expect that the entity recognition process will be fully automatic. It is common for some book characters to be referenced by several different names (nicknames, enderaments, etc.). It is usually not going to be possible to merge these different names of the same entity correctly without having a full understanding of the text (which the existing methods just don't have). We will therefore have to do at least part of this work by hand.\n",
    "\n",
    "We will therefore display the recognized named entities, sorting them according to the nubmer of occurences. Where necessary, we will merge several different names into a single entity. This merging could be partly automated, e.g. if we had some other source of information, such as a web page containing a list of characters will all their different nicknames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QwgT47sgPLhJ",
    "outputId": "0590bddd-a752-44be-be8f-730d6a18c8cf"
   },
   "outputs": [],
   "source": [
    "for k, v in sorted(entity_occurences.items(), key=lambda it: -len(it[1])):\n",
    "    print(\"{}x\\t{}\".format(len(v), k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T8OG7vUKoi_J",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We will do the manual merging using a dictionary of the following form:\n",
    "\n",
    "```\n",
    "entity_dict = {\n",
    "    'entity_identifier': {'Entity's Name', 'Alternative Name 1', 'Alternative Name 2', ...},\n",
    "\n",
    "    ...\n",
    "}\n",
    "```\n",
    "Entities that occur very few times may be unimportant: we can drop them from the dictionary completely at this stage, if we so wish.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pbmty5F0PLhT"
   },
   "outputs": [],
   "source": [
    "entity_dict = {\n",
    "    \"Athos\": {\"Athos\", \"Monsieur Athos\"},\n",
    "    \"Milady\": {\"Milady\", \"Winter\", \"MILADY\"},\n",
    "    \"Porthos\": {\"Porthos\", \"Monsieur Porthos\", \"PORTHOS\"},\n",
    "    \"Aramis\": {\"Aramis\", \"Monsieur Aramis\", \"ARAMIS\"},\n",
    "    \"Felton\": {\"Felton\"},\n",
    "    \"Bonacieux\": {\"Bonacieux\", \"Madame Bonacieux\", \"Monsieur Bonacieux\"},\n",
    "    \"Treville\": {\"Treville\"},\n",
    "    \"Planchet\": {\"Planchet\"},\n",
    "    \"Buckingham\": {\"Buckingham\"},\n",
    "    \"Grimaud\": {\"Grimaud\"},\n",
    "    \"Bazin\": {\"Bazin\"},\n",
    "    \"Mousqueton\": {\"Mousqueton\"},\n",
    "    \"La Rochelle\": {\"La Rochelle\"},\n",
    "    \"Richelieu\": {\"Richelieu\", \"Eminence\", \"Cardinal\", \"Monsieur Cardinal\"},\n",
    "    \"d'Artagnan\": {\"Gascon\"},\n",
    "    \"Rochefort\": {\"Rochefort\"},\n",
    "    \"de Chevreuse\": {\"Madame de Chevreuse\"},\n",
    "    \"Madame Coquenard\": {\"Madame Coquenard\", \"Coquenard\"},\n",
    "    \"Monsieur Dessessart\": {\"Monsieur Dessessart\"},\n",
    "    \"Louis XIII\": {\"Louis XIII\", \"Louis\"},\n",
    "    \"Louis XIV\": {\"Louis XIV\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKbyl-vYoi_S",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We will now invert the dictionary (so that it maps all the alternative names to the same character ID).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-1Wq2XXPLhc"
   },
   "outputs": [],
   "source": [
    "reverse_entity_dict = defaultdict(set)\n",
    "\n",
    "for entity, names in entity_dict.items():\n",
    "    for name in names:\n",
    "        reverse_entity_dict[name].update({entity})\n",
    "        \n",
    "reverse_entity_dict = dict(reverse_entity_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ar0Yk8joi_f",
    "tags": [
     "en"
    ]
   },
   "source": [
    "If necessary, we can test whether we have perhaps forgotten any important entities:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2beY5r-iPLhk",
    "outputId": "8653474c-6c91-4585-e5e4-2a86c27588e9"
   },
   "outputs": [],
   "source": [
    "forgotten_entity_occurences = {k: v for k, v in entity_occurences.items() if not k in reverse_entity_dict}\n",
    "\n",
    "for k, v in sorted(forgotten_entity_occurences.items(), key=lambda it: -len(it[1])):\n",
    "    print(\"{}x\\t{}\".format(len(v), k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0BfX6UNvoi_r",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We transform the list of occurrences using the inverted entity dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UQdZCEoPLht"
   },
   "outputs": [],
   "source": [
    "translated_occurences = defaultdict(list)\n",
    "\n",
    "for entity, occurences in entity_occurences.items():\n",
    "    try:\n",
    "        for translated_entity in reverse_entity_dict[entity]:\n",
    "            translated_occurences[translated_entity].extend(occurences)\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15qDdmSwoi_2",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We use a predefined function to find the co-occurrences:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2PtYzoydPLh4"
   },
   "outputs": [],
   "source": [
    "cooccurence = compute_cooccurence(translated_occurences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GhnhMWvAojAA",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We store the results in CSV files: the nodes (the entities) and the eges (labelled by the numbers of co-occurrences) go into separate files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v14C4pQ8PLh_"
   },
   "outputs": [],
   "source": [
    "with open(\"output/nodes.csv\", \"w\") as nodes_file:\n",
    "    nodes_file.write(\"Id,Label,Occurences\\n\")\n",
    "   \n",
    "    for entity, occurences in translated_occurences.items():\n",
    "        nodes_file.write(\"{},{},{}\\n\".format(entity, entity, len(occurences)))\n",
    "\n",
    "with open(\"output/edges.csv\", \"w\") as edges_file:\n",
    "    edges_file.write(\"Source,Target,Type,id,weight\\n\")\n",
    "   \n",
    "    for i, ((ent1, ent2), num_cooccur) in enumerate(cooccurence.items()):\n",
    "        if num_cooccur > 0:\n",
    "            edges_file.write(\"{},{},Undirected,{},{}\\n\".format(ent1, ent2, i, num_cooccur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dmozKog4ojAI",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Constructing, Analyzing and Visualizing the Graph\n",
    "\n",
    "We will now construct the graph that has resulted from our textual analysis and apply graph analysis to it. We will do all these steps in Python. However, it would be just as easy to use the CSV files that we have created and build the graph using an external tool, such as [gephi](https://gephi.org/), the well-known software for graph visualization.\n",
    "\n",
    "Let us now create the graph using the Python package `networkx` and add all the nodes found in `nodes.csv`. We are interested in the nodes' id, character name (to be used as label) and the total number of occurences of the character in text, so we add these as data to the node:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ajnos9LFojAL"
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "nodes = pd.read_csv(\"output/nodes.csv\")\n",
    "for nid, (node, label, occurences) in nodes[[\"Id\", \"Label\", \"Occurences\"]].iterrows():  \n",
    "    G.add_node(node, label=label, occurences=occurences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4A5KF4NEojAT",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Next we read the `edges.csv` file and add all the edges to the graph. We weight the edges by the number of co-occurrences, as recorded in the CSV file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dctof5rcojAY"
   },
   "outputs": [],
   "source": [
    "edges = pd.read_csv(\"output/edges.csv\")\n",
    "G.add_weighted_edges_from((\n",
    "       (src, tgt, w)\n",
    "       for i, (src, tgt, w) in\n",
    "       edges[[\"Source\", \"Target\", \"weight\"]].iterrows()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_pTeN9N3ojAd",
    "tags": [
     "en"
    ]
   },
   "source": [
    "If the graph contains some isolated nodes (nodes that have no edges associated to them), we remove them to make the graph more readable. We also convert node labels to integers to make them easier to work with:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C3Df7YYuojAh"
   },
   "outputs": [],
   "source": [
    "G.remove_nodes_from(list(nx.isolates(G)))\n",
    "G = nx.convert_node_labels_to_integers(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tT_3d1xUojAm",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Page-Rank to Determine Node \"Importance\"\n",
    "\n",
    "When visualizing graphs there is a lot of ways to encode relevant information about the graph into the plot. One of these is to use the size of the nodes. In our case we will compute the centrality of each node using an indicator known as the page-rank. Loosely speaking, this will give us a measure of how important the node is in the graph. Once we have the page-ranks, we will rescale them into a reasonable range and base the size of the nodes on the results. We will also use the same approach to scale the fonts of the nodes' labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "acCXBBL0ojAp",
    "outputId": "db0fe7cd-79a8-4a16-aa4f-22b4ceed253d"
   },
   "outputs": [],
   "source": [
    "page_rank = nx.pagerank(G)\n",
    "sizes = [page_rank[node] for node in G.nodes()]\n",
    "sizes = minmax_scale(sizes, (80, 1000))\n",
    "\n",
    "font_scaler = MinMaxScaler((12, 20))\n",
    "font_scaler.fit(np.reshape(sizes, (-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pFFF0XdojAu",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Width and Transparency of the Edges\n",
    "\n",
    "To determine the width of the edges, we will merely rescale the edges' weights: i.e. the numbers of co-occurrences. The same values, but at a different scale will be used to determine the transparency of the edges. Making some of the edges less solid will have the effect of visually decluttering the plot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3T3g7f7ojAy"
   },
   "outputs": [],
   "source": [
    "widths = [edge[2] for edge in G.edges(data='weight')]\n",
    "widths = minmax_scale(widths, (1, 5))\n",
    "edge_alpha = minmax_scale(widths, (0.3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0oAJ6svtojA1",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Detection of Communities in the Graph\n",
    "\n",
    "Next, we will use community detection from package `community` to partition nodes into communities. Roughly speaking, it is possible to think about this as a graph equivalent of clustering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7YBj5CchRE-i"
   },
   "outputs": [],
   "source": [
    "parts = community.best_partition(G, resolution=1.0)\n",
    "num_parts = np.max(list(parts.values()))\n",
    "\n",
    "communities = [[] for i in range(num_parts+1)]\n",
    "for k, v in parts.items():\n",
    "    communities[v].append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HW8NxozxojA-",
    "tags": [
     "en"
    ]
   },
   "source": [
    "The identified communities will be used to determine node and edge colours:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QWjrx0rXri_t"
   },
   "outputs": [],
   "source": [
    "colors = np.asarray([to_hex(color_list[parts.get(node)]) for node in G.nodes()])\n",
    "cmap = lambda c: color_list[c]\n",
    "\n",
    "edge_colors = [to_hex(\n",
    "    mix_colors([cmap(parts.get(src)), cmap(parts.get(dest))]),\n",
    "      keep_alpha=True) for src, dest, w in G.edges(data='weight')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hLWcKHKZojBE",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### The 2-D Layout\n",
    "\n",
    "Since we are laying out of graph in a 2-dimensional plot, we will naturally need to assign some positions to all the nodes. Ideally, this should be done so that closely related nodes are plotted near each other and the edges do not overlap too much. There is a bunch of methods for finding graph layouts. However, not all of them will yield good results when the graphs are complex and there is a large number of connections. To find our layout, we will therefore slightly misuse UMAP – a method primarily intended for dimensionality reduction – and we will compute our layout using it.\n",
    "\n",
    "UMAP takes the distances between the nodes as its input. In our case, we will produce a distance matrix by inverting the weights of the edges (the numbers of co-occurrences). This will make UMAP group nodes that co-occur a lot together.\n",
    "\n",
    "To get the nodes from the same community to stick together, we pick the initial positions of all the nodes so that the communities are mapped to equidistant points along the circumference of a circle (all nodes that belong to the same community are initially mapped to that same position).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "t0yZTVVVojBI",
    "outputId": "9cd02d75-cd30-4bd0-f2a7-5725586f756a"
   },
   "outputs": [],
   "source": [
    "# set up the distance matrix for UMAP\n",
    "num_nodes = G.number_of_nodes()\n",
    "dist_mat = np.zeros([num_nodes, num_nodes])\n",
    "max_weight = edges[\"weight\"].max()\n",
    "\n",
    "for n1, n2, w in G.edges(data=\"weight\"):\n",
    "    invdist = w / max_weight\n",
    "    dist_mat[n1, n2] += invdist\n",
    "    dist_mat[n2, n1] += invdist\n",
    "dist_mat = dist_mat.max() - dist_mat\n",
    "\n",
    "# set up the initial node positions for UMAP\n",
    "num_communities = len(communities)\n",
    "community_angles = np.asarray(\n",
    "    [ic*2*3.14/num_communities for ic in range(num_communities)])\n",
    "community_centers = np.stack(\n",
    "    [np.sin(community_angles) * 100, np.cos(community_angles) * 100], axis=1)\n",
    "\n",
    "init = np.zeros([num_nodes, 2])\n",
    "for c, mem in enumerate(communities):\n",
    "    for n in mem:\n",
    "        init[n] = community_centers[c]\n",
    "\n",
    "# run UMAP\n",
    "umap = UMAP(\n",
    "    metric='precomputed',\n",
    "    init=init,\n",
    "    min_dist=10,\n",
    "    spread=50,\n",
    ")\n",
    "\n",
    "pos = umap.fit_transform(dist_mat)\n",
    "pos = {ipos: p for ipos, p in enumerate(pos)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f54i3TqNpB8y",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Plotting the Graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2d48RYBpx5m"
   },
   "source": [
    "A nakoniec nezostáva už nič iné než vykresliť samotný graf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "qCi-QWbhr_Oy",
    "outputId": "10e83ede-68bd-4dbf-a4b3-fb7622739f4d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[14, 10])\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(\n",
    "     G,\n",
    "     pos = pos,\n",
    "     node_size = sizes,\n",
    "     node_color = colors,\n",
    "     linewidths = 1,\n",
    ")\n",
    "\n",
    "# edges\n",
    "edge_collection = nx.draw_networkx_edges(\n",
    "    G,\n",
    "    pos,\n",
    "    width = widths,\n",
    "    edge_color = edge_colors,               \n",
    ")\n",
    "\n",
    "# transparency of edges\n",
    "edge_alpha_colors = [tuple(col[:3]) + (al,) for al, col\n",
    "                        in zip(edge_alpha, edge_collection.get_colors())]\n",
    "edge_collection.set_color(edge_alpha_colors)\n",
    "\n",
    "# edge labels\n",
    "text_collection = nx.draw_networkx_labels(G, pos,\n",
    "    labels = {node[0]: node[1].replace(\" \", \"\\n\")\n",
    "                  for node in G.nodes(data=\"label\")})\n",
    "\n",
    "# font sizes\n",
    "for node, textobj in text_collection.items():\n",
    "    rank = np.reshape([page_rank[node]], (1, -1))\n",
    "    textobj.set_fontsize(font_scaler.transform(rank)[0])\n",
    "\n",
    "# minor re-styling of the plot\n",
    "plt.gca().collections[0].set_edgecolor(\"k\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F6uO6fJoPLiF"
   },
   "source": [
    "## Sentimentálny kontext postáv\n",
    "\n",
    "Metódy analýzy textu nám umožňujú odhadovať aj sentiment textu (pozitívny, negatívny a pod.). Jednoduché prístupy ku analýze sentimentu implementuje napríklad balíček `TextBlob`. Presnejšie výsledky by bolo možné získať napríklad pomocou niektorej z metód založených na hlbokom učení, ale aj výsledky získané pomocou tejto metódy by mali na hrubú analýzu stačiť.\n",
    "\n",
    "Skúsme teda extrahovať kontexty, v ktorých sa mená jednotlivých postáv vyskytujú, a určiť ich sentiment. V grafe si potom zobrazíme s akým prevládajúcim sentimentom sa jednotlivé postavy v knihe spájajú.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TZ7ZwK-jPLiS"
   },
   "outputs": [],
   "source": [
    "# context radius\n",
    "blob_radius = 10\n",
    "\n",
    "names = []\n",
    "sentiments = []\n",
    "num_occurences = []\n",
    "\n",
    "# we extract context and accumulate their polarities\n",
    "for ent, occurences in translated_occurences.items():\n",
    "    sentiment = 0\n",
    "    \n",
    "    for occ in occurences:\n",
    "        sentiment += TextBlob(\" \".join(chunk_list[occ-blob_radius:occ+blob_radius])).polarity\n",
    "        \n",
    "    sentiment /= len(occurences)\n",
    "    \n",
    "    names.append(ent)\n",
    "    sentiments.append(sentiment)\n",
    "    num_occurences.append(len(occurences))\n",
    "\n",
    "# we track the accumulated sentiment but also the number of occurrences\n",
    "data = np.asarray([sentiments, num_occurences]).transpose()\n",
    "entity_sentiments = pd.DataFrame(data, columns=[\"sentiment\", \"occurences\"], index=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMc3QlPussUy"
   },
   "source": [
    "Vo výslednom grafe zobrazíme len entity, ktoré sa vyskytujú väčší počet krát. Entity s menším počtom výskytov okrem toho budeme vizualizovať transparentnejšou farbou, aby sme ich odlíšili.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "BfmxrvpSPLiZ",
    "outputId": "fb4f8c19-d7a0-498e-d862-0742ab4d60c3"
   },
   "outputs": [],
   "source": [
    "s = entity_sentiments[entity_sentiments[\"occurences\"] > 25]\n",
    "s_occ = minmax_scale(np.log(s[\"occurences\"].values), (0.3, 1.0))\n",
    "s_vals = s[\"sentiment\"].values\n",
    "s_index = s[\"sentiment\"].index\n",
    "\n",
    "abs_max = np.abs(s_vals).max()\n",
    "norm = plt.Normalize(vmin=-abs_max, vmax=abs_max)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(s_vals)), s_vals,\n",
    "  color=[(0, 0, 1, occ) if val >= 0 else \n",
    "         (1, 0, 0, occ) for val, occ in zip(s_vals, s_occ)]\n",
    ")\n",
    "\n",
    "plt.xticks(range(len(s_vals)), s_index, rotation=90)\n",
    "plt.ylabel(\"sentiment\")\n",
    "\n",
    "plt.subplots_adjust(left=0.12, right=0.9, top=0.9, bottom=0.3)\n",
    "plt.grid(linestyle='--')\n",
    "plt.gca().set_axisbelow(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vff_XszeojCC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "4_book_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
