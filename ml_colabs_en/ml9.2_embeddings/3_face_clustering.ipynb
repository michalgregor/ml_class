{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSqfo5tB5x95",
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform, which provides free hardware acceleration. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook, using a local GPU.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DxcaMsqjqX-v",
    "outputId": "230f225e-16df-4370-87a0-6b1b2db6a943"
   },
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "import numpy as np\n",
    "from packaging.version import parse as parse_version\n",
    "\n",
    "if parse_version(np.__version__) > parse_version('1.20.0'):\n",
    "    !{sys.executable} -m pip install lapjv\n",
    "else:\n",
    "    !{sys.executable} -m pip install lapjv==1.3.12\n",
    "\n",
    "!{sys.executable} -m pip install umap-learn facenet-pytorch\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git\n",
    "\n",
    "# Install google-images-download\n",
    "!{sys.executable} -m pip install git+https://github.com/Joeclinton1/google-images-download.git\n",
    "\n",
    "# download the ultralytics bing scraper\n",
    "# from class_utils.download import download_file_maybe_extract\n",
    "# download_file_maybe_extract(\n",
    "#     \"https://raw.githubusercontent.com/ultralytics/google-images-download/master/bing_scraper.py\",\n",
    "#     directory=\".\"\n",
    "# )\n",
    "\n",
    "# install dependencies for ultralytics bing_scraper\n",
    "# !{sys.executable} -m pip install tqdm selenium\n",
    "# !apt update\n",
    "# !apt install chromium-chromedriver\n",
    "# !cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "# sys.path.insert(0, '/usr/lib/chromium-browser/chromedriver')\n",
    "# from selenium import webdriver\n",
    "# chrome_options = webdriver.ChromeOptions()\n",
    "# chrome_options.add_argument('--headless')\n",
    "# chrome_options.add_argument('--no-sandbox')\n",
    "# chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "# wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_L6HlamqYLT"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "from google_images_download.google_images_download import googleimagesdownload\n",
    "from class_utils import make_montage, plot_bboxes\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from lapjv import lapjv\n",
    "from umap import UMAP\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPIGQVpF9sKH"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(\"https://www.dropbox.com/s/i5kxprnwuqg4s95/george_martin_example.jpg?dl=1\", directory=\"data\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2v5kU7V53NS"
   },
   "outputs": [],
   "source": [
    "#@title -- Auxiliary Functions -- { display-mode: \"form\" }\n",
    "\n",
    "def download_images(keyword, limit, num_retries=5,\n",
    "                    output_directory='downloads',\n",
    "                    image_directory='.'):\n",
    "    for i in range(num_retries):\n",
    "        response = googleimagesdownload()\n",
    "        download = response.download(\n",
    "            {\"keywords\": k, \"limit\": 25,\n",
    "             \"output_directory\": output_directory,\n",
    "             \"image_directory\": image_directory})\n",
    "        absolute_image_paths = list(download[0].values())[0]\n",
    "\n",
    "        if len(absolute_image_paths) > 0:\n",
    "            break\n",
    "\n",
    "# def download_images(keyword, limit, num_retries=5,\n",
    "#                     output_directory='downloads',\n",
    "#                     image_directory='.', chromedriver=None):\n",
    "#     if chromedriver is None:\n",
    "#         chromedriver = '/usr/lib/chromium-browser/chromedriver'\n",
    "\n",
    "#     command = ('python3 bing_scraper.py --search \"{keyword:}\" --limit 25 ' + \n",
    "#               '--download --chromedriver {chromedriver:} ' +\n",
    "#               '--output_directory=\"{output_directory:}\" ' +\n",
    "#               '--image_directory=\"{image_directory:}\" ' \n",
    "#     ).format(keyword=keyword, chromedriver=chromedriver,\n",
    "#              output_directory=output_directory,\n",
    "#              image_directory=image_directory)\n",
    "\n",
    "#     !$command\n",
    "\n",
    "def get_image_filenames(\n",
    "    directory,\n",
    "    image_exts = ['.jpg', '.jpeg', '.png', '.gif'],\n",
    "    recursive = True\n",
    "):\n",
    "    images = []\n",
    "    \n",
    "    for fname in glob.glob(os.path.join(directory, \"**/*\"), recursive=recursive):\n",
    "        if os.path.isfile(fname) and os.path.splitext(fname)[-1] in image_exts:\n",
    "            images.append(fname)\n",
    "    \n",
    "    return images\n",
    "\n",
    "# def enforce_maxres(img, maxres):\n",
    "#     width, height = img.width, img.height\n",
    "    \n",
    "#     if width > height:\n",
    "#         if width > maxres:\n",
    "#             width, height = maxres, maxres / width * height\n",
    "#             img = img.resize((int(width), int(height)), resample=3)\n",
    "#     else:\n",
    "#         if height > maxres:\n",
    "#             width, height = maxres / height * width, maxres\n",
    "#             img = img.resize((int(width), int(height)), resample=3)\n",
    "            \n",
    "#     return img\n",
    "\n",
    "def plot_clusters(face_extracted, clusts, labelIDs,\n",
    "                  verbose=1, figsize=(10, 8),\n",
    "                  show_title=True):\n",
    "    figures = []\n",
    "\n",
    "    # loop over the unique face integers\n",
    "    for labelID in labelIDs:\n",
    "        # find all indexes into the `data` array that belong to the\n",
    "        # current label ID, then randomly sample a maximum of 25 indexes\n",
    "        # from the set\n",
    "        if verbose:\n",
    "            print(\"Faces for face ID: {}\".format(labelID))\n",
    "        \n",
    "        idxs = np.where(clusts == labelID)[0]\n",
    "        idxs = np.random.choice(idxs, size=min(25, len(idxs)),\n",
    "            replace=False)\n",
    "    \n",
    "        # initialize the list of faces to include in the montage\n",
    "        faces = np.asarray([face_extracted[i] for i in idxs])\n",
    "    \n",
    "        # create a montage of the faces\n",
    "        if len(faces):\n",
    "            montage = make_montage(faces, 5)\n",
    "        else:\n",
    "            montage = np.zeros((64, 64, 3))\n",
    "        \n",
    "        # show the output montage\n",
    "        title = \"Face ID #{}\".format(labelID)\n",
    "        title = \"Unknown Faces\" if labelID == -1 else title\n",
    "        \n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        plt.imshow(montage)\n",
    "        plt.axis('off')\n",
    "\n",
    "        if show_title:\n",
    "            plt.title(title)\n",
    "\n",
    "        figures.append(fig)\n",
    "\n",
    "    return figures\n",
    "\n",
    "def plot_faces(face_extracted, poses, w=0.08, h=0.08, ax=None):\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    for i in range(len(face_extracted)):\n",
    "        face = face_extracted[i]\n",
    "        pos = poses[i]\n",
    "        ax.imshow(face, extent=[pos[0] - w/2, pos[0] + w/2,\n",
    "                                pos[1] - h/2, pos[1] + h/2])\n",
    "\n",
    "    plt.xlim([np.min(poses[:, 0]) - w, np.max(poses[:, 0]) + w])\n",
    "    plt.ylim([np.min(poses[:, 1]) - h, np.max(poses[:, 1]) + h])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxuuVRO35x-c",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Human Face Clustering\n",
    "\n",
    "The notebook shows a simple way to peform face clustering in Python.\n",
    "\n",
    "---\n",
    "### Task 1: Downloading the Images\n",
    "\n",
    "Naturally, if we want to do face clustering, we will need some face images. We will therefore use the `googleimagesdownload` package to download a few images from the Google Images service – e.g. the photos of some famous celebrities.\n",
    "\n",
    "**Add the names of 5 or 6 celebrities into the `keywords` list below. These will be used as keywords when searching for the photos.** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hq5TNiiC5j8V",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "keywords = [\n",
    "\n",
    "    \n",
    "    \n",
    "    # XXXXXXXX\n",
    "    \n",
    "    \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3BiGOm55x_U",
    "tags": [
     "en"
    ]
   },
   "source": [
    "For every one of these keywords, we will now download a couple of images and store them in the `downloads` folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w19TZ6jTtR_a",
    "outputId": "29116c75-bb86-442f-b14a-6ef32efbc6bf"
   },
   "outputs": [],
   "source": [
    "# make sure that the dataset directory is clean before we start\n",
    "shutil.rmtree('dataset', ignore_errors=True)\n",
    "\n",
    "# download 25 images for each keyword\n",
    "for k in keywords:\n",
    "    download_images(keyword=k, limit=25, output_directory='dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXPdoYk11pwW",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Should the image download fail for any reason, uncomment the cell below to download a precollected dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shenJS121rC5"
   },
   "outputs": [],
   "source": [
    "# !wget -nc -O faceclust_dataset.zip https://www.dropbox.com/s/3mkdxof3r4rmmf2/faceclust_dataset.zip?dl=1\n",
    "# !unzip -oq -d dataset faceclust_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPu688KN5x_s",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Extraction and Transformation of Faces\n",
    "\n",
    "Given that the images we are using are downloaded directly from Google Images, they will contain not only human faces, but also entire figures and other objects. We will need to extract the faces somehow. \n",
    "\n",
    "Let us first select a device for PyTorch: either a GPU if it is available, or else a CPU. Then let's construct a pretrained network that is going to do the face extraction for us.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPzUFfFMmXXe"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mtcnn = MTCNN(\n",
    "    min_face_size=64,\n",
    "    device=device,\n",
    "    keep_all=True,\n",
    "    post_process=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_VDkeQ_AgOn",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Now let's try out the network on a sample image. We will first open the image and run `mtcnn.detect` on it. What we get is a list containing a bounding box for each face and the list of their corresponding confidence scores (0 for no confidence, 1 for total confidence).\n",
    "\n",
    "Finally we are also going to plot the bounding boxes onto the image to make sure that everything works correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "GwqnHm-4BDHw",
    "outputId": "7187f46f-005a-48f6-de94-91535c8c51b4"
   },
   "outputs": [],
   "source": [
    "img = Image.open(\"data/george_martin_example.jpg\")\n",
    "bboxes, probs = mtcnn.detect(img)\n",
    "\n",
    "for i, (bbox, prob) in enumerate(zip(bboxes, probs)):\n",
    "    print(f\"Bounding box {i}: {bbox}; score: {prob}\")\n",
    "\n",
    "fig = plot_bboxes(img, bboxes)\n",
    "fig.savefig(\"output/face_detection_cnn.jpg\",\n",
    "            bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbDvzyyKBdER",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Our `mtcnn` object can also be used to extract the actual facial images from our overall image. Let's run `mtcnn.extract` and display the extracted faces in a grid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "GQ1VA7A7oJhU",
    "outputId": "706d638d-8b2b-4c68-e39b-da3557b2cf35"
   },
   "outputs": [],
   "source": [
    "extracts = mtcnn.extract(img, list(bboxes), None) / 255.0\n",
    "face_montage = make_montage(extracts.permute(0, 2, 3, 1), 3)\n",
    "plt.imshow(face_montage); plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahrEtqN5BOg3",
    "tags": [
     "en"
    ]
   },
   "source": [
    "In a subsequent step, we will use a different deep neural network to transform our facial images into a new representation that will better express the similarities and differences between human faces than a raw-pixel representation could. The network has been pretrained as a classifier on a dataset with a large number of human faces (VGGFace2).\n",
    "\n",
    "After pretraining the network, we strip away its top layer. As a result, the network transforms each input image into a 512-dimensional embedding vector. We are going to use these vectors to represent our facial images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJHRkajypDoj"
   },
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained='vggface2', device=device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4tvhCCNEZSN",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Next we retrieve a list of all image files present in the \"dataset\" folder. We are going to walk over them, extract the faces present in each of the images and store them in the `face_extracted` tensor. We store a numpy copy of the tensor into `face_extracted_img`: we will be using this to plot the faces. We then apply `fixed_image_standardization` to the `face_extracted` tensor to transform it into the format that the neural network expects.\n",
    "\n",
    "Note that this is cell going to take a while to execute because some of the images will likely be quite high-resolution and we need to run each of them through the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KyN4872Vr3Lw",
    "outputId": "67f8f113-451b-4d4e-a440-a7acc591685f"
   },
   "outputs": [],
   "source": [
    "img_paths = get_image_filenames('dataset')\n",
    "face_extracted = []\n",
    "\n",
    "for i, img_path in enumerate(img_paths):\n",
    "    print(f\"Extracting from image {i}/{len(img_paths)}: '{img_path}'.\")\n",
    "    img = Image.open(img_path).convert(mode=\"RGB\")\n",
    "    with torch.no_grad():\n",
    "        extracts = mtcnn(img)\n",
    "\n",
    "    if not extracts is None:\n",
    "        face_extracted.append(extracts)\n",
    "\n",
    "face_extracted = torch.vstack(face_extracted)\n",
    "face_extracted_img = face_extracted.permute(0, 2, 3, 1).cpu().numpy() / 255.0\n",
    "face_extracted = fixed_image_standardization(face_extracted).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nj4bOP9yFY_w",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Having extracted all the faces, we will now be working with small images, which are now also going to have a standardized size so we are going to be able to batch them. This will make the following cell, where run each image through the embedding network, run much faster than then previous one (especially when using a GPU).\n",
    "\n",
    "Note that we are using `torch.no_grad` here: we will not be doing backpropagation so we do not need to build a computational graph in the forward pass – this saves us some more time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qkpGO1isiLL"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "face_embeddings = []\n",
    "\n",
    "for i in range(0, len(face_extracted), batch_size):\n",
    "    with torch.no_grad():\n",
    "        embedding = resnet(face_extracted[i:min(i+batch_size, len(face_extracted))])\n",
    "    face_embeddings.append(embedding)\n",
    "\n",
    "face_embeddings = torch.vstack(face_embeddings).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOBAsamF5x_9",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Our dataset may contain faces of people whose names were not among our keywords (the original photos might have contained other persons). Some faces may be extracted incorrectly and there is a chance that some non-faces will be extracted by mistake as well. It will be interesting to see how the network extracting the representations will be able to cope with this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5l9YZRqK5yAH",
    "tags": [
     "en"
    ]
   },
   "source": [
    "---\n",
    "### Task 2: Clustering\n",
    "\n",
    "**Apply clustering to the `encodings` array, e.g. using DBSCAN. Assign the resulting cluster IDs to variable clusts. Note that you may need to tweak the `eps` hyperparameter to get good results.** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5RZKF_d55yAS",
    "tags": [
     "student",
     "en"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# apply the clustering\n",
    "\n",
    "\n",
    "clusts =      # assign the cluster IDs to this variable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRFkZNt_5yAd",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Displaying the Results\n",
    "\n",
    "Finally, let's visualize the faces belonging to the individual clusters. The first image will correspond to faces that do not belong into any cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Txw4DZwm5yAg",
    "outputId": "2021681f-37af-4a80-ed0d-448e8141a6fe",
    "tags": [
     "en"
    ]
   },
   "outputs": [],
   "source": [
    "labelIDs = np.unique(clusts)\n",
    "numUniqueFaces = len(np.where(labelIDs > -1)[0])\n",
    "print(\"Number of unique faces: {}\".format(numUniqueFaces))\n",
    "print(\"The photos were found using {} different keywords.\".format(len(keywords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bsGbbRFaIhe1",
    "outputId": "0a442c92-12c9-45c2-cd22-c786d5412922"
   },
   "outputs": [],
   "source": [
    "figs = plot_clusters(face_extracted_img, clusts, labelIDs, verbose=0)\n",
    "\n",
    "for ifig, fig in enumerate(figs):\n",
    "    fig.savefig(\"output/clust_{}.pdf\".format(ifig),\n",
    "                bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocBDFfLf5yAw",
    "tags": [
     "en"
    ]
   },
   "source": [
    "---\n",
    "### Task 3: Reducing Dimensionality using UMAP\n",
    "\n",
    "**Use UMAP to reduce the dimensionality of the data in the `encodings` array from 128 to 2 – so that it is possible to plot the data. It may be necessary to tweak arguments `min_dist` and `spread` to get a nice, readable figure (i.e. avoid excessive face overlap and such). Assign the results to an array named `embeds`.** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLUGkh1x5yA5",
    "tags": [
     "en",
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# apply dimensionality reduction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embeds =          # assign the reduced data to this variable\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogC1Fu1W5yBI",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We normalize the reduced data into the range of [0, 1].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sB3MBcVwxvDI"
   },
   "outputs": [],
   "source": [
    "embeds -= embeds.min(axis=0)\n",
    "embeds /= embeds.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gh3zk0Nf5yBW",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We plot the faces on the embedding positions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "T3janEE3d67t",
    "outputId": "dc3a714e-3b24-40e7-8ac8-81b34481558a"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plot_faces(face_extracted_img, embeds)\n",
    "plt.xlabel('$d_1$')\n",
    "plt.ylabel('$d_2$')\n",
    "\n",
    "plt.savefig(\"output/faces_umap.pdf\",\n",
    "            bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vK5lkcYA5yBh",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Displaying the Faces on a Grid using the Jonker-Volgenant Algorithm\n",
    "\n",
    "The visualization produced by UMAP displays the distances between face clusters and such. However, the images overlap to a considerable extent, which makes the figure less readable. We can therefore take an additional step and project all the images into a regular grid using the Jonker-Volgenant algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzhsyFPBYErl"
   },
   "outputs": [],
   "source": [
    "sqrt_size = int(np.ceil(np.sqrt(len(embeds))))\n",
    "size = sqrt_size * sqrt_size\n",
    "grid = np.dstack(np.meshgrid(np.linspace(0, 1, sqrt_size), np.linspace(0, 1, sqrt_size))).reshape(-1, 2)\n",
    "\n",
    "padded_embeds = np.zeros((size, embeds.shape[1]))\n",
    "padded_embeds[:embeds.shape[0], :] = embeds\n",
    "\n",
    "cost_matrix = cdist(grid, padded_embeds, \"sqeuclidean\").astype(np.float32)\n",
    "cost_matrix = cost_matrix * (100000 / cost_matrix.max())\n",
    "row_as, col_as, _ = lapjv(cost_matrix)\n",
    "grid_jv = grid[col_as]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ib4kjs285yBm",
    "tags": [
     "en"
    ]
   },
   "source": [
    "The new positions have been stored in array `grid_jv`: we will now use them for the plotting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "id": "dMspz5e_Y2jx",
    "outputId": "4a9cb12d-8314-4238-b40d-386e19befcbd"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plot_faces(face_extracted_img, grid_jv)\n",
    "plt.axis('off')\n",
    "plt.savefig(\"output/faces_grid.pdf\",\n",
    "            bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LaMmGhqTulsV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "3_face_clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
