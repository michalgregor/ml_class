{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!apt install libgraphviz-dev pkg-config # to fix broken installation of pygraphviz\n",
    "!{sys.executable} -m pip install pygraphviz==1.7\n",
    "!{sys.executable} -m pip install git+https://gitlab.com/michalgregor/ani_torch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "from ani_torch import TorchGraph, trackable_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# hide a PYDEV warning triggered by the use of sys.gettrace in Google Colab\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='PYDEV DEBUGGER WARNING:.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Automatic Differentiation using PyTorch\n",
    "\n",
    "Given that most current machine learning techniques are based on optimization and many of the popular optimization methods are gradient-based (including the ones used in deep learning), we need to be able to compute gradients of mathematical expressions as easily and efficiently as possible.\n",
    "\n",
    "Automatic differentiation (autodiff; in the theory of artificial neural networks also known as the backprogation algorithm), is a method which computes gradients by constructing a graph of the expression and then running it forward (to compute the output) and backward (to propagate the gradients from the output back to the input). Autodiff can therefore compute the gradient at only roughly two times the cost of the forward run. This is incomparably more efficient than the other two methods that we have discussed so far: numerical differentiation and symbolic differentiation.\n",
    "\n",
    "### The Computation Graph and the Gradient\n",
    "\n",
    "In PyTorch, the computation graph is created automatically, by running standard imperative code, but using special objects. Instead of standard arrays, one will use PyTorch tensors. Also, instead of `numpy` operations such as `np.cos` or `np.exp` one will use their PyTorch equivalents `torch.cos` and `torch.exp`. Otherwise the code will look virtually identical.\n",
    "\n",
    "Let us start by defining a simple PyTorch function that will return $\\cos(ax + c)$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, a, c):\n",
    "    y = torch.sin(a*x + c)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "In order to run the function, all we need to do is to create PyTorch tensors. We can create them by converting standard Python data types or even numpy arrays. To compute gradients w.r.t. the inputs we will need to do two things though: ensure that the tensors have a floating-point data type and that their `requires_grad` flag is set to `True`. The latter is to prevent unnecessary computation: we rarely need to know gradients w.r.t. all variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2, dtype=float, requires_grad=True)\n",
    "a = torch.tensor(3, dtype=float, requires_grad=True)\n",
    "c = torch.tensor(4, dtype=float, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We will now run the function on our tensors and collect the output. We can also print it immediately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = func(x, a, c)\n",
    "print(y.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "To run the backward pass to compute the gradients, all we now need to do is to call `y.backward()`. The gradients then get backpropagated to the tensors and we can access them through the `.grad` attribute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(a.grad)\n",
    "print(c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Visualizing the Computation Graph\n",
    "\n",
    "We will now use an auxiliary library to display the computation graph. The library is not part of PyTorch: we will only be using it here to better illustrate how automatic differentiation works. All we need to do is to create a `TorchGraph` object using our function and some input values (these can be numbers or numpy arrays â€“ they will automatically be wrapped as PyTorch tensors).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = TorchGraph(func, [2, 3, 4])\n",
    "graph.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Visualizing the Forward and the Backward Run\n",
    "\n",
    "Even more importantly, we are able to visualize autodiff's forward and backward run using an animated figure. This will enable us to give visual explanations of how backpropagation of gradients works.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.animate(direction=\"forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.animate(direction=\"backward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Gradient Propagation for Some Common Cases\n",
    "\n",
    "Perhaps the easiest way to understand how autodiff works, is to go through a few of the more common cases such as addition, multiplication and such and explain how the gradients get backpropagated.\n",
    "\n",
    "#### Addition: Distributing the Gradient\n",
    "\n",
    "Addition is probably the simplest case: it merely distributes the gradient from the output into the two input branches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_add(a, b):\n",
    "    y = a + b\n",
    "    return y\n",
    "\n",
    "graph = TorchGraph(func_add, [2, 3])\n",
    "graph.plot(with_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = TorchGraph(func_add, [2, 3], [2])\n",
    "graph.plot(with_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Multiplication: Swapping and Multiplying\n",
    "\n",
    "With multiplication, we merely swap the inputs from the forward run (and obviously multiply them by the output gradient as per the chain rule).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_mult(a, b):\n",
    "    y = a * b\n",
    "    return y\n",
    "\n",
    "graph = TorchGraph(func_mult, [2, 3])\n",
    "graph.plot(with_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = TorchGraph(func_mult, [2, 3], [2])\n",
    "graph.plot(with_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Branches: Accumulation of Gradients\n",
    "\n",
    "Whenever branches occur in the graph and the same variable is used multiple times, gradients from all the branches accumulate in the backward run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_branch(x):\n",
    "    y1 = torch.sqrt(x)\n",
    "    y2 = torch.sqrt(x)\n",
    "    return y1, y2\n",
    "\n",
    "graph = TorchGraph(func_branch, [4], [4, 8])\n",
    "graph.plot(with_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### The `max` Operator: A Gradient Switch\n",
    "\n",
    "The `max` operator is frequently used as a pooling operation in deep convolutional networks. How do the gradients propagate through it? Clearly the output of the operator only depends on the greatest input. The full gradient propagates to that input. The gradients w.r.t. the other inputs are zero: the change in the other inputs has no effect.\n",
    "\n",
    "One could, of course, object, because changing the values of the inputs will have an effect provided that they become the greatest input instead. However, we need to recall that when computing gradients, we are interested in the effect of infinitesimally small changes and an infinitesimally small change in the input is not going to make one input larger than the other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_branch(a, b):\n",
    "    y = torch.max(a, b)\n",
    "    return y\n",
    "\n",
    "graph = TorchGraph(func_branch, [2, 5], [2])\n",
    "graph.plot(with_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Defining New Operations\n",
    "\n",
    "To get an even fuller understanding how autodiff works, we are going to implement a new operation: the sigmoid function. Its mathematical definition is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation}\n",
    "and its derivative is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\n",
    "\\end{equation}\n",
    "We will define our new function as a subclass of `torch.autograd.Function`. We will define two static methods (if you don't know what that means, don't worry: just add the `@staticmethod` decorator):\n",
    "\n",
    "* **forward:**  this takes care of the forward pass;\n",
    "* **backward:**  this backpropagates the gradients from the outputs to the inputs of our function.\n",
    "Clearly, the output of the forward pass could be reused when computing the backward pass so that we do not have to needlessly recompute the expensive nonlinear function multiple times. To cache the output, we store it in the context object `ctx` using `ctx.save_for_backward`.\n",
    "\n",
    "Finally, we decorate the class itself with `@trackable_function`. This decorator is not part of PyTorch: we are adding it so that our new function can be visualized. We also name it using `\"name = $\\sigma$\"` so that its name in the visualization is $\\sigma$ and not `sigmoid`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trackable_function\n",
    "class Sigmoid(torch.autograd.Function):\n",
    "    name = \"$\\sigma$\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        y = 1 / (1 + torch.exp(-x))\n",
    "        ctx.save_for_backward(y)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        y, = ctx.saved_tensors\n",
    "        grad_input = y * (1 - y) * grad_output\n",
    "        return grad_input\n",
    "\n",
    "sigmoid = Sigmoid.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_sigmoid(x):\n",
    "    y = sigmoid(x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = TorchGraph(func_sigmoid, [2])\n",
    "graph.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "---\n",
    "### Task 1: Run Autodiff on a Function\n",
    "\n",
    "**Run autodiff on the following function:** \n",
    "$$\n",
    "y = a \\sin(bx) + c\n",
    "$$\n",
    "\n",
    "**at** \n",
    "$$\n",
    "a=5, b=4, c=7, x=2.\n",
    "$$\n",
    "**What is the gradient w.r.t. $x$?** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def func(x, a, b, c):\n",
    "    \n",
    "    \n",
    "    # ---\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "# ---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a351393365bb1b108989afa08de3243f72f5e58927baf5d192f3cca79a41cbc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
