{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "InrBCZkJB4Ud",
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "colab_type": "code",
    "id": "jKVlYcZUB4VF",
    "outputId": "611fbecd-bad6-45bb-b1b9-32a1b18e9d9a"
   },
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!apt install libgraphviz-dev pkg-config # to fix broken installation of pygraphviz\n",
    "!{sys.executable} -m pip install pygraphviz==1.7\n",
    "!{sys.executable} -m pip install git+https://gitlab.com/michalgregor/ani_torch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "1C-ZJcKvB4Vn",
    "outputId": "998393f4-9792-4362-bc8f-96c4aab4a1c4"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "from sympy.utilities.lambdify import lambdify\n",
    "from scipy.optimize import approx_fprime\n",
    "from ani_torch import TorchGraph\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import torch\n",
    "import time\n",
    "\n",
    "sp.init_printing(use_latex='matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2hrO8PCB4V5"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POV2mlR0B4WN"
   },
   "outputs": [],
   "source": [
    "#@title -- Auxiliary Functions -- { display-mode: \"form\" }\n",
    "def sp_sum(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zEqtyrOkB4Wc",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Different Ways to Automatically Compute Gradient: A Comparison\n",
    "\n",
    "We have so far encountered no less than three different ways to automatically compute gradients of mathematical expressions:\n",
    "\n",
    "* symbolic differentiation;\n",
    "* numerical differentiation;\n",
    "* automatic differentiation (autodiff).\n",
    "### Symbolic Differentiation\n",
    "\n",
    "In the case of symbolic differentiation, we would define the expression symbolically (e.g. using the `sympy` package) and then derive its gradient in the symbolic form. This would then be converted into a standard python function and used to compute the gradients. Unfortunately, symbolic differentiation does not scale too well to complex expressions, because when the gradient of these is derived the result tends to have a huge amount of subexpressions that repeat again and again and that need to be recomputed all the time. Also, scaling to a large number of dimensions can be a problem, because that further multiplies the number of operations required.\n",
    "\n",
    "### Numeric Differentiation\n",
    "\n",
    "With numeric differentiation, we do not need a symbolic expression: all we need is a function that we can evaluate. As such, numeric differentiation will not have the same scaling problem that symbolic differentiation faces (however, the character of the expression can have a significant impact on the precision of the computed gradients). The gradient is approximated numerically by perturbing the point of interest and measuring the differences. The technique can even be applied to functions which are not in fact differentiable. However, numeric differentiation does not scale at all well with the number of dimensions. While it can be relatively effective for low-dimensional problems, its cost grows very fast with an increasing number of dimensions. This is because the point needs to be perturbed along every dimension to determine how fast the function changes along it. This means that the function will need to be evaluated a huge number of times.\n",
    "\n",
    "### Automatic Differentiation\n",
    "\n",
    "Finally, in the case of automatic differentiation, the interface does not look very different to that of symbolic differentiation, but the gradient is not in fact computed symbolically. Instead, we construct a graph of the expression, run it forward to get the output and then run it backward, propagating the gradients from the output back to the input. The cost of automatic differentiation is therefore only approximately 2 times the cost of the forward run. The graph allows the algorithm to cache and reuse intermediate results in a way that symbolic differentiation cannot. (This is related to a technique known as dynamic programming: autodiff is basically the combination of the chain rule and dynamic programming.)\n",
    "\n",
    "### The Expression That We Are Going to Differentiate\n",
    "\n",
    "The expression that we will be evaluating will be formed by stacking instances of a simpler subexpression on top of each other. The subexpression is displayed here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fR3gPIQBB4Wp"
   },
   "source": [
    "<img alt=\"alternative_label\" width=\"550\" src=\"data:image/jpg;base64,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\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pUy0TUTPB4Wu",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Autodiff using PyTorch\n",
    "\n",
    "We will start by defining the subexpression as a PyTorch function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zoVkwzDB4W3"
   },
   "outputs": [],
   "source": [
    "def torch_layer(x):\n",
    "    y = torch.tanh(x)\n",
    "    y1 = y ** 2\n",
    "    y = y1 + y\n",
    "    \n",
    "    y1 = torch.cos(y)\n",
    "    y2 = torch.sin(y)\n",
    "    y = y1 + y2\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQK0QKI0B4XD",
    "tags": [
     "en"
    ]
   },
   "source": [
    "As the next step we will stack several instances of `torch_layer` to form our full expression. We will also apply function `sum` to the result: if the output is a vector or a matrix, we will compute the gradient of the sum of their elements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bpeuxVzuB4XM"
   },
   "outputs": [],
   "source": [
    "def torch_func(x):\n",
    "    y = torch_layer(x)\n",
    "    y = torch_layer(y)\n",
    "    y = torch_layer(y)\n",
    "    y = torch_layer(y)\n",
    "    y = torch.sum(y)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vST57YOB4XY",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Finally, we will define the `torch_grad` function that will automatically compute the gradient by running `torch_func` forward and backward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kineJ0nlB4Xh"
   },
   "outputs": [],
   "source": [
    "def torch_grad(x):\n",
    "    x = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n",
    "    y = torch_func(x)\n",
    "    y.backward()\n",
    "    return x.grad.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBq5hYCPB4Xw",
    "tags": [
     "en"
    ]
   },
   "source": [
    "To test whether our function works, we can now invoke for some particular input $x$. The output will be a `numpy` array containing our scalar output. We will use its `.item` method to extract the scalar number to make the output more readable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "LhZ4RJZ9B4X5",
    "outputId": "c04ad0c3-ac72-47e2-81e9-82fdacefe1c3"
   },
   "outputs": [],
   "source": [
    "torch_grad(0.01).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "rpQyil_zB4YG",
    "outputId": "ba774eae-e12d-44a0-9c26-046067421573"
   },
   "outputs": [],
   "source": [
    "torch_grad(np.full((3, 3), 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtZxap3EB4YS",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Displaying the Graph\n",
    "\n",
    "Having defined our function, we can now display the resulting graph to check whether we implemented everything correctly. Let us start with the graph of the subexpression and then also display the full expression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "id": "BskJ0wh-B4Ya",
    "outputId": "0111372e-1ca7-4cc5-d7a9-b57f30ab374b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "graph = TorchGraph(torch_layer, [0.1])\n",
    "graph.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "id": "kfahz8HFB4Yl",
    "outputId": "a5978c8d-6609-4c93-8cdd-c6459288f72d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28, 6))\n",
    "graph = TorchGraph(torch_func, [0.1])\n",
    "graph.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "642TYG9QB4ZA",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Symbolic Differentiation using SymPy\n",
    "\n",
    "Having defined the PyTorch version of our expression, let us now move on to the symbolic version, which we will define using the `sympy` package. We will again start by defining the subexpression. The definition will be virtually identical to the PyTorch version: the only difference being that it uses the `sympy` namespace `sp` instead of the PyTorch namespace `torch`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9Nwk0kSB4ZH"
   },
   "outputs": [],
   "source": [
    "def sym_layer(x):\n",
    "    y = sp.tanh(x)\n",
    "    y1 = y ** 2\n",
    "    y = y1 + y\n",
    "\n",
    "    y1 = sp.cos(y)\n",
    "    y2 = sp.sin(y)\n",
    "    y = y1 + y2\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d_SQxpNQB4ZR",
    "tags": [
     "en"
    ]
   },
   "source": [
    "And we will again stack multiple instances of the subexpression to form our full expression. We will also again apply function `sum` to the result: if the output is a vector or a matrix, we will compute the gradient of the sum of their elements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qJvflZzSB4ZX"
   },
   "outputs": [],
   "source": [
    "def sym_func(x):\n",
    "    y = sym_layer(x)\n",
    "    y = sym_layer(y)\n",
    "    y = sym_layer(y)\n",
    "    y = sym_layer(y)\n",
    "    y = sp_sum(y)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Snc9DVfqB4Zh",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Finally, we will use symbolic differentiation to compute the gradient in the usual way and then transform into a standard python function called `sym_grad`. Even though our expression is not too large, to derive its symbolic gradient will already take a considerable amount of time. We will therefore time the execution of the next cell using the `%%time` magic command.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "nQfHwvvqB4Zr",
    "outputId": "66420959-a108-4933-e283-99bd9a9ba6da"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x = sp.symbols('x')\n",
    "y = sym_func(x)\n",
    "sym_sym_grad = sp.Matrix([y]).jacobian([x])\n",
    "sym_grad = lambdify((x), sym_sym_grad, \"numpy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F6PHOxSTB4Z2",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Like before, we can check whether our function works by invoking it. We can also compare the result with that of `torch_grad`. They should be very similar, but may not be exactly equal due to rounding errors (we can expect the results of symbolic differentiation to be a bit less precise because a lot more operations have to be carried out to compute them and so the errors will accumulate). For simpler expressions the results would match almost exactly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xfJ4Xed6B4Z9",
    "outputId": "9b57fc4f-e928-45fa-c300-7907020b9401"
   },
   "outputs": [],
   "source": [
    "sym_grad(0.01).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "v9rE4BE3B4aL",
    "outputId": "7770ed6f-d4a1-4c1d-c8d6-ac2fc8c2d13e"
   },
   "outputs": [],
   "source": [
    "sym_grad(np.full((3, 3), 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8LnTHOgJB4aU",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Displaying the Expressions\n",
    "\n",
    "We can also check the expressions. With the sub-expression this is going to be relatively easy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 42
    },
    "colab_type": "code",
    "id": "KYINO-hsB4aa",
    "outputId": "bc080957-9591-460c-ba10-66533e101f00"
   },
   "outputs": [],
   "source": [
    "sym_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vdc1fzwxB4aw",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Unfortunately, the full expression will be absurdly large.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 985
    },
    "colab_type": "code",
    "id": "ny9NL05_D_-_",
    "outputId": "8d8dfb9e-b7d8-4cb9-8003-c5f004a6d3d7"
   },
   "outputs": [],
   "source": [
    "str(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kz-pnuyMB4a-",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Unsurprisingly, the symbolic gradient is going to be much worse still.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CbHPDLiOEEOL",
    "outputId": "641ddfa4-b58d-4fc8-e409-63380fcfdac2"
   },
   "outputs": [],
   "source": [
    "str(sym_sym_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gLCLfv1BB4bT",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Numerical Differentiation Using SciPy\n",
    "\n",
    "For numerical differentiation, we will define the functions as standard Python functions using `numpy`. We will then use the `approx_fprime` function from the SciPy package to numerically compute the gradient. To this end we need to specify parameter epsilon: we use the same value that is used internally in SciPy by the optimization methods that use numeric gradients. Since `approx_fprime` operates on vectors (1-dimensional arrays), we will also need to do some reshaping before and after we invoke it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rfdyHsVUB4bY"
   },
   "outputs": [],
   "source": [
    "def np_layer(x):\n",
    "    y = np.tanh(x)\n",
    "    y1 = y ** 2\n",
    "    y = y1 + y\n",
    "\n",
    "    y1 = np.cos(y)\n",
    "    y2 = np.sin(y)\n",
    "    y = y1 + y2\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5jmcaBuJB4bh"
   },
   "outputs": [],
   "source": [
    "def np_func(x):\n",
    "    y = np_layer(x)\n",
    "    y = np_layer(y)\n",
    "    y = np_layer(y)\n",
    "    y = np_layer(y)\n",
    "    y = np.sum(y)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "akGhYfVwB4bn"
   },
   "outputs": [],
   "source": [
    "eps = np.sqrt(np.finfo(float).eps)\n",
    "\n",
    "def np_grad(x):\n",
    "    x_vec = np.hstack([np.asarray(x).reshape(-1), [1]])\n",
    "    g = approx_fprime(x_vec, np_func, epsilon=eps)\n",
    "    return g[:-1].reshape(np.shape(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xggiS2j4B4bu",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Next we can again verify whether our function works. Precision of the gradient will depend on the character of the expression – in our case it should be quite good, but there are expressions (we could replace `cos` with `exp` for instance), where numerical approximation will practically fail altogether, which the other two methods will be able to achieve relatively good results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "BXv9Q4sRB4bz",
    "outputId": "3c582ca8-c02f-4802-f77a-5092fd10fb89"
   },
   "outputs": [],
   "source": [
    "np_grad(0.01).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "pGNu2OgdB4b6",
    "outputId": "28fd4f7c-7668-4eeb-909c-902858afa86a"
   },
   "outputs": [],
   "source": [
    "np_grad(np.full((3, 3), 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sh4ayvrCB4cC",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Comparing the Run Times\n",
    "\n",
    "Finally, having set up all three methods of computing the gradients, we can now compare their run times. We will run a number of iterations for each method and measure the overall time using the `%%time` magic command. Furthermore, we will run a scalar and a matrix version of the computation for each method.\n",
    "\n",
    "We should see that autodiff is much faster than the other two methods. Also, the effect would become much more pronounced if we increased the complexity of the expression and the dimensionality of the inputs.\n",
    "\n",
    "#### Automatic differentiation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "3g4B5JBSB4cH",
    "outputId": "b915c8c6-e554-4bfb-ce89-57dd160f3f49"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grads = 0\n",
    "\n",
    "for i in range(500):\n",
    "    grads += torch_grad(np.random.uniform(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "glUMno-vB4cP",
    "outputId": "281dde93-0010-47a7-fd95-4419ae1ede16"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grads = 0\n",
    "\n",
    "for i in range(500):\n",
    "    grads += torch_grad(np.random.uniform(0, 1, (15, 15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yw2yMeb4B4cW",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Symbolic Differentiation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Lf3I37B5B4ca",
    "outputId": "dc9cda70-5f37-4328-b092-a0222b761b4b"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grads = 0\n",
    "\n",
    "for i in range(500):\n",
    "    grads += sym_grad(np.random.uniform(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "fe3LjLIIB4cj",
    "outputId": "cf7deb59-8b22-4897-fb71-12ba708f7862"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grads = 0\n",
    "\n",
    "for i in range(500):\n",
    "    grads += sym_grad(np.random.uniform(0, 1, (15, 15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9TKlH66lB4cu",
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Numerical Differentiation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "uKZHvQnMB4c1",
    "outputId": "56f18f35-ba11-4607-8a02-175e663f6562"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grads = 0\n",
    "\n",
    "for i in range(500):\n",
    "    grads += np_grad(np.random.uniform(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "HSpggupZB4dA",
    "outputId": "240fe7ef-c873-4823-b2ec-1676ef376fa2"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "grads = 0\n",
    "\n",
    "for i in range(500):\n",
    "    grads += np_grad(np.random.uniform(0, 1, (15, 15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4BUneDmzB4dM",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### A Graphical Comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "T4WwRoZfB4dV",
    "outputId": "5cf39a16-b28f-4ac5-e404-c4dd2ce4a060"
   },
   "outputs": [],
   "source": [
    "#@title -- Run the Experiments and Record the Results -- { display-mode: \"form\" }\n",
    "results = []\n",
    "\n",
    "def autodiff_test(config):\n",
    "    print(\"running autodiff, config {}...\".format(config))\n",
    "    start = time.time()\n",
    "    grads = 0\n",
    "    for i in range(500):\n",
    "        grads += torch_grad(np.random.uniform(0, 1, config))\n",
    "    end = time.time()\n",
    "    return end-start\n",
    "\n",
    "def symbolic_test(config):\n",
    "    print(\"running symbolic, config {}...\".format(config))\n",
    "    start = time.time()\n",
    "    grads = 0\n",
    "    for i in range(500):\n",
    "        grads += sym_grad(np.random.uniform(0, 1, config))\n",
    "    end = time.time()\n",
    "    return end-start\n",
    "\n",
    "def numerical_test(config):\n",
    "    print(\"running numerical, config {}...\".format(config))\n",
    "    start = time.time()\n",
    "    grads = 0\n",
    "    for i in range(500):\n",
    "        grads += np_grad(np.random.uniform(0, 1, config))\n",
    "    end = time.time()\n",
    "    return end-start\n",
    "\n",
    "cfg = (1,)\n",
    "results.append(('autodiff', cfg, autodiff_test(cfg)))\n",
    "results.append(('symbolic', cfg, symbolic_test(cfg)))\n",
    "results.append(('numerical', cfg, numerical_test(cfg)))\n",
    "\n",
    "cfg = (5, 5)\n",
    "results.append(('autodiff', cfg, autodiff_test(cfg)))\n",
    "results.append(('symbolic', cfg, symbolic_test(cfg)))\n",
    "results.append(('numerical', cfg, numerical_test(cfg)))\n",
    "\n",
    "cfg = (10, 10)\n",
    "results.append(('autodiff', cfg, autodiff_test(cfg)))\n",
    "results.append(('symbolic', cfg, symbolic_test(cfg)))\n",
    "results.append(('numerical', cfg, numerical_test(cfg)))\n",
    "\n",
    "cfg = (15, 15)\n",
    "results.append(('autodiff', cfg, autodiff_test(cfg)))\n",
    "results.append(('symbolic', cfg, symbolic_test(cfg)))\n",
    "results.append(('numerical', cfg, numerical_test(cfg)))\n",
    "\n",
    "# make a pandas dataframe\n",
    "df = pd.DataFrame(results, columns=[\"method\", 'config_tuple', 'time [s]'])\n",
    "df['input shape'] = df['config_tuple'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "colab_type": "code",
    "id": "IA1B0mfpB4dh",
    "outputId": "4da38e4f-f3a7-4b35-cde4-f807a24d3ff3"
   },
   "outputs": [],
   "source": [
    "sns.catplot(x=\"method\", y=\"time [s]\", hue=\"input shape\",\n",
    "            data=df, kind=\"bar\", aspect=1.25)\n",
    "plt.gca().set_axisbelow(True)\n",
    "plt.grid(ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sccHrWvhB4d3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [
    "gtZxap3EB4YS",
    "642TYG9QB4ZA",
    "8LnTHOgJB4aU",
    "gLCLfv1BB4bT",
    "sh4ayvrCB4cC",
    "Yw2yMeb4B4cW",
    "9TKlH66lB4cu",
    "4BUneDmzB4dM"
   ],
   "name": "2_autodiff_sym_num.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
