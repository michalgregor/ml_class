{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sO56guLDW4L",
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GdFecj4jDW4s"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, KBinsDiscretizer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "H60bOk4gDW5Q",
    "outputId": "eb28107c-964a-4cb1-e43f-eaf13d3b8602"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(\"https://www.dropbox.com/s/p5q7gzupa2ndw55/sigmoid_regression_data.csv?dl=1\", directory=\"data\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bWadrMa4DW5p",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Simgoid Regression using PyTorch's Autodiff\n",
    "\n",
    "Having shown how autodiff work in PyTorch, we can now attempt to apply it to sigmoid regression, which we have already solved using symbolic gradients. We will start by loading the corresponding dataset from a CSV file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "vcZAHNKxDW54",
    "outputId": "43fd1d35-2f53-48c3-f11e-ab3f756f8d80"
   },
   "outputs": [],
   "source": [
    "#@title -- Loading and Preprocessing the Data: X_train, Y_train, X_test, Y_test -- { display-mode: \"form\" }\n",
    "df = pd.read_csv(\"data/sigmoid_regression_data.csv\")\n",
    "\n",
    "kbins = KBinsDiscretizer(6, encode='ordinal')\n",
    "y_stratify = kbins.fit_transform(df[['y']])\n",
    "\n",
    "df_train, df_test = train_test_split(\n",
    "    df, stratify=y_stratify, test_size=0.3, random_state=4)\n",
    "\n",
    "plt.scatter(df_train['x'], df_train['y'], marker='x', label=\"training data\")\n",
    "plt.scatter(df_test['x'], df_test['y'], c='r', label=\"testing data\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(ls='--')\n",
    "plt.legend()\n",
    "\n",
    "categorical_inputs = []\n",
    "numeric_inputs = ['x']\n",
    "output = 'y'\n",
    "\n",
    "input_preproc = make_column_transformer(\n",
    "    (make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OrdinalEncoder()),\n",
    "     categorical_inputs),\n",
    "    \n",
    "    (make_pipeline(\n",
    "        SimpleImputer(),\n",
    "        StandardScaler()),\n",
    "     numeric_inputs)\n",
    ")\n",
    "\n",
    "X_train = input_preproc.fit_transform(df_train[categorical_inputs+numeric_inputs])\n",
    "Y_train = df_train[[output]].values\n",
    "\n",
    "X_test = input_preproc.transform(df_test[categorical_inputs+numeric_inputs])\n",
    "Y_test = df_test[[output]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYNS5EJuDW6M",
    "tags": [
     "en"
    ]
   },
   "source": [
    "As we know, PyTorch operates on tensors rather than plain array and so we need to wrap our data first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7WJouXFeDW6Z"
   },
   "outputs": [],
   "source": [
    "X_train_t = torch.as_tensor(X_train)\n",
    "Y_train_t = torch.as_tensor(Y_train)\n",
    "X_test_t = torch.as_tensor(X_test)\n",
    "Y_test_t = torch.as_tensor(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fxc2VkLIDW6p",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### The Sigmoid Function\n",
    "\n",
    "Let us recall that the sigmoid curve is defined as follows:\n",
    "\\begin{equation}\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}.\n",
    "\\end{equation}\n",
    "\n",
    "Since our sigmoid can be shifted or its steepness can change, we will pipe the input to the sigmoid through a linear transform and we will learn its parameters $a$ and $c$ from data. Our regression model will therefore look as follows:\n",
    "\\begin{align}\n",
    "u &= ax + c \\\n",
    "\\sigma(u) &= \\frac{1}{1 + e^{-u}}.\n",
    "\\end{align}\n",
    "\n",
    "Or if we fold it into a single function:\n",
    "\\begin{equation}\n",
    "f(x, a, c) = \\frac{1}{1 + e^{-ax - c}}\n",
    "\\end{equation}\n",
    "\n",
    "Let us now define our regression model using PyTorch operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ya-H6SzDW60"
   },
   "outputs": [],
   "source": [
    "def sigmoid_model(X, a, c):\n",
    "    return torch.sigmoid(X*a + c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3I8A57UDW7C",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### The Loss and the Variables\n",
    "\n",
    "Let us use the mean squared error as our loss function. We can define it as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8KYneHLADW7M"
   },
   "outputs": [],
   "source": [
    "def compute_loss(Y, y):\n",
    "    return ((y - Y)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qphfJAV_DW7a",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We also need to create variables `a` and `c` (we wrap the tensors as variables because we are going to be updating them) and specify the learning rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rOP_9IsCDW7j"
   },
   "outputs": [],
   "source": [
    "a = Variable(torch.as_tensor(np.random.uniform(0, 1)), requires_grad=True)\n",
    "c = Variable(torch.as_tensor(np.random.uniform(0, 1)), requires_grad=True)\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhWsOVrxDW7z",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Optimization using Gradient Descent\n",
    "\n",
    "We can now write a loop that will optimize our model using gradient descent. Recall that the gradient can be computed simply by doing the forward run and then calling `backward()` on the output â€“ the loss in our case.\n",
    "\n",
    "We also need to make sure that:\n",
    "\n",
    "* We stop tracking the gradients (using `with torch.no_grad():`) when we update parameters `a` and `c`: otherwise PyTorch will try to make this part of the computational graph too, which is going to fail.\n",
    "* Zero the gradient of each variable out after each epoch. This is because gradients accumulate and the new gradients would just be added to those from the previous epoch otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "qayygp22DW7-",
    "outputId": "40184a60-bff6-4a02-b98e-78617c6df5ee"
   },
   "outputs": [],
   "source": [
    "for epoch in range(2500):\n",
    "    y = sigmoid_model(X_train_t, a, c)\n",
    "    loss = compute_loss(Y_train_t, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        \n",
    "    a.grad.zero_()\n",
    "    c.grad.zero_()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {}; loss {}.\".format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oiBHCCD5DW8M",
    "tags": [
     "en"
    ]
   },
   "source": [
    "This gives us some values for `a` and `c`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "f8yhW5B_DW8S",
    "outputId": "908be50c-c0a4-4f07-b438-4193d0ca6c61"
   },
   "outputs": [],
   "source": [
    "print(\"a = {}\\nc = {}\\nloss = {}\".format(\n",
    "    a.item(), c.item(), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t3VtOWt0DW8f",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Let's see what our regression curve is going to look like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "2jnAieonDW8o",
    "outputId": "78cc57c5-1e99-4b20-b876-f3539fa11f8b"
   },
   "outputs": [],
   "source": [
    "xx = torch.linspace(-5, 5, 100)\n",
    "yy = torch.sigmoid(xx*a + c)\n",
    "\n",
    "plt.plot(xx.detach().numpy(), yy.detach().numpy())\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(ls='--')\n",
    "\n",
    "plt.scatter(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7Pz4zyzDW82",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Using a Built-in Optimizer\n",
    "\n",
    "Luckily, when using PyTorch, we do not have to write our own optimization procedures by hand. PyTorch has several of the best-known optimizers built in. If we wanted to use the `Adam` optimizer for instance, we would simply instantiate it with the tensors that it is supposed to update and run its `step()` method at each epoch. \n",
    "\n",
    "Naturally, gradients still need to be zeroed out at each epoch, which is now done using optimizer's `zero_grad()` method. Also, we do not have to define the mean squared error by hand either: PyTorch also has all the most common loss functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQswgCwFDW8_"
   },
   "outputs": [],
   "source": [
    "a = Variable(torch.as_tensor(np.random.uniform(0, 1)), requires_grad=True)\n",
    "c = Variable(torch.as_tensor(np.random.uniform(0, 1)), requires_grad=True)\n",
    "optimizer = torch.optim.Adam([a, c], lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "3IFRzMmCDW9L",
    "outputId": "a4428d42-fe5d-45d1-9191-466f0709bef2"
   },
   "outputs": [],
   "source": [
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y = sigmoid_model(X_train_t, a, c)\n",
    "    loss = torch.nn.functional.mse_loss(Y_train_t, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {}; loss {}.\".format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "5odXYHD2DW9W",
    "outputId": "b6693d9b-8e3e-4689-a739-976b9749104f"
   },
   "outputs": [],
   "source": [
    "print(\"a = {}\\nc = {}\\nloss = {}\".format(\n",
    "    a.item(), c.item(), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23_cTjnXDW9i",
    "tags": [
     "en"
    ]
   },
   "source": [
    "And we can again inspect our regression curve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "LpeCx1AdDW9s",
    "outputId": "4f222068-75f3-4c10-9f6a-cb7349abdbb1"
   },
   "outputs": [],
   "source": [
    "xx = torch.linspace(-5, 5, 100)\n",
    "yy = torch.sigmoid(xx*a + c)\n",
    "\n",
    "plt.plot(xx.detach().numpy(), yy.detach().numpy())\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(ls='--')\n",
    "\n",
    "plt.scatter(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2TiYAmndDW93"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "name": "3_autodiff_sigmoid.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
