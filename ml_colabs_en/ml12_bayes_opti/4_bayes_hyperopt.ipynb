{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install hyperopt\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Ch8q9Pzy1-o"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt import space_eval\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9tNXv6tWzHfC",
    "outputId": "97bfd8dd-b4db-4a82-dd41-cda5e57c13d1"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(\"https://www.dropbox.com/s/u8u7vcwy3sosbar/titanic.zip?dl=1\",\n",
    "                            directory=\"data/titanic\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Bayesian Hyperparameter Optimization\n",
    "\n",
    "We already know how to use `scikit-learn` to set up and train a simple model. We also know that models usually have hyperparameters: parameters that are not learned but need to be set in some way beforehand or, ideally, selected by some kind of hyperparameter optimization method.\n",
    "\n",
    "Of course, training a model is often very expensive and has to be repeated for many different sets of hyperparameters while looking for the optimal configuration. This is why Bayesian optimization is often used for hyperparameter tuning – as we have already discussed, it is an approach geared towards being able to optimize the objective function with as few actual queries to its value as possible.\n",
    "\n",
    "In this notebook we are going to show a practical approach to Bayesian hyperparameter optimization using a popular package called `hyperopt`.\n",
    "\n",
    "### Loading and Preprocessing the Data\n",
    "\n",
    "As usual, we will start by loading and preprocessing data. We will make use of the well-known [Titanic](https://www.kaggle.com/c/titanic) dataset. Since, at this point, there is no need to go over the loading and preprocessing in detail, the code of the next cell is hidden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rAxn47cLy1_A"
   },
   "outputs": [],
   "source": [
    "#@title -- Loading and preprocessing: X_train, Y_train, X_test, Y_test -- { display-mode: \"form\" }\n",
    "df = pd.read_csv(\"data/titanic/train.csv\")\n",
    "df_train, df_test = train_test_split(df, test_size=0.25,\n",
    "                     stratify=df[\"Survived\"], random_state=4)\n",
    "\n",
    "# we split the columns into categorical and numeric inputs and the output\n",
    "categorical_inputs = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
    "numeric_inputs = [\"Age\", \"SibSp\", 'Parch', 'Fare']\n",
    "output = [\"Survived\"]\n",
    "\n",
    "# we create our preprocessing pipeline\n",
    "input_preproc = make_column_transformer(\n",
    "    (make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OrdinalEncoder(categories='auto')),\n",
    "     categorical_inputs),\n",
    "    \n",
    "    (make_pipeline(\n",
    "        SimpleImputer(),\n",
    "        StandardScaler()),\n",
    "     numeric_inputs)\n",
    ")\n",
    "\n",
    "# we fit the pipeline on the train set and then apply it to both train and test\n",
    "X_train = input_preproc.fit_transform(df_train[categorical_inputs + numeric_inputs])\n",
    "Y_train = df_train[output]\n",
    "\n",
    "X_test = input_preproc.transform(df_test[categorical_inputs + numeric_inputs])\n",
    "Y_test = df_test[output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Bayesian Optimization\n",
    "\n",
    "The first thing that we will need to do before we apply Bayesian optimization will, of course, be to define the objective function that the method is to minimize.\n",
    "\n",
    "Given that our goal it to find hyperparameters with which our model will achieve the best results, the input arguments will be the hyperparameters. We will use these to set up a model (a decision tree based on class  `DecisionTreeClassifier`).\n",
    "\n",
    "The performance of the model will then be evaluated using $k$-fold cross-validation. (The training data will be split into $k$ folds, one of which will be used for testing and the other ones for training each time. Once we have tested the model on all combinations of training and testing datasets in this way, the final score will be determined by averaging the results from all the individual runs.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k1cDccIfy1_4"
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    model = DecisionTreeClassifier(**params)\n",
    "    \n",
    "    score = cross_validate(model, X_train, Y_train,\n",
    "                           scoring='f1_macro',\n",
    "                           cv=10, n_jobs=10)['test_score'].mean()\n",
    "    print(\"Score {:.3f} params {}\".format(score, params))\n",
    "\n",
    "    # minus because we want the score to be as high as\n",
    "    # possible, but the objective function is to be minimized\n",
    "    return -score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "As our next step, we will need to set up the search space: i.e. to specify our method's hyperparamters and to determine what values they can take. Let us start by displaying the docstring of class `DecisionTreeClassifier`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-Cdw2Dsy2AC",
    "outputId": "c5e46b8d-d6f3-4bcb-a183-7789207a2739"
   },
   "outputs": [],
   "source": [
    "?DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "---\n",
    "### Task 1: Setting up the Search Space\n",
    "\n",
    "**Use the next cell to define the search space `space` of decision tree hyperparameters.** \n",
    "\n",
    "---\n",
    "To set up the space, create a dictionary of the following form:\n",
    "\n",
    "```\n",
    "space = {\n",
    "    # categorical variable:\n",
    "    'cat_var': hp.choice(\"cat_var\", [\"opt1\", \"opt2\", \"opt3\"]),\n",
    "\n",
    "    # a uniformly distributed integer:\n",
    "    'int_var': scope.int(hp.quniform(\"int_var\", 1, 15, 1)),\n",
    "\n",
    "    # a uniformly distributed real nubmer:\n",
    "    'float_var': hp.uniform('float_var', 0.2, 1.0),\n",
    "}\n",
    "```\n",
    "Further options and more detailed documentation of how to define such parameter spaces can be found at [hyperopt's wiki](https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "space = {\n",
    "    \n",
    "    \n",
    "    # ---\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Running the Optimization\n",
    "\n",
    "Next, we can run the optimization itself. We'll specify the objective function, the search space, the maximum number of evaluations of the objective function and the algorithm. We will be using `tpe`, i.e. the Tree-structured Parzen Estimator. This approach is better at dealing with high-dimensional spaces than Gaussian processes are, but the basic aim remains the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B2nSn_mLy2AU"
   },
   "outputs": [],
   "source": [
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Function `fmin` will return the best solution found. We then decode it using function `space_eval`, which will yield a representation that we can use when creating our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txxTstqAy2Ab"
   },
   "outputs": [],
   "source": [
    "best_params = space_eval(space, best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Retraining the Model with the Best Hyperparameters\n",
    "\n",
    "Now that we have identified the best set of hyperparameters, we will use them to retrain the model: this time using the entire training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cg4jffZky2A6"
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(**best_params)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Testing\n",
    "\n",
    "And finally, we are ready to test the model on the test set. We will display the confusion matrix and our standard metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDVM_08jy2BC"
   },
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xMWRhiA-y2BI",
    "outputId": "241040d5-250d-48dc-c8d1-b735f1868fa3"
   },
   "outputs": [],
   "source": [
    "cm = pd.crosstab(Y_test.values.reshape(-1), y_test,\n",
    "                 rownames=['actual'],\n",
    "                 colnames=['predicted'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BiqPdl5Ty2BO",
    "outputId": "37970425-a391-4a47-dddc-27099310f7b2"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy = {}\".format(accuracy_score(Y_test, y_test)))\n",
    "print(\"Precision = {}\".format(precision_score(Y_test, y_test)))\n",
    "print(\"Recall = {}\".format(recall_score(Y_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The performance should be better than with default hyperparameters (at least on average – it is difficult to say anything reasonable about one particular run because of all the stochasticity). We can and verify whether this is the case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_model = DecisionTreeClassifier()\n",
    "def_model.fit(X_train, Y_train)\n",
    "y_test = def_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy = {}\".format(accuracy_score(Y_test, y_test)))\n",
    "print(\"Precision = {}\".format(precision_score(Y_test, y_test)))\n",
    "print(\"Recall = {}\".format(recall_score(Y_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "---\n",
    "### Task 2: Optimizing XGBoost's Hyperparameters\n",
    "\n",
    "**Try to apply the same procedure to a different classification method now: to the XGBClassifier from package `xgboost`. It will be necessary to redefine especially method `objective`, so that it uses the new model, and the search space `space`, so that it corresponds to the hyperparameters of the new method.** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbUUNGwiy2Bb"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-58Jap7Vy2Bf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "3_bayes_dtree.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
