{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sp\n",
    "from sympy.utilities.lambdify import lambdify\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Auxiliary Functions -- { display-mode: \"form\" }\n",
    "def plot_func(xx, yy, zz, X=None):\n",
    "    plt.contour(xx, yy, zz, cmap='Spectral')\n",
    "    # both axes at the same scale + create a legend\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.xlabel('x'); plt.ylabel('y')\n",
    "    plt.colorbar(label='z')\n",
    "    \n",
    "    if not X is None:\n",
    "        plt.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Optimization Using `scipy`\n",
    "\n",
    "In our next example we are going to show how optimization can be applied using the `scipy` package. This package implements several advanced method, including second-order methods. These are typically more effective than gradient descent and its various versions, which we have considered up till now. Their disadvantage, however, is the lack of scalability: they typically cannot be applied to problems with a large number of parameters (and there is a similar scaling problem with dataset size in the context of machine learning).\n",
    "\n",
    "### Defining the Objective Function\n",
    "\n",
    "As the first step we will again define the objective function and derive its gradient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symx, symy = sp.symbols('x y')\n",
    "symf = (5*symx)**2 + symy ** 2\n",
    "f = lambdify((symx, symy), symf, \"numpy\")\n",
    "\n",
    "sym_grad_f = sp.Matrix([symf]).jacobian([symx, symy])\n",
    "grad_f = lambdify((symx, symy), sym_grad_f, \"numpy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "As usual, we will also display the visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.mgrid[-10:10.2:0.2, -10:10.2:0.2]\n",
    "zz = f(xx, yy)\n",
    "plot_func(xx, yy, zz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Minimization Using `scipy`\n",
    "\n",
    "Next we are going to apply the `minimize` method. We will specify the following arguments:\n",
    "\n",
    "* The objective function `fun` that is to be minimized. The function is expected to accept a vector as its input, which is why we will wrap our function in a lambda function, which will unwrap the input vector into the individual arguments $x$ and $y$ using operator *.\n",
    "* The initial point `x0` from which the optimization starts.\n",
    "* The method: we can pick one of a range of different solvers.\n",
    "* Gradient: here denoted `jac`, because it is also possible to specify a full Jacobian (for vector functions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(fun=lambda X: f(*X),\n",
    "               x0=[-9, -8],\n",
    "               method='L-BFGS-B',\n",
    "               jac=lambda X: grad_f(*X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The function will return an object that contains the resulting point as well as the value of the objective function at that point:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The point: {}\".format(res.x))\n",
    "print(\"The value: {}\".format(res.fun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "More detailed documentation of the function can be displayed using:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(minimize.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Visualizing the Minimization\n",
    "\n",
    "If we intend (like in the previous examples) to visualize the minimization itself and not just the result, we can also use the `callback` argument, which will add each new point into list `X`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[-9, -8]]\n",
    "\n",
    "res = minimize(fun=lambda X: f(*X),\n",
    "               x0=X[0],\n",
    "               method='L-BFGS-B',\n",
    "               jac=lambda X: grad_f(*X),\n",
    "               callback=X.append)\n",
    "\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The resulting visualization will then look as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.mgrid[-10:10.2:0.2, -10:10.2:0.2]\n",
    "zz = f(xx, yy)\n",
    "plot_func(xx, yy, zz, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Not Specifying the Gradient\n",
    "\n",
    "It is possible to invoke function `minimize` without specifying the gradient (`jac`). For one thing, some solvers do not use the gradient. But even for the solvers that do, the gradient can be estimated numerically (by perturbing the input variable). Gradient can only be effectively numerically estimated if the input is low-dimensional â€“ otherwise it becomes too computationally expensive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[-9, -8]]\n",
    "\n",
    "res = minimize(fun=lambda X: f(*X),\n",
    "               x0=X[0],\n",
    "               method='L-BFGS-B',\n",
    "               callback=X.append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The result: {}\".format(res.x))\n",
    "print(\"The function's value: {}\".format(res.fun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "You can compare the result of the minimization with that computed before. It is possible that it will be a bit less precise because the function does not have the real gradient a its disposal now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
