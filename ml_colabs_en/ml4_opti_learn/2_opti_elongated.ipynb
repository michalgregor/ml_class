{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3tXwag27Ppo4"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy.utilities.lambdify import lambdify\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-O-tGqq9PppM"
   },
   "outputs": [],
   "source": [
    "#@title -- Auxiliary Functions -- { display-mode: \"form\" }\n",
    "def plot_func(xx, yy, f, X=None):\n",
    "    if not X is None:\n",
    "        Xmin, Xmax = X[:, 0].min(), X[:, 0].max() \n",
    "        Ymin, Ymax = X[:, 1].min(), X[:, 1].max()\n",
    "        \n",
    "        if (Xmin < xx.min() or Xmax > xx.max() or\n",
    "                Ymin < yy.min() or Ymax > yy.max()):            \n",
    "            xx = np.linspace(Xmin, Xmax, 100)\n",
    "            yy = np.linspace(Ymin, Ymax, 100)\n",
    "            xx, yy = np.meshgrid(xx, yy)\n",
    "            \n",
    "        plt.scatter(X[:, 0], X[:, 1], zorder=10)\n",
    "        \n",
    "    zz = f(xx, yy)\n",
    "    plt.contour(xx, yy, zz, cmap='Spectral')\n",
    "    # both axes at the same scale + create a legend\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.xlabel('x'); plt.ylabel('y')\n",
    "    plt.colorbar(label='z')\n",
    "        \n",
    "def grad_desc(grad_f, init_point,\n",
    "              num_steps, learning_rate):\n",
    "    X = np.zeros((num_steps + 1, 2))\n",
    "    X[0] = init_point\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        X[i+1] = X[i] - learning_rate * grad_f(*X[i])\n",
    "    \n",
    "    return X        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Gradient Descent: An Elongated Paraboloid\n",
    "\n",
    "In the previous notebook we have shown how to apply gradient descent to minimization of a simple paraboloid. Let's now try to apply the same procedure to a slightly more complex problem: to an elongated paraboloid. Let the function that we are going to minimize have the following form:\n",
    "\n",
    "\\begin{equation}\n",
    "z = f(x, y) = (5x)^2 + y^2\n",
    "\\end{equation}\n",
    "The main problem that we are going to be faced with is the inability to find a learning rate that would work well for both dimensions.\n",
    "\n",
    "### Visualizing the Function\n",
    "\n",
    "As the first step we will again define and visualize the function. We will define the function symbolically so that we can later use the automatic way to compute its symbolic gradient:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rcu4b_yGPppm",
    "outputId": "edfdd02a-2c5e-4159-9bb4-454a36b2477a"
   },
   "outputs": [],
   "source": [
    "symx, symy = sp.symbols('x y')\n",
    "symf = (5*symx)**2 + symy ** 2\n",
    "symf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FrA5fpsoPpp2"
   },
   "outputs": [],
   "source": [
    "f = lambdify((symx, symy), symf, \"numpy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We visualize the function using the same procedure that we applied before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "s8Nhp9SbPpqM",
    "outputId": "8ed8646c-0fd5-4e0a-bb02-80914d42d955"
   },
   "outputs": [],
   "source": [
    "xx, yy = np.mgrid[-10:10.2:0.2, -10:10.2:0.2]\n",
    "zz = f(xx, yy)\n",
    "plot_func(xx, yy, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "---\n",
    "### Task 1: Automatically Deriving the Gradient\n",
    "\n",
    "**Use the `sympy` package again to automatically derive the gradient. Then convert its symbolic form into a standard numeric function `grad_f`:** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HJV0zf74Ppqh",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "sym_grad_f =     # ----\n",
    "\n",
    "\n",
    "grad_f =         # ----\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Minimizing the Function Using Gradient Descent\n",
    "\n",
    "If we now try to minimize the objective function using gradient descent, we will encounter a problem: it will be difficult to come up with a learning rate that would work for both dimensions. We will either make the learning rate large and this will cause oscillation in the direction where the function decreases more quickly or we will make it small and minimization will proceed extremely slowly along the other direction.\n",
    "\n",
    "We can test this practically. With $\\gamma = 0.1$ oscillation will occur:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "TzkxLefzPprm",
    "outputId": "ff652db1-07db-41eb-e6f6-fe8d7a091146"
   },
   "outputs": [],
   "source": [
    "X = grad_desc(grad_f, init_point=[-9, -8],\n",
    "              num_steps=6, learning_rate=0.1)\n",
    "plot_func(xx, yy, f, X)\n",
    "plt.gca().set_aspect(800)\n",
    "plt.gcf().set_size_inches([12, 2])\n",
    "plt.savefig(\"output/grad_elongated_fast.pdf\",\n",
    "            bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "With $\\gamma = 0.01$ the minimization will be very slow in the direction of $y$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "D4bigl6hPpr1",
    "outputId": "c099c0c1-e4c3-4659-c94a-7d6ca1705d26"
   },
   "outputs": [],
   "source": [
    "X = grad_desc(grad_f, init_point=[-9, -8],\n",
    "              num_steps=20, learning_rate=0.01)\n",
    "plot_func(xx, yy, f, X)\n",
    "plt.savefig(\"output/grad_elongated_slow.pdf\",\n",
    "            bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abmJdTZAPpsH",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Gradient Descent with Momentum\n",
    "\n",
    "One way to counteract the problem that we have come across when applying gradient descent, is to augment the update rule with an additional term â€“ with momentum. In that case we would take the magnitude of the change from the last iteration into account when computing a new point:\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta \\mathbf{x}_{i+1} &= \\alpha \\Delta \\mathbf{x}_i - \\gamma \\nabla f(\\mathbf{x}_i) \\\\\n",
    "\\mathbf{x}_{i+1} &= \\mathbf{x}_i + \\Delta \\mathbf{x}_{i+1},\n",
    "\\end{align}\n",
    "where $\\alpha$ is the momentum coefficient (determines the relative importance of the change at the previous iteration $\\Delta \\mathbf{x}_i$).\n",
    "\n",
    "The advantage of this is that if the minimization keeps making steps in the same direction, the momentum will accumulate and the steps in that direction will grow larger over time. And vice versa: in the direction where the algorithm oscillates, the sign of the change will keep changing, which will have the tendency to dampen the oscillation.\n",
    "\n",
    "---\n",
    "#### Task 2: Add the Momentum Term\n",
    "\n",
    "**Fill in the blanks in the following cell with the code for gradient descent with momentum (according to the formulas stated above):** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZnS8TbmJPpsK",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def grad_desc_momentum(grad_f, init_point,\n",
    "              num_steps, learning_rate, alpha):\n",
    "    X = np.zeros((num_steps + 1, 2))\n",
    "    X[0] = init_point\n",
    "    deltaX = 0\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        \n",
    "        \n",
    "        # ----\n",
    "        \n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Let's see now, how well our new algorithm is going to be doing on the elongated paraboloid:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "fxUhx3EPPpta",
    "outputId": "45dcd8a4-c2c2-45b1-cffc-b99ba2bab4ea"
   },
   "outputs": [],
   "source": [
    "X = grad_desc_momentum(grad_f, init_point=[-9, -8],\n",
    "              num_steps=20, learning_rate=0.01, alpha=0.8)\n",
    "plot_func(xx, yy, f, X)\n",
    "plt.savefig(\"output/grad_elongated_momentum.pdf\",\n",
    "            bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "As we can see, the augmented algorithm can approach the minimum without issues, because the momentum term accelerates steps along the direction where the sign of the gradient does not change for a long time and vice versa: dampens the oscillation in the direction where the minimum is being stepped over.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "2_opti_problem.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
