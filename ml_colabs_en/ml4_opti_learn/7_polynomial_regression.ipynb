{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(\"https://www.dropbox.com/s/p5q7gzupa2ndw55/sigmoid_regression_data.csv?dl=1\", directory=\"data\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Auxiliary Functions -- { display-mode: \"form\" }\n",
    "def fit_poly(X, Y, d):\n",
    "    x_min = np.min(X)\n",
    "    x_max = np.max(X)\n",
    "    \n",
    "    polynomial_features = PolynomialFeatures(degree=d)\n",
    "    X_poly = polynomial_features.fit_transform(X)\n",
    "    linreg = LinearRegression()\n",
    "    linreg = linreg.fit(X_poly, Y)\n",
    "    \n",
    "    xx = np.linspace(x_min, x_max, 250).reshape((-1, 1))\n",
    "    xx_poly = polynomial_features.transform(xx)\n",
    "    yy = linreg.predict(xx_poly)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(X, Y)\n",
    "    plt.grid(ls='--')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "\n",
    "    plt.plot(xx, yy, 'r')\n",
    "    plt.title(\"degree {}\".format(d))    \n",
    "    plt.savefig(\"output/poly_{}_fit.pdf\".format(d), bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "### Linear Regression Will Not Work Everywhere\n",
    "\n",
    "Linear regression will not work well for every imaginable dataset. For an instance:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Linear Regression on Polynomial Data -- { display-mode: \"form\" }\n",
    "X = np.arange(-5, 10, 0.2)\n",
    "Y = (X - 2 * (X ** 2) + 0.5 * (X ** 3)\n",
    "        + np.random.normal(0, 15, len(X)))\n",
    "\n",
    "X = X.reshape([-1, 1])\n",
    "Y = Y.reshape([-1, 1])\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg = linreg.fit(X, Y)\n",
    "y_lin = linreg.predict(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, Y, s=10)\n",
    "plt.plot(X, y_lin, 'r')\n",
    "plt.grid(ls='--')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.savefig(\"output/poly_linfit.pdf\", bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "It is clear that a line is not able to express the character of the data very well.\n",
    "\n",
    "### Applying Polynomial Regression\n",
    "\n",
    "Polynomial regression is one among many other types of regression. It fits the data using a polynomial of a certain degree. The regresion model looks as follows:\n",
    "\\begin{equation}\n",
    "\\hat y = a_0 + a_1 x + a_2 x^2 + ... + a_n x^n\n",
    "\\end{equation}\n",
    "\n",
    "The good news is that polynomial regression can be done in exactly the same way as linear regression â€“ all we need to do is to transform the regression problem so that the input of the linear regression model will not be $x$ directly, but rather a vector of its powers.\n",
    "\n",
    "The input matrxi of the linear regression model will then take the following form:\n",
    "\\begin{equation}\n",
    "X = \\left(\n",
    "\\begin{matrix}\n",
    "1 & x_1 & x_1^2 & ... & x_1^n \\\n",
    "1 & x_2 & x_2^2 & ... & x_2^n \\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\n",
    "1 & x_m & x_m^2 & ... & x_m^n\n",
    "\\end{matrix}\n",
    "\\right),\n",
    "\\end{equation}\n",
    "where $n$ is the degree of the polynomial.\n",
    "\n",
    "Package `sklearn` contains an object called `PolynomialFeatures`, which helps us to preprocess the data into just such format.\n",
    "\n",
    "For a degree 3 polynomial:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(degree=3), LinearRegression())\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Regression Curve vs. Data -- { display-mode: \"form\" }\n",
    "x_min = np.min(X)\n",
    "x_max = np.max(X)\n",
    "xx = np.linspace(x_min, x_max, 250).reshape((-1, 1))\n",
    "yy = model.predict(xx)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, Y, s=10)\n",
    "plt.plot(xx, yy, 'r')\n",
    "plt.grid(ls='--')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.savefig(\"output/polyfit.pdf\", bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "As we can see, the results are much better: this is because the original data actually comes precisely from a degree 3 polynomial.\n",
    "\n",
    "### How to Choose the Degree of the Polynomial?\n",
    "\n",
    "When selecting the degree of the polynomial, we need to make the same consideration that we always keep track of when applying machine learning. On one hand, the model should be sufficiently complex to express regularities inherent in the data, but not complex enough to simply memorize the data: otherwise it is unlikely to generalize correctly. If the model is not sufficiently complex, we speak about **underfitting** . If it is too complex, **overfitting**  occurs.\n",
    "\n",
    "The problem can be illustrated using the following simple example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Data Loading and Preprocessing; X, Y -- { display-mode: \"form\" }\n",
    "df = pd.read_csv(\"data/sigmoid_regression_data.csv\")\n",
    "\n",
    "# we specify the inputs and the outputs\n",
    "categorical_inputs = []\n",
    "numeric_inputs = ['x']\n",
    "output = ['y']\n",
    "\n",
    "# we create the pipeline\n",
    "input_preproc = make_column_transformer(\n",
    "    (make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OrdinalEncoder()),\n",
    "     categorical_inputs),\n",
    "    \n",
    "    (make_pipeline(\n",
    "        SimpleImputer(),\n",
    "        StandardScaler()),\n",
    "     numeric_inputs)\n",
    ")\n",
    "\n",
    "# we fit and apply the pipeline\n",
    "X = input_preproc.fit_transform(df[categorical_inputs+numeric_inputs])\n",
    "Y = df[output].values\n",
    "\n",
    "# we plot the data for visual inspection\n",
    "plt.scatter(X, Y, marker='x', label=\"training data\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(ls='--')\n",
    "plt.legend()\n",
    "plt.savefig(\"output/regression_data.pdf\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Let us try to fit the data using polynomials of different degrees (function `fit_poly` was defined at the beginning in the auxiliary function section):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_poly(X, Y, 2)\n",
    "fit_poly(X, Y, 5)\n",
    "fit_poly(X, Y, 7)\n",
    "fit_poly(X, Y, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "As we can see, low-degree polynomials cannot really express the shape of the curve from which the original data was sampled. If we instead opt in for a polynomial of too high a degree, it will pass through the points very precisely, but it will behave unreasonably in between them.\n",
    "\n",
    "Naturally, the main mistake in this case is to approximate a curve of this kind using polynomial regression in the first place. The original relationship is suspiciously similar to a logistic (sigmoid) curve. It would therefore very likely be a much better idea to fit a sigmoid curve to it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Fitting the Data using the Sigmoid Curve -- { display-mode: \"form\" }\n",
    "def sigmoid(x, x0, k, a, c):\n",
    "    y = a / (1 + np.exp(-k*(x-x0))) + c\n",
    "    return y\n",
    "\n",
    "x_min = np.min(X)\n",
    "x_max = np.max(X)\n",
    "xx = np.linspace(x_min, x_max, 250).reshape((-1, 1))\n",
    "\n",
    "popt, pcov = curve_fit(sigmoid, X.reshape(-1), Y.reshape(-1))\n",
    "yy = sigmoid(xx, *popt)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, Y)\n",
    "plt.grid(ls='--')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(xx, yy, 'r')\n",
    "plt.savefig(\"output/sigmoid_fit.pdf\", bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "As we can see, the results are incomparably better in this case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
