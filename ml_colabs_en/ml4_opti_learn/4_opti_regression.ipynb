{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "from sympy.utilities.lambdify import lambdify\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, KBinsDiscretizer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from class_utils import error_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(\"https://www.dropbox.com/s/p5q7gzupa2ndw55/sigmoid_regression_data.csv?dl=1\", directory=\"data\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Optimization-based Machine Learning\n",
    "\n",
    "As our next example we will introduce a very simple application of optimization in the context of machine learning. Our goal will be to carry out regression: we will get input and output data and we will be trying to identify the function, which produces such a relationship.\n",
    "\n",
    "This kind of task is easily transformed to an optimization problem. We will assume that we have a parametric function $f_\\theta(\\mathbf{x})$, the character of which is determined by a parameter vector $\\theta$. Our goal will be to find parameters that will minimize the error of model $f_\\theta(\\mathbf{x})$ on out dataset.\n",
    "\n",
    "Let our dataset be composed of samples $(\\mathbf{x_i}, \\mathbf{y_i})$, where $\\mathbf{x_i}$ is the input and $\\mathbf{y}_i$ is the desired output for sample $i$. We can then formalize our goal as the following optimization problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^* = \\underset{\\theta}{\\arg\\min} \\sum_{(\\mathbf{x}_i, \\mathbf{y}_i)} \\mathcal{L}(f_\\theta(\\mathbf{x}_i), \\mathbf{y}_i)\n",
    "\\end{equation}i.e. we want to find a parameter vector $\\theta^*$, which would minimize the difference between the actual and the desired outputs over the entire dataset in terms of some loss function: e.g. the **squared error** .\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "We will start the definition of our regression problem by loading the dataset â€“ our data will be from a noisy sigmoid curve and it will be read from a CSV file. Given that we have loaded and preprocessed the same data before, we will not repeat the exercise here. The necessary code is in the following cell and it is hidden for the sake of brevity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Data Loading and Preprocessing; X_train, Y_train, X_test, Y_test -- { display-mode: \"form\" }\n",
    "df = pd.read_csv(\"data/sigmoid_regression_data.csv\")\n",
    "\n",
    "# we create a discretized version of the y column\n",
    "# to allow for stratification\n",
    "kbins = KBinsDiscretizer(6, encode='ordinal')\n",
    "y_stratify = kbins.fit_transform(df[['y']])\n",
    "\n",
    "# we split the dataset into train and test\n",
    "df_train, df_test = train_test_split(df, stratify=y_stratify,\n",
    "                                 test_size=0.3, random_state=4)\n",
    "\n",
    "# we specify the inputs and the outputs\n",
    "categorical_inputs = []\n",
    "numeric_inputs = ['x']\n",
    "output = 'y'\n",
    "\n",
    "# we create the pipeline\n",
    "input_preproc = make_column_transformer(\n",
    "    (make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OrdinalEncoder()),\n",
    "     categorical_inputs),\n",
    "    \n",
    "    (make_pipeline(\n",
    "        SimpleImputer(),\n",
    "        StandardScaler()),\n",
    "     numeric_inputs)\n",
    ")\n",
    "\n",
    "# we fit and apply the pipeline on the train set\n",
    "X_train = input_preproc.fit_transform(df_train[categorical_inputs+numeric_inputs]).reshape(-1)\n",
    "Y_train = df_train[output].values\n",
    "\n",
    "# we apply the same pipeline to the test set,\n",
    "# taking care to use transform and not fit_transform\n",
    "X_test = input_preproc.transform(df_test[categorical_inputs+numeric_inputs]).reshape(-1)\n",
    "Y_test = df_test[output].values\n",
    "\n",
    "# we plot the data for visual inspection\n",
    "plt.scatter(X_train, Y_train, marker='x', label=\"training data\")\n",
    "plt.scatter(X_test, Y_test, c='r', label=\"testing data\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(ls='--')\n",
    "plt.legend()\n",
    "plt.savefig(\"output/regression_data.pdf\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "---\n",
    "### Task 1: Defining the Regression Function\n",
    "\n",
    "The shape of the relationship that we have just visualized is conspicuously similar to the sigmoid (logistic) curve, which is defined as follows:\n",
    "\\begin{equation}\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}.\n",
    "\\end{equation}\n",
    "However, it seems that the curve is shifted by a bit in the direction of the $x$ axis and its steepness may also not be the same. We will therefore form our regression model by piping the input to the sigmoid function through a linear transform, the parameters $a$ and $c$ of which we will learn from data. Our entire regression model will then look as follows:\n",
    "\\begin{align}\n",
    "u &= ax + c \\\n",
    "\\sigma(u) &= \\frac{1}{1 + e^{-u}}.\n",
    "\\end{align}\n",
    "\n",
    "Or folded into a single function:\n",
    "\\begin{equation}\n",
    "\\mathrm{f}(x, a, c) = \\frac{1}{1 + e^{-ax - c}}\n",
    "\\end{equation}\n",
    "\n",
    "**Use the `sympy` package to symbolically define our regression model as function $f(x, a, c)$.** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "symx, syma, symc = sp.symbols('x a c')\n",
    "\n",
    "\n",
    "\n",
    "symf =      # ----\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f = lambdify([symx, syma, symc], symf, \"numpy\")\n",
    "\n",
    "symf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We will check function `f(x, a, c)` by visualizing it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Regression Function Visualization -- { display-mode: \"form\" }\n",
    "xx = np.linspace(-5, 5, 100)\n",
    "a = 1; c = 0\n",
    "yy = [f(x, a, c) for x in xx]\n",
    "\n",
    "plt.plot(xx, yy)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(ls='--')\n",
    "\n",
    "plt.savefig(\"output/sigmoid.pdf\", bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Our Objective Function and Its Gradient\n",
    "\n",
    "As already stated, our goal is to minimize the error on our dataset, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^* = \\underset{\\theta}{\\arg\\min} \\sum_{(\\mathbf{x}_i, \\mathbf{y}_i)} \\mathcal{L}(f_\\theta(\\mathbf{x}_i), \\mathbf{y}_i)\n",
    "\\end{equation}\n",
    "The outer sum can be ignored when computing the gradient. The following holds because of the linearity of differentiation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla \\sum_{(\\mathbf{x}_i, \\mathbf{y}_i)} \\mathcal{L}(f_\\theta(\\mathbf{x}_i), \\mathbf{y}_i) = \\sum_{(\\mathbf{x}_i, \\mathbf{y}_i)} \\nabla \\mathcal{L}(f_\\theta(\\mathbf{x}_i), \\mathbf{y}_i)\n",
    "\\end{equation}and it therefore suffices to solve the inner part of the sum and then sum up the gradients for all the individual samples from the dataset.\n",
    "\n",
    "If we use the squared error as our loss function, we will obtain:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(f_\\theta(\\mathbf{x}_i), \\mathbf{y}_i) = \\left(\n",
    "    f_\\theta(\\mathbf{x}_i) - \\mathbf{y}_i\n",
    "\\right)^2.\n",
    "\\end{equation}\n",
    "Let us now define this function symbolically and determine its gradient (only w.r.t. parameters $a$ and $c$ because we are only going to be tuning those):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symy = sp.symbols('y')\n",
    "symL = (symf - symy)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = lambdify((symx, symy, syma, symc), symL, \"numpy\")\n",
    "\n",
    "sym_grad_L = sp.Matrix([symL]).jacobian([syma, symc])\n",
    "grad_L_func = lambdify((symx, symy, syma, symc), sym_grad_L, \"numpy\")\n",
    "\n",
    "sym_grad_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Having made such definitions we will substitute all the necessary arguments ($x$, $y$, $a$, $c$) into `grad_L`, but the function will only return a 2-element vector containing the partial derivatives w.r.t. $a$ and $c$. E.g.:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0; y = 1; a = 1; c = 0\n",
    "print(\"L:\\t\\t{}\".format(L(x, y, a, c)))\n",
    "print(\"grad_L:\\t\\t{}\".format(grad_L_func(x, y, a, c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### The Total Loss and Its Gradient\n",
    "\n",
    "As we already know we will obtain the total loss as the sum of the losses over the individual samples and the same goes for the total gradient, which will be the sum of the gradients over the individual samples. Let us therefore define two functions, which will allow us to compute both of these:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumL(a, c, X, Y):\n",
    "    L_sum = 0\n",
    "    \n",
    "    for x, y in zip(X, Y):\n",
    "        L_sum += L(x, y, a, c)\n",
    "        \n",
    "    return L_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_sumL(a, c, X, Y):\n",
    "    grad_sum = np.zeros(2)\n",
    "    \n",
    "    for x, y in zip(X, Y):\n",
    "        grad_sum = grad_sum + grad_L_func(x, y, a, c)\n",
    "        \n",
    "    return grad_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "To make sure that everything works we can test our functions now:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sumL(a, c, X_train, Y_train))\n",
    "print(grad_sumL(a, c, X_train, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Minimizing the Objective Function\n",
    "\n",
    "As our next step we only need to apply function `minimize` to minimize the total loss and to visualize the resulting regression curve. The minimization can be applied as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(fun=lambda xx: sumL(*xx, X_train, Y_train),\n",
    "               x0=np.random.uniform(0, 1, 2),\n",
    "               method='L-BFGS-B',\n",
    "               jac=lambda xx: grad_sumL(*xx, X_train, Y_train)\n",
    "              )\n",
    "\n",
    "a, c = res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Testing Our Regression Model\n",
    "\n",
    "We will get the outputs of our regression model on the training data and compute the performance indicators.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [f(x, a, c) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(Y_test, y_test)\n",
    "mae = mean_absolute_error(Y_test, y_test)\n",
    "\n",
    "print(\"MSE: {}\".format(mse))\n",
    "print(\"MAE: {}\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "To get a fuller idea we can also plot the histogram of outputs and errors just as we did in one of our earlier notebooks:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Histogram of Outputs and Errors -- { display-mode: \"form\" }\n",
    "plt.figure(figsize=(8, 6))\n",
    "error_histogram(Y_test, y_test, Y_fit_scaling=Y_train)\n",
    "plt.savefig(\"output/error_output_histogram.pdf\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Given that our data is 2-dimensional, it will be even more useful to visualize the original data and our resulting regression curve. This will give us a strong intuition as to whether the regression curve behaves as it should:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Regression Curve vs. Data -- { display-mode: \"form\" }\n",
    "x_min = min(np.min(X_train), np.min(X_test))\n",
    "x_max = max(np.max(X_train), np.max(X_test))\n",
    "\n",
    "xx = np.linspace(x_min, x_max, 250).reshape((-1, 1))\n",
    "yy = [f(x, a, c) for x in xx]\n",
    "\n",
    "plt.scatter(X_train, Y_train, marker='x', label=\"training data\")\n",
    "plt.scatter(X_test, Y_test, c='r', label=\"testing data\")\n",
    "\n",
    "plt.plot(xx, yy, label=\"regression curve\", c='k')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(ls='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"output/regression.pdf\", bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### An Easier Implementation of Regression Using `curve_fit`\n",
    "\n",
    "There are, of course, other and simpler tools for performing regression. We can mention `scipy.optimize.curve_fit` as a good example of one such general function. When applying it we only need to enter the regression function in the prescribed form (and optionally also its gradient) and we do not need to consider things such as the iteration of the dataset etc.\n",
    "\n",
    "Using `scipy.optimize.curve_fit` we could simplify our example as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, a, c):\n",
    "    return 1 / (1 + np.exp(-a*x - c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt, _ = curve_fit(sigmoid, X_train.reshape(-1), Y_train.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Evaluation on the Test Set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = sigmoid(X_test, *popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(Y_test, y_test)\n",
    "mae = mean_absolute_error(Y_test, y_test)\n",
    "\n",
    "print(\"MSE: {}\".format(mse))\n",
    "print(\"MAE: {}\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Regression Curve vs. Data -- { display-mode: \"form\" }\n",
    "x_min = min(np.min(X_train), np.min(X_test))\n",
    "x_max = max(np.max(X_train), np.max(X_test))\n",
    "\n",
    "xx = np.linspace(x_min, x_max, 250).reshape((-1, 1))\n",
    "yy = sigmoid(xx, *popt)\n",
    "\n",
    "plt.scatter(X_train, Y_train, marker='x', label=\"training data\")\n",
    "plt.scatter(X_test, Y_test, c='r', label=\"testing data\")\n",
    "\n",
    "plt.plot(xx, yy, label=\"regression curve\")\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(ls='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"output/regression2.pdf\", bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
