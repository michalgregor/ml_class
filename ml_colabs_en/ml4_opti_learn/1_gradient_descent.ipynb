{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hide_input": false,
    "id": "smKKouO4LT2q"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sympy.utilities.lambdify import lambdify\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "This notebook is going to illustrate how gradient descent works. Let us recall that gradient descent is an gradient-based iterative optimization method. The optimization starts from a certain initial point $\\mathbf{x}_0$, which is at each step being shifted a little against the direction of the gradient. Since gradient represents the direction of the steepest ascent of a function at the specified point, by going against the direction of the gradient the function will be minimized.\n",
    "\n",
    "The update rule which is applied at every step to compute the next point $\\mathbf{x}_{i+1}$ is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}_{i+1} = \\mathbf{x}_i - \\gamma \\nabla f(\\mathbf{x}_i)\n",
    "\\end{equation}\n",
    "where $\\nabla f(\\mathbf{x}_i)$ is the gradient of the minimized function and $\\gamma$ (a small number from interval $( 0, 1 \\rangle$) is the learning rate.\n",
    "\n",
    "### Minimization of a Paraboloid: An Example\n",
    "\n",
    "As an example that illustrates gradient descent we are going to use the minimization of a simple function – a paraboloid according to:\n",
    "\n",
    "\\begin{equation}\n",
    "z = f(x, y) = x^2 + y^2\n",
    "\\end{equation}\n",
    "#### Visualization of the Paraboloid\n",
    "\n",
    "As the first step we are going to define and visualize the function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VRAgxhoaLT3Y"
   },
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return x**2 + y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We are going to generate all combinations of points $x, y$ from a certain range and compute $z = f(x, y)$ for them:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQeU3qWSLT3r"
   },
   "outputs": [],
   "source": [
    "xx, yy = np.mgrid[-10:10.2:0.2, -10:10.2:0.2]\n",
    "zz = f(xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We are going to visualize the results in a 3D plot:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "DgJwz6wOLT3-",
    "outputId": "29b6137c-da97-473c-e1c7-5db8392fd23b"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.plot_surface(xx, yy, zz, cmap='Spectral',\n",
    "                linewidth=0, antialiased=True)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "plt.savefig(\"output/gradient_3d_plot.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "3D plots are notorious for being hard to read because some of their elements tend to overlap with each other. To make our plots more readable we will therefore use 2D contour plots instead of 3D plots:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "eTW7lOU9LT4X",
    "outputId": "1fe6a5b9-c4e7-41db-ac0d-5a5bc5bcbf90"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.contour(xx, yy, zz, cmap='Spectral')\n",
    "# both axes at the same scale + create a legend\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.xlabel('x'); plt.ylabel('y')\n",
    "plt.colorbar(label='z')\n",
    "plt.savefig(\"output/gradient_func_contour.pdf\",\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Let's wrap this code in an auxiliary function so that we need not repeat it each time from now on:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bwmhWtKaLT4t"
   },
   "outputs": [],
   "source": [
    "def plot_func(xx, yy, f, X=None):\n",
    "    if not X is None:\n",
    "        Xmin, Xmax = X[:, 0].min(), X[:, 0].max() \n",
    "        Ymin, Ymax = X[:, 1].min(), X[:, 1].max()\n",
    "        \n",
    "        if (Xmin < xx.min() or Xmax > xx.max() or\n",
    "                Ymin < yy.min() or Ymax > yy.max()):            \n",
    "            xx = np.linspace(Xmin, Xmax, 100)\n",
    "            yy = np.linspace(Ymin, Ymax, 100)\n",
    "            xx, yy = np.meshgrid(xx, yy)\n",
    "            \n",
    "        plt.scatter(X[:, 0], X[:, 1], zorder=10)\n",
    "        \n",
    "    zz = f(xx, yy)\n",
    "    plt.contour(xx, yy, zz, cmap='Spectral')\n",
    "    # both axes at the same scale + create a legend\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.xlabel('x'); plt.ylabel('y')\n",
    "    plt.colorbar(label='z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Gradient of a Function\n",
    "\n",
    "To be able to minimize function $f(x, y)$ using gradient descent, we need to determine its gradient. Let us recall that gradient $\\nabla f(x, y)$ of function $f(x, y)$ is the vector of its first-order partial derivatives. In our case therefore:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla f(x, y) = \\left(\n",
    "    \\frac{\\partial f}{\\partial x},\n",
    "    \\frac{\\partial f}{\\partial y}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "Let us also recall that our function has the form of $f(x, y) = x^2 + y^2$. It is therefore easy to determine the partial derivatives. When computing the partial derivative by $x$, term $y^2$ will be considered a constant and we will only be differentiating $x^2$. When computing the partial derivative by $y$ it will be vice versa. And so we obtain:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial x} &= 2x \\\\[0.75em]\n",
    "\\frac{\\partial f}{\\partial y} &= 2y\n",
    "\\end{align}\n",
    "Our iterative update rule was expressed in vector form – that is to say, for our function with two arguments $x, y$, vector $\\mathbf{x}$ will be 2-dimensional and it will take the form of $\\mathbf{x} = (x, y)$.\n",
    "\n",
    "---\n",
    "#### Task 1: Computing the Gradient\n",
    "\n",
    "**Fill in the blanks in the following cell so that function `grad_f` will return the gradient of function $f(x, y)$:** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0H4sV7gyLT5N",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def grad_f(x, y):\n",
    "    return np.array([      ,      ])    # ----\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Visualizing the Gradient\n",
    "\n",
    "As we know, the gradient of a function indicates the direction of its steepest ascent. To get a better idea of what this means we can visualize the gradient, which we have just defined:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owLy5FAyLT5x"
   },
   "outputs": [],
   "source": [
    "xxg, yyg = np.mgrid[-10:11:1.5, -10:11:1.5]\n",
    "gg = np.array(\n",
    "    [[grad_f(x, y) for x, y in zip(rx, ry)] \n",
    "          for rx, ry in zip(xxg, yyg)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "WS5GgEj8LT6E",
    "outputId": "30b64544-adee-4569-9193-899ec9d4afc9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,8])\n",
    "plot_func(xx, yy, f)\n",
    "plt.quiver(xxg, yyg, gg[..., 0], gg[..., 1])\n",
    "plt.savefig(\"output/gradient_func_quiver.pdf\",\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The arrows indicate the gradient's direction. As we can see, they are all pointing outwards – in the direction the paraboloid increases. The size of the arrows indicates the magnitude of the gradient. The arrows close to the centre are tiny (the derivative at a minimum is zero) and they grow towards the margins: because the function grows faster and faster.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Now we will continue by applying gradient descent in order to minimize our function. As the first step we will define a few parameters: the number of steps and the learning rate:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g5PmPNhjLT6k"
   },
   "outputs": [],
   "source": [
    "num_steps = 20\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store all the computed points in a matrix so that we can later visualize them. The matrix will have 2 columns – one for $x$ and the other for $y$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmUYj8FyLT6r"
   },
   "source": [
    "Aby sme si mohli neskôr postup minimalizácie vizualizovať, uložíme si všetky vypočítané body do matice. Matica má 2 stĺpce – jeden pre $x$ a druhý pre $y$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KSooss2kLT6t"
   },
   "outputs": [],
   "source": [
    "X = np.zeros((num_steps + 1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The initial point can either be selected randomly:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPLJ0uY-LT67"
   },
   "outputs": [],
   "source": [
    "X[0] = np.random.uniform(-10, 10, (2,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "or we can opt in for some fixed point, so that we get the same result every time:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCkZxt6HLT7E"
   },
   "outputs": [],
   "source": [
    "X[0] = [-9, -8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Let us once again recall our iterative update rule for computing the next point:\n",
    "\\begin{equation}\n",
    "\\mathbf{x}_{i+1} = \\mathbf{x}_i - \\gamma \\nabla f(\\mathbf{x}_i)\n",
    "\\end{equation}\n",
    "where $\\gamma$ is the learning rate and $\\nabla f(\\mathbf{x}_i)$ is the gradient of the function we are going to minimize.\n",
    "\n",
    "---\n",
    "#### Task 2: Implementing Gradient Descent\n",
    "\n",
    "**Fill in the blanks in the following cell (rewrite the update rule in code):** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ZkqfcF4LT7m",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "for i in range(num_steps):\n",
    "    \n",
    "    \n",
    "    X[i+1] =      # ----\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The computed points can finally be visualized and we can ascertain that they really do converge to the minimum of the paraboloid:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJZ_6w4yLT76",
    "outputId": "03098f45-ecf6-4ee6-d0ef-b30cfc5def03"
   },
   "outputs": [],
   "source": [
    "plot_func(xx, yy, f, X)\n",
    "plt.savefig(\"output/gradient_mini_steps.pdf\",\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "---\n",
    "#### Task 3: Wrapping the Code in a Function\n",
    "\n",
    "**Let us now again wrap this piece of code in a function so that we can call it repeatedly:** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZZF1sbaLT9G",
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def grad_desc(grad_func, init_point,\n",
    "              num_steps, learning_rate):\n",
    "    X = np.zeros((num_steps + 1, 2))\n",
    "    X[0] = init_point\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        \n",
    "        \n",
    "        X[i+1] =      # ---- \n",
    "        \n",
    "        \n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We can test our new function using:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jai8XBYqLT9-",
    "outputId": "e3f3cbc1-6e99-43b2-d63e-2bdec25ebaf3"
   },
   "outputs": [],
   "source": [
    "X = grad_desc(grad_f, init_point=[-9, -8],\n",
    "              num_steps=20, learning_rate=0.1)\n",
    "plot_func(xx, yy, f, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Testing Different Learning Rates\n",
    "\n",
    "To illustrate how things work we will now try doing gradient descent with different learning rates.\n",
    "\n",
    "Let's start with $\\gamma = 0.45$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ldMyRMrjLT-H",
    "outputId": "211495c5-692b-4b2e-9318-0b3155841b32"
   },
   "outputs": [],
   "source": [
    "X = grad_desc(grad_f, init_point=[-9, -8], num_steps=20,\n",
    "      learning_rate=0.45\n",
    ")\n",
    "plot_func(xx, yy, f, X)\n",
    "plt.savefig(\"output/gradient_lr_0_45.pdf\",\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "For $\\gamma = 1.0$ the algorithm will start to oscillate and it will no longer converge to the minimum:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MFwsRSleLT-R",
    "outputId": "08eb3261-6d08-4314-a7d1-929c0aeca222"
   },
   "outputs": [],
   "source": [
    "X = grad_desc(grad_f, init_point=[-9, -8], num_steps=20,\n",
    "      learning_rate=1.0\n",
    ")\n",
    "plot_func(xx, yy, f, X)\n",
    "plt.savefig(\"output/gradient_lr_1_0.pdf\",\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "With $\\gamma > 1$ the algorithm will start to diverge and it will actually keep moving away from the minimum:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkZDJ5vNLT-b",
    "outputId": "6afa3ee4-3352-408b-af02-1388b787085c"
   },
   "outputs": [],
   "source": [
    "X = grad_desc(grad_f, init_point=[-9, -8], num_steps=20,\n",
    "      learning_rate=1.02\n",
    ")\n",
    "plot_func(xx, yy, f, X)\n",
    "plt.savefig(\"output/gradient_lr_1_02.pdf\",\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Computing Symbolic Gradient Automatically\n",
    "\n",
    "When deriving the gradient of our function above we have computed it analytically and then rewrote it as source code by hand. However, in Python it is possible to compute the symbolic gradient automatically – using the `sympy` package. In the following example we will show how this can be done.\n",
    "\n",
    "We will start by defining some symbolic variables that we are going to need – $x$ and $y$ in our case:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gEzjTaPkLT-1"
   },
   "outputs": [],
   "source": [
    "symx, symy = sp.symbols('x y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We will now define function $f(x, y)$ using the symbolic variables:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6rl_9lH4LT_C",
    "outputId": "2f38f4a3-6fd0-4182-d2d4-17d3b1fb7553"
   },
   "outputs": [],
   "source": [
    "symf = symx**2 + symy**2\n",
    "symf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "When computing the symbolic gradient we will use a little trick. We will first transform our scalar function into matrix form and then compute its Jacobian. We will get a row vector that corresponds to the gradient as a result:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "slggN0K9LT_J",
    "outputId": "87aa738c-2989-405d-d977-1e0ef1e6bb45"
   },
   "outputs": [],
   "source": [
    "sym_grad_f = sp.Matrix([symf]).jacobian([symx, symy])\n",
    "sym_grad_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "To be able to use the resulting symbolic representation of gradient to actually compute it for particular values, we'll need to convert it to a standard numeric function. We will also do the same for function $f$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xtl0XF_cLT_P"
   },
   "outputs": [],
   "source": [
    "f = lambdify((symx, symy), symf, \"numpy\")\n",
    "grad_f = lambdify((symx, symy), sym_grad_f, \"numpy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We can now apply gradient descent in exactly the same way we did before – but now we no longer need to compute the gradient by hand:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qg0GQFbFLT_b",
    "outputId": "2f5812ae-4d1d-4928-f46a-1eea35262684"
   },
   "outputs": [],
   "source": [
    "xx, yy = np.mgrid[-10:10.2:0.2, -10:10.2:0.2]\n",
    "zz = f(xx, yy)\n",
    "\n",
    "X = grad_desc(grad_f, init_point=[-9, -8],\n",
    "              num_steps=20, learning_rate=0.1)\n",
    "plot_func(xx, yy, f, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "1_gradient_descent.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
