{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install yellowbrick\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "from class_utils import ColGrid, sorted_order, show_tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from yellowbrick.contrib.classifier import DecisionViz\n",
    "from yellowbrick.cluster import SilhouetteVisualizer, KElbowVisualizer\n",
    "# revert yellowbrick's invasive changes to matplotlib's\n",
    "# styling; also suppressing deprecation warnings\n",
    "import warnings\n",
    "import yellowbrick\n",
    "\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    yellowbrick.style.rcmod.set_aesthetic('reset')\n",
    "    yellowbrick.style.rcmod.reset_orig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "\n",
    "# create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# create a synthetic dataset\n",
    "_blobs, _labels = datasets.make_blobs(\n",
    "    n_samples=600, random_state=3,\n",
    "    cluster_std=0.75, centers=5\n",
    ")\n",
    "\n",
    "_df_blobs = pd.DataFrame(np.hstack([_blobs, _labels.reshape(-1, 1)]),\n",
    "                         columns=['x', 'y', 'label'])\n",
    "_df_blobs['y'] *= 100\n",
    "_df_blobs.to_csv(\"data/blobs_2d.csv\", index=False)\n",
    "\n",
    "del _blobs\n",
    "del _labels\n",
    "del _df_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Auxiliary Functions -- { display-mode: \"form\" }\n",
    "cluster_colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k', 'gray']\n",
    "\n",
    "def scatter_legend(ax, sc, labels, num_colors, color_array,\n",
    "                   s, edgecolor):\n",
    "    handles = []\n",
    "    \n",
    "    for i in range(num_colors):\n",
    "        h = mlines.Line2D([0], [0], ls=\"\", color=color_array[i],\n",
    "                          ms=s, marker=sc.get_paths()[0],\n",
    "                          markeredgecolor=edgecolor)\n",
    "        handles.append(h)\n",
    "\n",
    "    ax.legend(handles=handles, labels=labels)\n",
    "\n",
    "def plot_data(\n",
    "    data, cluster_centres=None, color='b', ax=None,\n",
    "    cluster_colors=cluster_colors,\n",
    "    edgecolors='k', labels=None,\n",
    "    center_color='orange', center_size=200,\n",
    "    legend=True\n",
    "):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    if labels is None:\n",
    "        ax.scatter(data[:, 0], data[:, 1], s=50,\n",
    "                   color=color, edgecolors=edgecolors)\n",
    "    else:\n",
    "        c = np.asarray(cluster_colors)[labels]\n",
    "        \n",
    "        sc = ax.scatter(data[:, 0], data[:, 1], s=50,\n",
    "                        c=c, edgecolors=edgecolors,\n",
    "                        #cmap=plt.cm.get_cmap('category10', np.max(labels)+1)\n",
    "                       )\n",
    "        \n",
    "        if legend:\n",
    "            nclusts = np.max(labels)+1\n",
    "            scatter_legend(ax, sc, ['$c_{}$'.format(i) for i in range(nclusts)],\n",
    "                           nclusts, cluster_colors, s=6, edgecolor='k')\n",
    "        \n",
    "    if not cluster_centres is None:\n",
    "        ax.scatter(cluster_centres[:, 0],\n",
    "                   cluster_centres[:, 1],\n",
    "            s=center_size, c=center_color,\n",
    "            edgecolors=edgecolors)\n",
    "\n",
    "    ax.grid(ls='--')\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_axisbelow(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## k-means in Scikit-Learn\n",
    "\n",
    "Now that we have explored the principles behind $k$-means, we will have a look at a practical implementation from the well-known `scikit-learn` package. In addition to illustrating how to apply $k$-means to a dataset, we will have a look at some visualization techniques that will allow us to explore some of the clusters' properties and even help us to select a good number of clusters $k$.\n",
    "\n",
    "### Preprocessing: Remember to Normalize the Data\n",
    "\n",
    "To load and preprocess the data, we will be using our standard preprocessing pipeline, which **normalizes**  (standardizes) numeric columns. Given that $k$-means is based on distances, proper scaling is crucial and we **should not forget to normalize** . If the range of a certain column is much larger than that of the other columns and we fail to normalize it, that column will have a far greater influence on the results of the clustering than the other columns. This is usually not desirable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load the data from the CSV\n",
    "df = pd.read_csv(\"data/blobs_2d.csv\")\n",
    "\n",
    "# all inputs are numeric\n",
    "categorical_inputs = []\n",
    "numeric_inputs = list(df.columns[:-1])\n",
    "\n",
    "# the preprocessing pipeline\n",
    "input_preproc = make_column_transformer(\n",
    "    (make_pipeline(\n",
    "        SimpleImputer(strategy='constant', fill_value='MISSING'),\n",
    "        OneHotEncoder()),\n",
    "     categorical_inputs),\n",
    "    \n",
    "    (make_pipeline(\n",
    "        SimpleImputer(),\n",
    "        StandardScaler()),\n",
    "     numeric_inputs)\n",
    ")\n",
    "\n",
    "# the preprocessed data and the classes\n",
    "X = input_preproc.fit_transform(df[categorical_inputs+numeric_inputs])\n",
    "labels = df[\"label\"]\n",
    "\n",
    "# let's also keep the unnormalized data\n",
    "X_unnorm = df[categorical_inputs+numeric_inputs].values\n",
    "\n",
    "# plot the data\n",
    "plt.figure(figsize=(6, 5))\n",
    "plot_data(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### The Clustering\n",
    "\n",
    "Now that we have the data, it is the easiest thing in the world to do the clustering. All we need to do is create a `KMeans` object from `scikit-learn`, specifying the number of clusters as $k=5$. We then fit the `KMeans` object to the data using the standard `fit` interface. Note that we are doing unsupervised learning here so there are no desired outputs `y`. Method `predict` returns our cluster identifiers.\n",
    "\n",
    "Once the clustering is done, we plot our data again: this time colouring points by the computed cluster identifiers. This will allow us to verify that clustering went correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=5)\n",
    "model.fit(X)\n",
    "clusts = model.predict(X)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plot_data(X, labels=clusts, legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### Clustering with Unnormalized Data\n",
    "\n",
    "To see why normalizing the data is so essential, we will now also compute a clustering for the unnormalized version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=5)\n",
    "model.fit(X_unnorm)\n",
    "clusts = model.predict(X_unnorm)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plot_data(X_unnorm, labels=clusts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "As you can see, the clusters are not quite right this time. This is because dimension $y$ has much greater range and it is therefore given more weight.\n",
    "\n",
    "### Determining the Number of Clusters $k$\n",
    "\n",
    "In the example above we assumed that we know the correct number of clusters $k$. In practice this is seldom true: unless we already know precisely what we are looking for. So how do we determine a good value of $k$? There are, in fact, several methods.\n",
    "\n",
    "#### The Elbow Plot\n",
    "\n",
    "One way to determine a good value of $k$ is using an elbow plot. The idea behind this kind of plot is to run $k$-means clustering for different values of $k$, compute the distortion score for each and plot the results. In the plot, we are then looking for an \"elbow\", i.e. the point with the maximum curvature – where the steepness of the plot changes the most. To create our elbow plots, we will be using the `yellowbrick` package, which also finds and visualizes the elbow point automatically using a knee point detection algorithm [[yellowbrick]](#yellowbrick).\n",
    "\n",
    "The distortion score is computed as the sum of squared errors (SSE), i.e. the sum of Euclidean distances between points and their corresponding cluster centres [[yellowbrick](#yellowbrick), [k_research](#k_research)]:\n",
    "$$\n",
    "J(C) = \\sum*{j=1}^{k} \\sum* {x_i \\in c_j} | x_i - \\mu_j |^2.\n",
    "$$\n",
    "\n",
    "Note that this is the same criterion that $k$-means is trying to minimize. Consider also that we cannot simply pick the $k$ that results in the smallest distortion: that would mean creating as many clusters as there are points, which would reduce distortion to zero, but would not result in a good clustering. The intuition behind picking the elbow point is that once you arrive at the correct value of $k$, the distortion should decrease sharply because there is now a cluster centre reasonably close to each point. Adding further clusters should not have as much effect now: it is just going to split already well-defined clusters into smaller ones.\n",
    "\n",
    "It is possible to create an elbow plot using other score functions, e.g. the Calinski-Harabasz index or the Silhoutte score Calinski-Harabasz [[yellowbrick](#yellowbrick), [ch_index](#ch_index)]: feel free to experiment with that. Also, we will be using Silhoutte scores later in the notebook to perform Silhoutte analysis. \n",
    "\n",
    "#### The Elbow Plot: An Example\n",
    "\n",
    "Now let us use the elbow plot to determine the best $k$ for our dataset. We will use the `KElbowVisualizer` class from `yellowbrick` and try $k \\in \\{2, 3, ..., 9\\}$. Since we already know that the correct number of clusters is 5 in our case, we should observe that the elbow is at $k=5$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = KElbowVisualizer(model, k=(2, 10), timings=False)\n",
    "visualizer.fit(X)\n",
    "visualizer.ax.grid(ls='--')\n",
    "visualizer.finalize()\n",
    "\n",
    "plt.savefig(\"output/kmeans_elbow.svg\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Silhouette Analysis\n",
    "\n",
    "Silhouette analysis is another approach that can be used to select a good value of $k$. It also provides a way to visualize some key cluster properties for each $k$. The approach is based on the Silhouette coefficient, which is defined as follows [[k_research]](#k_research):\n",
    "$$\n",
    "s(x_i) = \\frac{\n",
    "    b_i - a_i\n",
    "}{\n",
    "    \\max{ a_i, b_i }\n",
    "},\n",
    "$$\n",
    "\n",
    "where $a_i$ is the **intra-cluster dissimilarity** , i.e. the average distance of sample $x_i$ from all the other samples in the same cluster; and $b_i$ is the **inter-cluster dissimilarity** , i.e. the shortest distance to a sample from a different cluster. The smaller the intra-cluster dissimilarity $a_i$, the more sample $x_i$ should belong in the cluster. The greater the inter-cluster dissimilarity $b_i$, the less the sample $x_i$ should belong to any other cluster [[k_research]](#k_research).\n",
    "\n",
    "The **Silhouette score**  is the mean of the Silhouette coefficients across all samples $x_i \\in X$:\n",
    "$$\n",
    "S = \\frac{\\sum_{x_i \\in X} s(x_i)}{|X|},\n",
    "$$\n",
    "where $X$ is the dataset and $|X|$ is the number of samples in it (its cardinality).\n",
    "\n",
    "The greater the Silhouette score, the greater on average is the degree to which samples should belong in their clusters and not in any other clusters. We can therefore plot the Silhouette scores for multiple values of $k$ and pick the $k$ with the maximum score. We will again use `KElbowVisualizer` to create the plot, but this time we specify `silhouette` as the metric. Clearly the maximum value is at $k=5$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = KElbowVisualizer(model, k=(3, 8),\n",
    "                              metric='silhouette',\n",
    "                              timings=False)\n",
    "visualizer.fit(X)\n",
    "visualizer.ax.grid(ls='--')\n",
    "visualizer.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The great thing about Silhouette analysis though, is that it also provides an easy way to visualize key properties of all the individual clusters. What we do is compute the Silhouette coefficients for all the points, group them by clusters, order them by magnitude and display them.\n",
    "\n",
    "The resulting plots will show how large each cluster is and how certain it is that each of its individual points should belong into the cluster and not into other clusters: points with lower Silhouette coefficients are likely to be on the borders of clusters and points with very low coefficients might be clustered incorrectly.\n",
    "\n",
    "In the plots below we show Silhouette plots and scatter plots side by side to make comparisons easier, but note that in practice, when working with high-dimensional datasets, you would, of course, only have the Silhouette plots at your disposal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(3, 8)\n",
    "fig, axes = plt.subplots(len(k_range), 2)\n",
    "\n",
    "for k, ax in zip(k_range, axes):\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    visualizer = SilhouetteVisualizer(\n",
    "        model, colors=cluster_colors,\n",
    "        alpha=1.0, ax=ax[0])\n",
    "    visualizer.fit(X)\n",
    "    visualizer.ax.grid(ls='--')\n",
    "    visualizer.finalize()\n",
    "    \n",
    "    clusts = model.predict(X)\n",
    "    plot_data(X, labels=clusts, ax=ax[1])\n",
    "\n",
    "fig.set_size_inches([10, len(k_range)*3])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Note the very low Silhouette coefficients of some points indicating that they are probably not clustered correctly. With $k=5$ no points have very low coefficients, which indicates that this is a much better clustering. Note also how we can visually compare cluster sizes using these plots.\n",
    "\n",
    "### Interpreting Clusters\n",
    "\n",
    "In data analysis finding clusters is usually the easy part of the job: interpreting them is more challenging. However, the set of tools required to interpret cluster is fundamentally as same as that used for exploratory data analysis. We can add the cluster identifiers as a new column into our dataset and then proceed to examine interactions with other columns, plot the ones that show correlation in more detail etc.\n",
    "\n",
    "#### Violin Plots\n",
    "\n",
    "In our case, violin plots might be useful: they would tell us to what ranges of $x$ and $y$ each cluster corresponds roughly. To make comparisons easier, we also include a corresponding scatter plot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=5)\n",
    "model.fit(X)\n",
    "clusts = model.predict(X)\n",
    "\n",
    "# add the cluster identifiers as a new column to the dataset\n",
    "df['cluster'] = clusts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ColGrid(df, ['cluster'], ['x', 'y'])\n",
    "fig, axes = g.map_dataframe(sorted_order(sns.violinplot),\n",
    "                  palette=cluster_colors)\n",
    "fig.set_size_inches((10, 4))\n",
    "\n",
    "for ax in axes:\n",
    "    ax.grid(ls='--')\n",
    "    ax.set_axisbelow(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X_unnorm, labels=clusts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "#### A Decision Tree\n",
    "\n",
    "We could also fit a small decision tree to the data and see what rules it comes up with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_unnorm, clusts)\n",
    "show_tree(dtree, numeric_inputs, filled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "To make it easier to check that the decision tree got the cluster boundaries right, we can, in our case, also plot its decision boundaries w.r.t. $x$ and $y$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = DecisionViz(\n",
    "    dtree, features=numeric_inputs,\n",
    "    classes=['$c_{}$'.format(i) for i in range(np.max(clusts)+1)]\n",
    ")\n",
    "\n",
    "viz.fit(X_unnorm, clusts)\n",
    "viz.draw(X_unnorm, clusts)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "<a id=\"k_research\">[k_research]</a> Yuan, C. and Yang, H., 2019. Research on K-value selection method of K-means clustering algorithm. J—Multidisciplinary Scientific Journal, 2(2), pp.226-235.\n",
    "\n",
    "<a id=\"ch_index\">[ch_index]</a> Wang, X. and Xu, Y., 2019, July. An improved index for clustering validation based on Silhouette index and Calinski-Harabasz index. In IOP Conference Series: Materials Science and Engineering (Vol. 569, No. 5, p. 052024). IOP Publishing.\n",
    "\n",
    "<a id=\"yellowbrick\">[yellowbrick]</a> Yellowbrick. URL: <https://github.com/DistrictDataLabs/yellowbrick>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a351393365bb1b108989afa08de3243f72f5e58927baf5d192f3cca79a41cbc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
