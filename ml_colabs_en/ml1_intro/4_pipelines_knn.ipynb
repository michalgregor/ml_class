{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PO3oXeh8oKtU",
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bs97NTOSoeVF"
   },
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "0JZDvAVnoop7",
    "outputId": "490a1dcf-baf9-47b6-b144-e281279c1758"
   },
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "DATA_HOME = \"https://github.com/michalgregor/ml_notebooks/blob/main/data/{}?raw=1\"\n",
    "\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(DATA_HOME.format(\"titanic.zip\"), directory=\"data/titanic\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdSF9qRZoKuT",
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Preprocessing Data Using Pipelines\n",
    "\n",
    "In the previous notebook we have shown how to apply data preprocessing in a reproducible way. The method that we have shown was correct, but rather laborious (and therefore error-prone). We will now introduce a more practical approach to reproducible preprocessing – using the concept of **pipelines**  – also from the `scikit-learn` package.\n",
    "\n",
    "### Loading the Data\n",
    "\n",
    "As the first step we will again load the [Titanic](https://www.kaggle.com/c/titanic) dataset and split it into training and testing data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MW-EFVyGoKud"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/titanic/train.csv\")\n",
    "df_train, df_test = train_test_split(df, test_size=0.25,\n",
    "                     stratify=df[\"Survived\"], random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "gfL4GQeToKup",
    "outputId": "b6abe323-188c-4efc-eb69-5c6719eeebd5"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FSALKiA4oKu2",
    "tags": [
     "en"
    ]
   },
   "source": [
    "Let's recall what each column stands for:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "colab_type": "code",
    "id": "gxYGhx8eoKu9",
    "outputId": "3abae9b5-dab0-4fee-ddfd-9c8c3a7340c7"
   },
   "outputs": [],
   "source": [
    "with open(\"data/titanic/description\", \"r\") as file:\n",
    "    print(\"\".join(file.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e_RYZbIfoKvJ",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Column Selection\n",
    "\n",
    "As we already know, our dataset has a number of columns. Some of them are categorical and numerical. As we have seen in the previous notebook we will want to apply slightly different kinds of preprocessing to each of these.\n",
    "\n",
    "It is likely that some columns we will not want to use at all, because the information contained in them is either not useful, or we are at least not able to extract it yet. Column `PassengerId`, for an instance, contains a unique numeric identifier for each record. It is probably not a good idea to use this as an input, because it does not contain any generalizable information. The unique identifiers should have been assigned at random in our case and they should not carry any information content.\n",
    "\n",
    "Columns `Name`, `Cabin` and others might be found to contain generalizable information, if we were able to extract it using suitable preprocessing (e.g. the names contain titles, which could carry generalizable information; also, the cabin number could indicate which part of the ship the cabin was in etc.). However, since we do not know how to do such preprocessing yet, we will simply drop such columns.\n",
    "\n",
    "We will split the remaining columns into two groups based on whether they are numeric or categorical. Column `Survived` represents the desired output: we will not preprocess it along with the other columns, but by itself (also, it already takes values 0 and 1, so no actual preprocessing is even necessary).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pFZI0Pe3oKvT"
   },
   "outputs": [],
   "source": [
    "categorical_inputs = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
    "numeric_inputs = [\"Age\", \"SibSp\", 'Parch', 'Fare']\n",
    "\n",
    "output = \"Survived\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zWjb6HXhoKvd",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Constructing the Pipeline and Preprocessing the Data\n",
    "\n",
    "Given that numeric columns need to be preprocessed in a different way than categorical columns, we will use the built-in `make_column_transformer` function, which will allow us to specify different pipelines for different columns. The columns that we do not list at all will be dropped. If we want to reproduce the preprocessing from the previous notebook using pipelines we can use the following code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GDcTInJfoeW2"
   },
   "outputs": [],
   "source": [
    "input_preproc = make_column_transformer(\n",
    "    (make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OrdinalEncoder()),\n",
    "     categorical_inputs),\n",
    "    \n",
    "    (make_pipeline(\n",
    "        SimpleImputer(),\n",
    "        StandardScaler()),\n",
    "     numeric_inputs)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nlqUd9fxoKvv",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We will first use the `fit_transform` function to fit our new pipeline object and also preprocess our original dataset at the same time. We will also extract the column with desired outputs from the dataset. We will also reshape the desired outputs into a 1-dimensional array, since this is what our `KNeighborsClassifier` will expect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QyEPeHwloeXM"
   },
   "outputs": [],
   "source": [
    "X_train = input_preproc.fit_transform(df_train[categorical_inputs+numeric_inputs])\n",
    "Y_train = df_train[output].values.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PbGePVcAoKwB",
    "tags": [
     "en"
    ]
   },
   "source": [
    "To preprocess the testing data we will use the same pipeline object.\n",
    "\n",
    "**Let us keep in mind that now we will be using the `transform` method a not the `fit_transform` method, because we do not want to fit our pipeline. We only want to transform the testing data in the same way we did with the training data.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBxScMLRoKwI"
   },
   "outputs": [],
   "source": [
    "X_test = input_preproc.transform(df_test[categorical_inputs+numeric_inputs])\n",
    "Y_test = df_test[output].values.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djsLMC0coKwQ",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Training\n",
    "\n",
    "Finally, everything is ready for training the model itself. We can again use the `KNeighborsClassifier`, which we already know from one of our previous notebooks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "N1B9m9wAoKwX",
    "outputId": "11c3d285-e90e-47e7-cee5-f21fb45ea4e8"
   },
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wKPeUoroKwg",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Testing\n",
    "\n",
    "We can next test our model on the testing data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_10sGf5roKwk"
   },
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "suvB79PQoKwu",
    "tags": [
     "en"
    ]
   },
   "source": [
    "We will display the confusion matrix and compute the accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "V3iE2EzXoKwz",
    "outputId": "a8fb6a3e-fb19-4827-fa54-2c84b292461c"
   },
   "outputs": [],
   "source": [
    "cm = pd.crosstab(Y_test, y_test,\n",
    "                 rownames=['actual'],\n",
    "                 colnames=['predicted'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "d9tPhzZIoKw6",
    "outputId": "1d8ae7b3-3cee-49f5-862b-30d87aa29125"
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(Y_test, y_test)\n",
    "print(\"Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MdG2q1P0ojj1",
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Keeping Track of Which Values Were Missing\n",
    "\n",
    "When considering columns with missing values – whether numeric or categorical – in addition to imputing the missing values, it can be a useful practice to keep track of which values were missing. Our imputation procedure may, for instance, systematically over- or underestimate the missing values. If our model knows which values were missing it can learn to compensate for that.\n",
    "\n",
    "We can automatically identify the columns with missing values and apply the `MissingIndicator` transformer to them: this will produce new binary columns indicating whether a particular value was missing or not. Naturally, there is no guarantee that doing this will always improve the results – it may depend on the dataset and on the machine learning method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jjGE2W2QoKxP"
   },
   "outputs": [],
   "source": [
    "has_missing = df_train.isnull().any()\n",
    "for_missing_tracking = has_missing[has_missing].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HxsrjPNQoKxX"
   },
   "outputs": [],
   "source": [
    "tracking_input_preproc = make_column_transformer(\n",
    "    (make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OrdinalEncoder()),\n",
    "     categorical_inputs),\n",
    "    \n",
    "    (make_pipeline(\n",
    "        SimpleImputer(),\n",
    "        StandardScaler()),\n",
    "     numeric_inputs),\n",
    "    \n",
    "    # ---------------------\n",
    "    (MissingIndicator(),\n",
    "     for_missing_tracking)\n",
    "    # ---------------------\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ZGDJvGwoKx4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "4_pipelines_knn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
