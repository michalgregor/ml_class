{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "**NOTE: This notebook is written for the Google Colab platform. However it can also be run (possibly with minor modifications) as a standard Jupyter notebook.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Installation of Packages -- { display-mode: \"form\" }\n",
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/michalgregor/class_utils.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Import of Necessary Packages -- { display-mode: \"form\" }\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, KBinsDiscretizer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from class_utils import error_histogram\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title -- Downloading Data -- { display-mode: \"form\" }\n",
    "DATA_HOME = \"https://github.com/michalgregor/ml_notebooks/blob/main/data/{}?raw=1\"\n",
    "\n",
    "from class_utils.download import download_file_maybe_extract\n",
    "download_file_maybe_extract(DATA_HOME.format(\"sigmoid_regression_data.csv\"), directory=\"data\")\n",
    "\n",
    "# also create a directory for storing any outputs\n",
    "import os\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Regression and Corresponding Data Preprocessing\n",
    "\n",
    "As our next example we will show how to solve a simple regression task. As our regression method (just as in the case of classification) we will use the $k$ nearest neighbours method (KNN). For classification the prediction was determined by voting among the $k$ nearest neighbours. In the case of regression the prediction is formed by computing the average of the $k$ nearest neighbours' values. \n",
    "\n",
    "### Loading the Dataset\n",
    "\n",
    "Our example will begin as usual: by loading a dataset from a CSV file: in this case the file will only have two columns. The first column, $x$, will represent the input of our model and the second, $y$, will represent the desired output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/sigmoid_regression_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Stratification When Splitting the Dataset\n",
    "\n",
    "The next step, of course, is to split the dataset into the train and the test set. Let us recall that we used stratification by the label when splitting the dataset for a classification task. This was to ensure that the ratio of the classes in the training and the testing set will remain close to that in the original dataset.\n",
    "\n",
    "Obviously in the case of regression we cannot do the same thing – or at least not quite as easily as for classification. This is because the desired outputs are now continuous. In some cases, however, it may still be desirable to apply some kind of stratification – especially if the dataset is small, as it is in our case. There is otherwise a risk that the training and the testing set will not be representative.\n",
    "\n",
    "To show how such problem might manifest itself is very easy: all we need to do is to split our dataset in the normal way without using stratification and to visualize the training and the testing set in the same plot:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_train['x'], df_train['y'], marker='x', label=\"tréningové dáta\")\n",
    "plt.scatter(df_test['x'], df_test['y'], c='r', label=\"testovacie dáta\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(ls='--')\n",
    "plt.legend()\n",
    "plt.savefig(\"output/regression_split_plain.pdf\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "What we should be able to see in the plot is that the testing data does not even begin to cover the entire space. So how do we apply stratification to prevent this problem? Given that the column by which we stratify should be discrete, the simplest solution is, of course, to discretize our continuous column – e.g. using the `KBinsDiscretizer` transformer from `scikit-learn`. The discretization itself might look as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbins = KBinsDiscretizer(6, encode='ordinal')\n",
    "y_stratify = kbins.fit_transform(df[['y']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The discretized column `y_stratify` can now be used for stratification:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, stratify=y_stratify,\n",
    "                                 test_size=0.3, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "When we visualize the results of such splitting, we should be able to observe that the samples have been split into the training and the testing set much more uniformly:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_train['x'], df_train['y'], marker='x', label=\"training data\")\n",
    "plt.scatter(df_test['x'], df_test['y'], c='r', label=\"testing data\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(ls='--')\n",
    "plt.legend()\n",
    "plt.savefig(\"output/regression_split_stratif.pdf\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "---\n",
    "### Task 1: Constructing the Pipeline\n",
    "\n",
    "**The next step is, as usual, to construct the preprocessing pipeline. As the first step, determine which input columns are categorical and which are numeric and then create the same pipeline object that we have been using the previous examples.** \n",
    "\n",
    "NOTE: It will not be necessary to preprocess the output column in our case.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "categorical_inputs = [           ]  # ----\n",
    "\n",
    "numeric_inputs = [               ]  # ----\n",
    "\n",
    "output = 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "input_preproc = make_column_transformer(\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "The data will be preprocessed in the usual way:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = input_preproc.fit_transform(df_train[categorical_inputs+numeric_inputs])\n",
    "Y_train = df_train[output].values\n",
    "\n",
    "X_test = input_preproc.transform(df_test[categorical_inputs+numeric_inputs])\n",
    "Y_test = df_test[output].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Training the Model\n",
    "\n",
    "Our model will be trained using the unified interface – we will only need to change its type. We will be using a `KNeighborsRegressor` instead of a `KNeighborsClassifier`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsRegressor(n_neighbors=5)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Testing the Model\n",
    "\n",
    "Finally the behaviour of our model will be evaluated on the testing data. For regression we will naturally not use accuracy as a performance metric – regression has its own indicators. Among the most frequently used ones are the **mean squared error**  (MSE):\n",
    "\\begin{equation}\n",
    "MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2.\n",
    "\\end{equation}\n",
    "where $y_i$ is the desired output and $\\hat{y}_i$ is the actual output of our model for sample $i$ from the dataset and $N$ is the total number of samples in the dataset.\n",
    "\n",
    "and the **mean absolute error**  (MAE):\n",
    "\\begin{equation}\n",
    "MAE = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|.\n",
    "\\end{equation}\n",
    "\n",
    "Mean squared error is more sensitive to large errors (due to the square). The advantage of mean absolute error is that it from the same scale as the data and it is therefore easier to interpret (even though it is alway possible to take the square root of the mean squared error as well). There is, of course, a number of other indicators (e.g. percentage errors, the coefficient of determination, etc.), but we will not be considering these in the present notebook.\n",
    "\n",
    "Let us then use our regression model and compute predictions for the testing set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "We will compute and display the mean square and absolute error. Let us recall that the range of the outputs is approximately 0 to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(Y_test, y_test)\n",
    "mae = mean_absolute_error(Y_test, y_test)\n",
    "\n",
    "print(\"MSE: {}\".format(mse))\n",
    "print(\"MAE: {}\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "To get a fuller idea of how the errors are distributed we can also visualize the entire distribution using the output and error histogram. We will standardize the outputs for the purposes of this plot to get the histograms of the outputs and the error to align. We will also use scaled data to compute the mean absolute error to ensure that it is on the same scale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "error_histogram(Y_test, y_test, Y_fit_scaling=Y_train)\n",
    "plt.savefig(\"output/error_output_histogram.pdf\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Visualizing the Regression Relationship\n",
    "\n",
    "Given that our regression relationship is 2-dimensional, we can easily visualize it in a 2D plot and evaluate it visually as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = min(np.min(X_train), np.min(X_test))\n",
    "x_max = max(np.max(X_train), np.max(X_test))\n",
    "\n",
    "xx = np.linspace(x_min, x_max, 250).reshape((-1, 1))\n",
    "yy = model.predict(xx)\n",
    "\n",
    "plt.scatter(X_train, Y_train, marker='x', label=\"training data\")\n",
    "plt.scatter(X_test, Y_test, c='r', label=\"testing data\")\n",
    "\n",
    "plt.plot(xx, yy, label=\"regression curve\")\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(ls='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"output/knn_regression.pdf\", bbox_inches=\"tight\", pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "One thing that we should notice in the plot is that the regression relationship is not smooth. This is, of course, because there are areas, where all the points share the same nearest neighbours and the output is therefore insensitive to the input unless it leaves the area. In later notebooks we will explore other kinds of models, which will be able to achieve better results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
